\documentclass[12pt]{extarticle}

\setlength{\headheight}{16pt} % ??? we do what fancyhdr tells us to do  

\title{Analysis 4}
\author{Giacomo Ellero}
\date{a.y. 2024/2025}

\usepackage{preamble}

\renewcommand{\vec}[1]{\uvec{#1}}

\begin{document}
\oldfirstpage

The first part of the course will be about complex analysis and the second one will be about
differential geometry.

\section{Complex analysis}

Naively we can start by replacing $\R$ with $\C \approxeq \R^2$.
A lot things carry over from real analysis but \emph{complex differentiable} functions have
many more interesting properties.

\subsection{Constructing complex numers}

\subsubsection{The boring way}
The \say{boring} definition of complex number is a set $\C \supseteq \R$ where $\sqrt{-1}$ exists
and is defined as $i$ and $-i$.
Formally, this is the smallest \emph{field} where we include $\R$ and $i$.

A complex number $z$ can be written as
\begin{equation}
	z = a + bi \in \C \quad \text{with } a, b \in \R
\end{equation}

This representation is useful since it has a bunch of nice properties:
\begin{itemize}
	\item It is closed under multiplication;
	\item We define the complex conjugate $\overline{a + bi} = a - bi$ and
	      if we multiply $z$ with $\overline z$ we get a real number;
	\item We define a norm over $\C$ as $\norm{z} = z \overline z$;
	\item There exists an inverse, that is for each $z = a + bi \neq 0$
	      (i.e. either $a$ or $b$ $\neq 0$) we can define
	      \begin{equation}
		      z^{-1} = \frac{\overline z}{\norm{z}^2}
	      \end{equation}
\end{itemize}

Now we can formally say that $a + bi$ is the formal expression of a number in $\C$,
define the sum as the usual sum in $\R^2$ and the product as
\begin{equation}
	(a, b) \cdot (c, d) = (ac - bd, ad + bc)
\end{equation}
and we are able to prove that this is commutative and associative,
the proof is long and boring therefore we skip it.

\subsubsection{Geometric construction}

First we start with a simple lemma about isometries.

\begin{lemma}{Isometries that fix the origin are linear maps}{}
	Consider an isometry $f: \R^n \to \R^n$,
	that is $\abs{f(p) - f(q)} = \abs{p-q}$ for all $p, q \in \R^2$.
	If $f(0) = 0$, then $f$ is linear and we can write $f(v) = Av$
	where $A \in O(n)$ is an orthogonal $n\cross n$ matrix, that is $A^{-1} = A^T$.
\end{lemma}

In dimension $n = 2$ we can explicitly write a matrix $A$ which rotates the plane counterclockwise
by an angle $\theta$ as
\begin{equation}
	A = \begin{pmatrix}
		\cos \theta & -\sin \theta \\
		\sin \theta & \cos \theta
	\end{pmatrix}
\end{equation}
Rotation matrices form a subset of $O(n)$, namely those with $\det = 1$.

On the other hand, we define a \emph{homothety} or \emph{dilation} as the linear maps which
can be written as $f = \rho I$, with $\rho \geq 1$.
Now we can combine rotations and dilations to obtain a set of matrices of the form
\begin{align}
	\mathcal C & = \left\{ A = \rho I \cdot R_\theta \mid \rho \geq 0, \theta \in \R\right\} \\
	           & =  \left\{ A = \begin{pmatrix}
		                            \rho \cos \theta & - \rho \sin \theta \\
		                            \rho \sin \theta & \rho \cos \theta
	                            \end{pmatrix}
	\mid \rho \geq 0, \theta \in \R\right\}                                                  \\
	           & = \left\{ A = \begin{pmatrix}
		                           a & -b \\
		                           b & a
	                           \end{pmatrix}
	\mid a, b \in \R\right\}
\end{align}
We notice that $\mathcal C$ is a vector space of matrices of dimension 2 with bases
$I$ (the identity matrix) and $J = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$.

We can now draw a parallel between $\mathcal C$ and $\C$:
$I$ corresponds to $1$ and $J$ corresponds to $i$.

We can now write a formal definition using the matrices:
the sum and the product is the same as in matrices,
note that this set is closed under multiplication as $J ^2 = -I$.
Moreover we get for free that the product is associative (since it is that way with matrices)
and since $IJ = JI$ it is also commutative.

\subsection{Complex differentiability}
We will identify $\C \approxeq \R^2$ so that we get for free the definition of open and closed sets.

\begin{remark}{Product of complex numbers as a matrix}{}
	We can prove that identifying two complex numbers as pairs in $\R^2$ we get
	\begin{equation}
		(a, b) \cdot (c, d) = \begin{pmatrix} a & -b \\ b & a \end{pmatrix}
		\begin{pmatrix} c \\d \end{pmatrix}
	\end{equation}
\end{remark}

Now we can define complex differentiability:
\begin{definition}{Complex differentiability}{}
	Let $U \in \C$ open, $f: U \to \C$ be a function and $z_0 \in U$.
	We say that $f$ is complex differentiable at $z_0$ if
	\begin{equation}
		f(z) = f(z_0) + \alpha (z-z_0) + \o(\abs{z -z_0})
	\end{equation}
	for some $\alpha \in \C$ or equivalently
	\begin{equation}
		f'(z_0) \coloneq \lim_{z \to z_0} \frac{f(z) - f(z_0)}{z - z_0} = \alpha \in \C
	\end{equation}
	exists for some $\alpha$.
\end{definition}
Basically this definition tells us that $f$ is complex differentiable
if the first order taylor expansion exists and is linear.

\begin{definition}{Holomorfic function}{}
	A function $f: U \to \C$ is holomorfic if it is complex differentiable at every $z_0 \in U$.
\end{definition}

\begin{proposition}{Cauchy-Riemann equations}{}
	$f$ is complex differentiable at $z_0 \in U$ $\iff$
	$Df(z_0) = \begin{pmatrix} a & -b \\ b & a \end{pmatrix}$ for some $a, b \in \R$, in which case
	$f'(z_0) = a + bi$.

	Equivalently we can say that there exists $g, h: \R \to \R$ that satisfy
	\begin{equation}
		\begin{cases}
			\pdv{g}{x}()(z_0) = \pdv{h}{y}()(z_0) \\
			\pdv{h}{z}()(z_0) = -\pdv{g}{y}()(z_0)
		\end{cases}
	\end{equation}
	and
	\begin{equation}
		f(z) = f(x + yi) = g(x+yi) + h(x+yi)i
	\end{equation}
\end{proposition}

\begin{proof}
	This is actually quite easy, just see $f$ as a function from and to $\R^2$.
	Now we can compute the differential the usual way and if we want it to look like
	$\begin{pmatrix} a & -b \\ b & a \end{pmatrix}$
	we have to impose the two equations above.
\end{proof}

\subsection{Cauchy integral theorem}

In this section we will discuss and prove Cauchy integral theorem.
We will assume $U$ to be an open subset of $\C$.

\subsubsection{Integration over \texorpdfstring{$\C$}{C}}

\begin{definition}{Piecewise $C^1$ curve}{piecewise-c1}
	A function of the form $\gamma: [s, t]: U$ with $U$ and $s \leq t \in \R$
	is piecewise $C^1$ if for some suitable $s = t_0 < t_1 < \dots < t_N = t$
	the restriction $\eval{\gamma}_{[t_{j-1}, t_j]}$ is of class $C^1$ for all $j = 1, \dots, N$.

	Moreover, we say $\gamma$ is a \emph{loop} if $\gamma(s) = \gamma(t) = z_0$ and we call $z_0$ the
	\emph{basepoint} of the loop.
\end{definition}

\begin{definition}{Line integral}{line-integral}
	Given a continuous function $f: U \to \C$ and a piecewise $C^1$ curve $\gamma$ in $U$ we define the line
	integral of $f$ along $\gamma$ as
	\begin{equation}
		\int_\gamma f(z) \dd z \coloneq
		\sum_{j = 1}^N \int_{t_{j-1}}^{t_j} f(\gamma(\tau)) \cdot \gamma'(\tau) \dd{\tau}
	\end{equation}
\end{definition}

\begin{proposition}{Reparametrization independence}{reparametrization}
	Let $\gamma: U \to [s, t]$ and $\varphi: [s, t] \to [s', t']$ strictly increasing and both
	piecewise $C^1$.
	Let $\delta = \gamma \circ \varphi$, then, for all $f$ continuous,
	\begin{equation}
		\int_\gamma f(z) \dd z = \int_\delta f(z) \dd z
	\end{equation}
\end{proposition}
\begin{proof}
	This is the result of applying the change of variable formula with $z'= \varphi(z)$
	together with the chain rule.
\end{proof}

From now on we will often describe a loop by just its shape and it's orientation, without specifying
the parametrization or the basepoint, since, as we just saw, the integral does not depend on those.

\subsubsection{Cauchy integral theorem special form}

TODO: lec 3

\begin{theorem}{Cauchy integral theorem for rectangles}{cauchy-integral-rect}
\end{theorem}

\subsubsection{Local primitives}

\begin{definition}{Connected set}{connected-set}
	A non-empty set $V \subseteq \C$ is connected if
	$\nexists S_1, S_2 \in V$ open with $S_1, S_2 \neq \varnothing$,
	$V = S_1 \cup S_2$ and $S_1 \cap S_2 = \varnothing$.
\end{definition}

We note that a set is connected if and only if given $p, q \in V$ we can find $\gamma: [s, t] \in V$
continuous such that $\gamma(s) = p$ and $\gamma(t) = q$.
Moreover, if we have a family of connected sets $V_j$ such that they have a point in common, then
their union is still connected.

\begin{definition}{Local primitive}{local-primitive}
	Let $f: U \to \C$ holomorfic and $V \subseteq U$ open and connected.
	Then $F: V \to \C$ is a local primitive of $f$ on $V$ if
	\begin{equation}
		F' = f
	\end{equation}
\end{definition}

\begin{proposition}{Uniqueness of local primitives}{uniq-local-prim}
	Two local primitives $F_1, F_2: V \to \C$ of the same $f$, defined on the same $V \subseteq U$
	are unique up to a constant.
\end{proposition}

This is the same behavior we observe for primitives over $\R$.

\begin{proof}
	Let $H = F_2 - F_1$, we want to prove that $H$ is constant.
	Let $\lambda \in H(V)$ be one of the values assumed by $H$, then
	\begin{equation}
		V = \{ z \in V : H(z) = \lambda \} \cup \{ z \in V : H(z) \neq \lambda \}
	\end{equation}
	This is a disjoint union and we claim that the sets are both open, hence, since $V$ is connected,
	one of the two must be empty.
	We know that $\{ z \in V : H(z) = \lambda \} \neq \varnothing$ therefore necessarily
	$\{ z \in V : H(z) \neq \lambda \} = \varnothing$ and $\lambda$ is the only value assumed by $H$.

	Now, $\{ z \in V : H(z) \neq \lambda \}$ is open because it is the preimage of an open set
	($\C \setminus \lambda$) through a continuous function
	($H$ is holomorfic and therefore continuous).

	To show that $\{ z \in V : H(z) = \lambda \}$ is open we fix a point $z_0 \in V$ such that
	$H(z_0) = \lambda$. Since $V$ is open we can find $B_r(z_0) \in V$ and we will show that
	$H(B_r(z_0)) = \lambda$ which is enough to conclude that $\{ z \in V : H(z) = \lambda \}$ is open.
	To prove this we use the fact that $H' = f - f = 0$ and viewing it as a function from $\R^2$ to
	$\R^2$ we get that $DH(z) = 0$ everywhere.
	Now we consider $z_1 \in B_r(z_0)$ and we let $\gamma: [0,1] \to B_r(z_0)$ be the parametrization
	of the segment from $z_0$ to $z_1$:
	\begin{equation}
		\gamma(t) = H(z_0 + t(z_1 - z_0))
	\end{equation}
	and by the chain rule we have that
	\begin{equation}
		(H\circ \gamma)'(t) = DH(\gamma(t)) [\gamma'(t)] = 0
	\end{equation}
	Thus $H \circ \gamma: [0, 1] \to \R^2$ is a function with zero derivative at each point,
	which means that
	\begin{equation}
		H(z_1) = H(\gamma(1)) = H(\gamma(0)) = H(z_0) = \lambda
	\end{equation}
	as desired.
\end{proof}

\subsubsection{Integration over piecewise \texorpdfstring{$C^1$}{C1} curves}

\begin{proposition}{Fundamental theorem of calculus for curves}{fundamental-curves}
	Given a local primitive $F: V \to \C$ of $f$ and a piecewise $C^1$ curve $\gamma: [s, t] \to V$
	we have
	\begin{equation}
		\int_\gamma f(z) \dd z = F(\gamma(t)) - F(\gamma(s))
	\end{equation}
\end{proposition}

\begin{proof}
	Let $s = t_0 < t_1 < \dots < t_N = t$ such that the restriction $\eval{\gamma}_{[t_{j-1}, t_j]}$
	is $C^1$ for all $j = 1, \dots, N$.
	The usual proof of the chain rule gives us that
	\begin{equation}
		\dv{\tau} (F\circ \gamma)(\tau) = F'(\gamma(\tau)) \cdot \gamma'(\tau)
	\end{equation}
	where the lat $\cdot$ is the product of complex numbers.

	Since $F' = f$ we get
	\begin{align}
		\int_\gamma f(z) \dd z
		 & = \sum^N_{j = 1} \int^{t_j}_{t_{j-1}} f(\gamma(\tau)) \cdot \gamma'(\tau) \dd \tau  \\
		 & = \sum^N_{j = 1} \int^{t_j}_{t_{j-1}} F'(\gamma(\tau)) \cdot \gamma'(\tau) \dd \tau \\
		 & = \sum^N_{j = 1} \int^{t_j}_{t_{j-1}} (F\circ \gamma)'(\tau) \dd \tau               \\
		 & = \sum^N_{j = 1} [F \circ \gamma(t_j) - F \circ \gamma(t_{j-1})]                    \\
		 & = F \circ \gamma(t) - F \circ \gamma(s)
	\end{align}
	where we have used the fundamental theorem of calculus and
	the fact that the last sum is telescopic.
\end{proof}

\begin{corollary}{}{}
	If a local primitive $F: V \to \C$ exists and $\gamma: [s, t] \to V$ is a piecewise $C^1$ loop
	we have
	\begin{equation}
		\int_\gamma f(z) \dd z = 0
	\end{equation}
\end{corollary}
\begin{proof}
	Follows from the proposition since $F(\gamma(t)) = F(\gamma(s))$.
\end{proof}

\subsubsection{Local primitives in a ball}

We will now show that a local primitive always exists within open balls
(often called disks when over $\C$).

We will also introduce the following notation:
\begin{itemize}
	\item Given two curves $\eta$ and $\beta$ we indicate with $\eta * \beta$ the concatenation
	      of two curves such that
	      \begin{equation}
		      \int_{\eta * \beta} f(z) \dd z = \int_\eta f(z) \dd z + \int_\beta f(z) \dd z
	      \end{equation}
	\item We denote $[p, q]$ any parametrization of the segment that connects the two points $p, q$.
	      A standard parametrization is $\tau \mapsto p + \tau(q-p)$ with $\tau \in [0, 1]$.
\end{itemize}

\begin{proposition}{Existence of local primitives in a ball}{existence-local-primitive}
	Let $f: U \to \C$ holomorfic and $V = B_r(z_0) \subseteq U$ be an open ball.
	Then there exists a local primitive $F(z)$ for all $z \in V$.
\end{proposition}

\begin{proof}
	Let $z \in V$ and $a + bi = z - z_0$, consider two piecewise $C^1$ curves $\gamma, \delta$
	defined as
	\begin{align}
		\gamma & = [z_0, z_0 + bi] * [z_0 + bi, z] \\
		\delta & = [z_0, z_0 + a] * [z_0 + a, z]
	\end{align}
	that is, both curves start at $z_0$ and reach $z$ but
	$\gamma$ travels first vertically then horizontally while
	$\delta$ travels first horizontally and then vertically.

	Let $\alpha = \delta * \gamma^{-1}$ be the concatenation of $\delta$ and the \emph{reverse} of
	$\gamma$, that is, $\alpha$ parametrizes the boundary of the rectangle $R$ with vertices
	$\{z_0, z_0 + a, z_0 + bi, z\}$.
	This gives
	\begin{equation}
		\int_\delta f(w) \dd w - \int_\eta f(w) \dd w = \int_{\partial R} f(z) \dd z = 0
	\end{equation}
	by \cref{thm:cauchy-integral-rect} which we have already proven. Also note that all $w \in R$ are
	also included in $V$.
	Now, since their difference is $0$, we can let $F(z)$ be the common value of these two integrals:
	\begin{equation}
		F(z) = \int_\delta f(w) \dd w = \int_\eta f(w) \dd w
	\end{equation}

	Interpreting $F$ as a map of the form $F: \R^2 \to \C$, we have
	\begin{equation}
		\pdv{F}{x}()(z) = \lim_{\varepsilon \to 0} \frac{F(z + \varepsilon) - F(z)}{\varepsilon}
	\end{equation}
	where $\varepsilon \in \R$.
	Now if we consider the definition of $F$ using $\gamma$, we have
	\begin{align}
		F(z + \varepsilon) & = \int_{[z_0, z_0 + ib] * [z_0 + ib, z + \varepsilon]} f(w) \dd w          \\
		                   & = \int_{[z_0, z_0 + ib] * [z_0 + ib, z] * [z, z + \varepsilon]} f(w) \dd w \\
		                   & = \int_{[z_0, z_0 + ib] * [z_0 + ib, z]} f(w) \dd w
		+ \int_{[z, z + \varepsilon]} f(w) \dd w                                                        \\
		                   & = F(z) + \int_{[z, z + \varepsilon]} f(w) \dd w
	\end{align}
	then we choose $\tau \mapsto z + \tau \varepsilon$ as our parametrization of $[z, z + \varepsilon]$
	so that $\dv{\tau} (z + \tau \varepsilon) = \varepsilon$.
	We get
	\begin{equation}
		F(z + \varepsilon) - F(z) = \int_0^1 f(z + \tau \varepsilon) \cdot \varepsilon \dd \tau
		= \int_0^\varepsilon f(z + \sigma) \dd{\sigma}
	\end{equation}
	where we have used the change of variable $\sigma = \tau \varepsilon$.
	Then, since $f$ is continuous, (and???)
	\begin{equation}
		\pdv{F}{x}()(z) = \lim_{\varepsilon \to 0} \frac{1}{\varepsilon}
		\int_0^\varepsilon f(z + \sigma) \dd \sigma = f(z)
	\end{equation}

	Similarly for the imaginary part we have
	\begin{equation}
		\pdv{F}{y}()(z) = \lim_{\varepsilon \to 0} \frac{1}{\varepsilon}
		\int_0^\varepsilon f(z + i \sigma) \cdot i \dd \sigma = f(z) \cdot i
	\end{equation}
	where the additional $i$ comes from the fact that now the extra piece we are integrating over is
	$[z, z + i \varepsilon]$.

	We have just showed that $F$ admits a derivative at any point $z \in V$ and it is continuous,
	since $f$ is continuous, therefore, we can invoke the powers of multivariate calculus to say that
	$F$ is indeed differentiable and of class $C^1$.
	Moreover if we write $f(z) = \alpha + i \beta$ we have
	\begin{align}
		\pdv{F}{x}()(z) & = \alpha + i \beta  \\
		\pdv{F}{y}()(z) & = - \beta + i\alpha
	\end{align}
	which means that $F$ satisfies Cauchy-Riemann equations therefore $F$ is holomorfic
	and $F'(z) = f(z)$ for all $z \in V$.
\end{proof}

\subsubsection{Integration over continuous curves}

We now state a more general result for integrating over all continuous curves
(not just those which are piecewise $C^1$).

\begin{proposition}{Line integral over a continuous curve}{integral-continuous-curve}
	Let $f: U \to C$ holomorfic and $\gamma: [s, t] \to U$ continuous,
	then there exists a subdivision $s = t_0 < t_1 \dots < t_N = t$ such that
	for each $j = 1, \dots, N$ the image of $\gamma([t_{j-1}, t_j])$ is included in the domain
	$V_j \subseteq U$ of some local primitive $F_j: V \to \C$ of $f$.

	Moreover,
	\begin{equation}
		\int_\gamma f(z) \dd z = \sum_{j = 1}^N [F_j(\gamma(t_j)) - F_j(\gamma(t_{j-1}))]
	\end{equation}
	and this value does not depend on the choice of the subdivision, of $V_j$ or of $F_j$.
\end{proposition}

\begin{proof}
	If $U = \C$ we can just take $N = 1$ and $V_1$ be any ball included in the image of $\gamma$.

	Otherwise we let $\rho: \C \to \R$ be the function that assigns to each $z \in \C$ the distance to
	$\C \setminus U$, that is
	\begin{equation}
		\rho(z) = \inf_{w \in \C \setminus U} \abs{z - w}
	\end{equation}
	Moreover, by the triangle inequality we have that $\rho(z') \leq \abs{z-z'} + \rho(z)$
	and interchanging the roles of $z$ and $z'$ we get $\abs{\rho(z') - \rho(z)} \leq \abs{z' - z}$
	which means that $\rho$ is Lipschitz continuous.
	Furthermore, since $U$ is open, $\rho$ is positive on $U$, thus we can let
	\begin{equation}
		r = \min_{\tau \in [s, t]} \rho(\gamma(\tau))) > 0
	\end{equation}
	Now, since $\gamma$ is continuous and its domain is compact it is also uniformly continuous,
	thus, for some $N$ large enough, we have
	\begin{equation}
		\abs{\gamma(\tau) - \gamma(t_j)} < r \quad \text{for } \tau \in [t_{j-1}, t_j],
		\quad \text{with } t_j = s + j \frac{t-s}{N}
	\end{equation}
	which means we can take $\gamma(t_j)$ as the center of the ball $V_j$ with radius $r$, which is
	guaranteed to be included in $U$ since $r \leq \rho(\gamma(t_j))$,
	and invoking \cref{prop:existence-local-primitive} we obtain a local primitive $F_j$.
\end{proof}

\end{document}
