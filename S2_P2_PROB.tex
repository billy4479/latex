\documentclass[14pt]{extarticle}

\usepackage{preamble}

\title{Probability Notes, Partial 2}
\date{Semester 2, 2023/2024}

\setlength{\headheight}{17pt} % ??? we do what fancyhdr tells us to do

\newcommand{\cov}{{\operatorfont Cov}}
% Distributions
\newcommand{\Normal}{{\operatorfont N}}
\newcommand{\GammaD}{{\operatorfont Gamma}}
\newcommand{\Poisson}{{\operatorfont Poisson}}
\newcommand{\Exponential}{{\operatorfont Exponential}}
\newcommand{\BivariateNormal}{{\operatorfont BivariateN}}

\renewcommand{\vec}[1]{\uvec{#1}}

\begin{document}

\firstpage

% Class of 20/03/2024
\section{Introduction}

In the last partial we studied discrete random variables, which are random variables that can take a finite or countably infinite number of values.
Now we will consider random variables that take values in an interval of the real line, which are called \emph{continuous random variables}.

\begin{definition}[continuous random variable]
    \label{def:continuous_random_variable}

    A random variable $X$ is said to be \emph{continuous} if $\exists f(x) : \forall a, b \in \R, a < b$,
    \begin{equation}
        P(a \leq X \leq b) = \int_a^b f(x) \dd{x}
    \end{equation}
    where $f(x)$ is called the \emph{probability density function} (pdf) of $X$.
\end{definition}

Note that $f$ is not unique, and in fact, any function $f$ that satisfies the above equation is a valid pdf for $X$. Usually we will choose the simplest function that satisfies the equation, for example, a continuous one.

Moreover, we also note that if $f(x)$ is negative in an interval, then it is possible to have $P(a \leq X \leq b) < 0$, which is not possible.
Therefore we will consider only non-negative functions as pdfs.

\begin{proposition}[properties of the pdf]
    \label{prop:properties_of_pdf}
    Let $X$ be a continuous random variable with pdf $f(x)$, then

    \begin{enumerate}[label=\roman*)]
        \item $\forall x \in \R, P(X = x) = 0$
        \item $F(x) = \int_{-\infty}^x f(u) \dd{u}$ \item $\int_{-\infty}^{+\infty} f(x) \dd{x} = 1$
        \item if $f$ is continuous at $x$, then $F'(x) = f(x)$
    \end{enumerate}
\end{proposition}

\begin{proof}
    \skiplineafterproof
    \begin{enumerate}[label=\roman*)]
        \item
              \begin{align}
                  P(X = x) & = P(\bigcap_{n=1}^\infty (x \leq X \leq x + \frac{1}{n}))      \\
                           & = \lim_{n \to \infty} P(x \leq X \leq x + \frac{1}{n})         \\
                           & = \lim_{n \to \infty} \int_x^{x + \frac{1}{n}} f(u) \dd{u} = 0
              \end{align}

        \item
              \begin{align}
                  F(x) & = P(X \leq x)                                  \\
                       & = P (\bigcup_{n=1}^\infty (x-n \leq X \leq x)) \\
                       & = \lim_{n \to \infty} P(x-n \leq X \leq x)     \\
                       & = \lim_{n \to \infty} \int_{x-n}^x f(u) \dd{u} \\
                       & = \int_{-\infty}^x f(u) \dd{u}
              \end{align}

        \item
              \begin{align}
                  \int_{-\infty}^{+\infty} f(x) \dd{x} & = \int_{-\infty}^x f(x) \dd{x}  \\
                                                       & = \lim_{x \to -\infty} F(x) = 1
              \end{align}
        \item This is a direct consequence of ii) and the fundamental theorem of calculus.
    \end{enumerate}
\end{proof}

\begin{remark}
    The pdf $f(x)$ can be interpreted as the \emph{density} of the random variable $X$ at the point $x$, that is
    \begin{equation}
        P(x \leq X \leq x + h) \sim f(x) h \quad \text{as } h \to 0
    \end{equation}
\end{remark}

\begin{lemma}[indipendent random variables]
    \label{lem:independent_random_variables}
    Let $X$ and $Y$ be continuous random variables such that $X \indep Y$, and let $B_1, B_2$ be any borel sets, then
    \begin{equation}
        P\left((X \in B_1) \cap (Y \in B_2)\right) = P(X \in B_1) P(Y \in B_2)
    \end{equation}

    This is equivalent to saying
    \begin{equation}
        P\left((X \in x) \cap (Y \in y)\right) = P(X \in x) P(Y \in y)
    \end{equation}
\end{lemma}

\begin{proof}
    We will not prove this as it is quite hard and not very useful.
\end{proof}

\begin{example}
    Our experiment is extracting random numbers from the interval $[0, 1]$.
    Let $X, Y$ such that $X \indep Y$.

    We want to calculate $P\left((0.5 \leq X \leq 0.75) \cap (0.5 \leq Y \leq 0.75)\right)$.

    Since they are independent we can use the lemma to get $P(0.5 \leq X \leq 0.75) P(0.5 \leq Y \leq 0.75)$.

    Note, by the nature of the experiment, that $P(a \leq X \leq b) = b - a$, then we get
    $P(0.5 \leq X \leq 0.75) P(0.5 \leq Y \leq 0.75) = 0.25 \cdot 0.25 = 0.0625$.

    Moreover we note that
    \begin{equation}
        P(a \leq X \leq b) = \int_a^b I_{[0,1]}(x) \dd{x} = b - a
    \end{equation}
    where $I_{[0,1]}(x)$ is the indicator function of the interval $[0, 1]$.
\end{example}

\subsection{Transformation of random variables}

\begin{proposition}[transformation of independent random variables]
    \label{prop:transformation_indep_crv}

    If $X \indep Y$, then $g(X) \indep h(Y)$.
\end{proposition}

\begin{proof}
    \skiplineafterproof
    \begin{align}
        P\left(
        (g(X) \in B_1) \cap (h(Y) \in B_2)
        \right) & = P\left(
        (X \in g^{-1}(B_1)) \cap (Y \in h^{-1}(B_2))
        \right)                                               \\
                & = P(X \in g^{-1}(B_1)) P(Y \in h^{-1}(B_2)) \\
                & = P(g(X) \in B_1) P(h(Y) \in B_2)
    \end{align}
    where $B_1, B_2$ are borel sets and $g^{-1}(B_1), h^{-1}(B_2)$ are the preimages of $B_1, B_2$ under $g, h$ respectively.
\end{proof}

\begin{remark}
    Let $X$ be a continuous random variable with pdf $f_X(x)$ and let $Y = g(X)$,
    then $Y$ could be either continuous, discrete, or neither.
\end{remark}

\begin{theorem}[transformation of random variables]
    \label{thm:transformation_crv}

    Let $X$ be a continuous random variable, $f_X(x)$ its pdf, such that $X$ takes values in an open interval $U$.
    Let $g: U \to V$ be a one-to-one, strictly increasing or strictly decreasing function continuously differentiable in $U$.

    Then $Y = g(X)$ is a continuous random variable with pdf $f_Y(y)$ given by
    \begin{equation}
        f_Y(y) = f_X(g^{-1}(y)) \abs{\dv{g^{-1}(y)}{y}}
    \end{equation}
\end{theorem}

\begin{proof}
    Let $a, b \in V$, such that $a < b$.
    Then
    \begin{align}
        P(a \leq Y \leq b) & = P(a \leq g(X) \leq b)                                  \\
                           & = P(g^{-1}(a) \leq X \leq g^{-1}(b))                     \\
                           & = \int_{g^{-1}(a)}^{g^{-1}(b)} f_X(x) \dd{x}             \\
                           & = \int_a^b f_X(g^{-1}(y)) \abs{\dv{g^{-1}(y)}{y}} \dd{y}
    \end{align}

    where we used the change of variables $x = g^{-1}(y)$.
    Note that the inverse of $g$ exists because $g$ is one-to-one.

    We need the absolute value because if $g$ is decreasing, then we get a minus sign from the change of variables that we don't want.
\end{proof}

\begin{example}
    Let $X$ a random variable representing the radius of a circle, and let $Y$ be the area of such circle.
    Let $f_X(x) = 6x(1-x)I_{(0,1)}(x)$, then we want to find $f_Y(y)$.

    We have that $Y = \pi X^2$, that is $g(x) = \pi x^2$ and its inverse is $g^{-1}(y) = \sqrt{\frac{y}{\pi}}$.

    Moreover $\dv{g^{-1}(y)}{y} = \frac{1}{2} \frac{1}{\sqrt{\pi y}}$.

    We use \autoref{thm:transformation_crv} to get

    \begin{align}
        f_Y & = 6\sqrt{\frac{y}{\pi}} \left(1- \sqrt{\frac{y}{\pi}}\right) \frac{1}{2} \frac{1}{\sqrt{\pi y}} I_{(0, 1)}\left(\frac{y}{\pi}\right) \\
            & = \frac{3}{\pi} \left(1- \frac{y}{\pi}\right) I_{(0, \pi)}(y)
    \end{align}

\end{example}

% Class of 22/03/2024
\subsection{Expectation and variance}

\begin{definition}[expectation]
    Let $X$ be a continuous random variable with pdf $f(x)$, then the \emph{expectation} of $X$ exists if one at least of the following integrals is finite
    \begin{equation}
        E(X) = \int_{-\infty}^{0} x f(x) \dd{x} + \int_{0}^{+\infty} x f(x) \dd{x}
    \end{equation}

    If $E(X)$ exists and is finite we say that $X$ is \emph{integrable}.
\end{definition}

\begin{lemma}[condition for integrability]
    $X$ is integrable iff
    \begin{equation}
        \int_{-\infty}^{+\infty} \abs{x} f(x) \dd{x} < +\infty
    \end{equation}
\end{lemma}

\begin{example}
    Let $X$ have a pdf $f(x) = \frac{1}{\pi (1+x^2)}$.
    We want to find $E(X)$.

    We have that
    \begin{equation}
        \int_0^{+\infty} x \frac{1}{\pi (1+x^2)} \dd{x} \quad \text{and} \quad \int_{-\infty}^0 x \frac{1}{\pi (1+x^2)} \dd{x}
    \end{equation}

    Note that as $x \to +\infty$, $x \frac{1}{\pi (1+x^2)} \sim \frac{1}{\pi x}$, which diverges to $+\infty$.
    For $x \to -\infty$ we use the same argument to get that the integral diverges to $-\infty$.

    Therefore $E(X)$ does not exist.
\end{example}

\begin{theorem}
    Let $X$ be a continuous random variable with pdf $f(x)$ continuous defined on a support $[a, b]$, with $a < 0, b > 0$, and $f(x) = 0$ outside $[a, b]$.

    Then
    \begin{equation}
        E(X) = -\int_{a}^0 F(x) \dd{x} + \int_0^b (1 - F(x)) \dd{x}
    \end{equation}
    with $F'(x) = f(x)$.
\end{theorem}

\begin{proof}
    We have
    \begin{align}
        E(X) & = \int_{-\infty}^0 x f(x) \dd{x} + \int_0^{+\infty} x f(x) \dd{x}           \\
             & = \int_{a}^0 x f(x) \dd{x} + \int_0^{b} x f(x) \dd{x}                       \\
             & = [x F(x)]_a^0 - \int_a^0 F(x) \dd{x} + [x F(x)]_0^b - \int_0^b F(x) \dd{x} \\
             & = -\int_a^0 F(x) \dd{x} + b \int_0^b F(x) \dd{x}                            \\
             & = -\int_a^0 F(x) \dd{x} + \int_0^b (1-F(x)) \dd{x}
    \end{align}
    by applying integration by parts.
\end{proof}

\begin{remark}
    This is true even for $a = -\infty$ and $b = +\infty$.
\end{remark}

\begin{proposition}[transformation of expectation]
    Let $X$ be a continuous random variable with pdf $f(x)$ and $Y = g(X)$, then
    \begin{equation}
        E(Y) = \int_{-\infty}^{+\infty} g(x) f(x) \dd{x}
    \end{equation}

    Moreover $Y$ is integrable iff
    \begin{equation}
        \int_{-\infty}^{+\infty} \abs{g(x)} f(x) \dd{x} < +\infty
    \end{equation}
\end{proposition}

\begin{example}
    Let $X$ be a random variable with pdf $f(x) = 6x(1-x) I_{(0, 1)}(x)$, and $Y = X^2$.
    Then
    \begin{align}
        E(Y) & = \int_{-\infty}^{+\infty} x^2 f(x) \dd{x} \\
             & = \int_0^1 x^2 6x(1-x) \dd{x}              \\
             & = 6 \int_0^1 (x^3 - x^4) \dd{x}            \\
             & = \frac{6}{20}
    \end{align}
\end{example}

\begin{remark}
    In general we cannot say that $E(g(X)) = g(E(X))$.
\end{remark}

\begin{theorem}[linearity of expectation]
    Let $X$ be a random variable, $a, b \in \R$, then
    \begin{equation}
        E(aX + b) = aE(X) + b
    \end{equation}
\end{theorem}

\begin{proof}
    \skiplineafterproof
    \begin{align}
        E(aX + b) & = \int_{-\infty}^{+\infty} (ax + b) f(x) \dd{x}                                     \\
                  & = a \int_{-\infty}^{+\infty} x f(x) \dd{x} + b \int_{-\infty}^{+\infty} f(x) \dd{x} \\
                  & = aE(X) + b
    \end{align}
\end{proof}

\begin{definition}[moment]
    Let $X$ be an integrable random variable, then we can define the following

    \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{moment}       & \textbf{symbol} & \textbf{definition} \\
            \hline
            first moment          & $\mu$           & $E(X)$              \\
            $k$-th moment         &                 & $E(X^k)$            \\
            $k$-th central moment &                 & $E((X - E(X))^k)$   \\
            variance              & $\sigma^2$      & $E((X - E(X))^2)$   \\
            \hline
        \end{tabular}

        \caption{Moments of a random variable}
        \label{tab:moments}
    \end{table}

    The even $k$-th moment have the propriety that $\int_{-\infty}^0 x^k f(x) \dd{x} = 0$.
\end{definition}

\begin{theorem}
    Let $X$ be a random variable with finite $k$-th moment.
    Then $\forall j < k$, the $j$-th moment exists and is finite.

    Moreover the $k$-th central moment exists and is finite.
\end{theorem}

\begin{proof}
    We know that $E(X^k)$ exists iff $E(\abs{X}^k)$ exists, we want to prove that $E(\abs{X}^j)$ exists $\forall j < k$.

    \begin{equation}
        E(\abs{X}^j) = \int_{-\infty}^{+\infty} \abs{x}^j f(x) \dd{x}
    \end{equation}

    We would like to say that $\abs{x}^j \leq \abs{x}^k$, but this is not always true, for instance, this is true only if $\abs{x} \geq 1$.

    Then we can write $E(\abs{X}^j)$ as
    \begin{align}
        E(\abs{X}^j) & \leq \int_{-\infty}^{+\infty} \max(\abs{x}^j, 1) f(x) \dd{x}                                      \\
                     & \leq \int_{-\infty}^{+\infty} \max(\abs{x}^k, 1) f(x) \dd{x}                                      \\
                     & \leq \int_{-\infty}^{+\infty} (\abs{x}^k + 1) f(x) \dd{x}                                         \\
                     & = \int_{-\infty}^{+\infty} \abs{x}^k f(x) \dd{x} + \int_{-\infty}^{+\infty} f(x) \dd{x} < +\infty
    \end{align}

    For the second part we can write

    \begin{align}
        E(\abs{X - \mu}^k) & = \int_{-\infty}^{+\infty} \abs{x - \mu} f(x) \dd{x}                                                 \\
                           & \leq \int_{-\infty}^{+\infty} (\abs{x} + \abs{\mu}) f(x) \dd{x}                                      \\
                           & = \int_{-\infty}^{+\infty} \sum_{j=0}^k \binom{k}{j} \abs{x}^j \abs{\mu}^{k-j} f(x) \dd{x}           \\
                           & = \sum_{j=0}^k \binom{k}{j} \abs{\mu}^{k-j} \int_{-\infty}^{+\infty} \abs{x}^j f(x) \dd{x} < +\infty
    \end{align}
\end{proof}

\begin{definition}[variance]
    Let $X$ be a random variable, then the \emph{variance} of $X$ is defined as
    \begin{align}
        V(X) & = \sigma^2 = E((X - \mu)^2)                        \\
             & = \int_{-\infty}^{+\infty} (x - \mu)^2 f(x) \dd{x}
    \end{align}
\end{definition}

\begin{theorem}[properties of variance]
    The following equations hold:
    \begin{align}
         & V(X) = E(X^2) - E(X)^2 \label{eq:prop_variance:1} \\
         & V(X + b) = V(X)        \label{eq:prop_variance:2} \\
         & V(aX) = a^2 V(X) \label{eq:prop_variance:3}
    \end{align}
\end{theorem}

\begin{proof}
    \skiplineafterproof
    \begin{enumerate}[label=\roman*.]
        \item (\autoref{eq:prop_variance:1})
              \begin{align}
                  V(X) = \int_{-\infty}^{+\infty} (x - \mu)^2 f(x) \dd{x} & = \int_{-\infty}^{+\infty} (x^2 - 2x\mu + \mu^2) f(x) \dd{x}                                                                          \\
                                                                          & = \int_{-\infty}^{+\infty} x^2 f(x) \dd{x} - 2\mu \int_{-\infty}^{+\infty} x f(x) \dd{x} + \mu^2 \int_{-\infty}^{+\infty} f(x) \dd{x} \\
                                                                          & = E(X^2) - 2\mu E(X) + \mu^2 = E(X^2) - E(X)^2
              \end{align}

        \item (\autoref{eq:prop_variance:2})
              \begin{align}
                  V(X + b) & = E((X + b)^2) - E(X + b)^2                     \\
                           & = E(X^2 + 2bX + b^2) - (E(X) + b)^2             \\
                           & = E(X^2) + 2bE(X) + b^2 - E(X)^2 - 2bE(X) - b^2 \\
                           & = V(X)
              \end{align}

        \item (\autoref{eq:prop_variance:3})
              \begin{align}
                  V(aX) & = E((aX)^2) - E(aX)^2     \\
                        & = E(a^2 X^2) - (aE(X))^2  \\
                        & = a^2 E(X^2) - a^2 E(X)^2 \\
                        & = a^2 (E(X^2) - E(X)^2)   \\
                        & = a^2 V(X)
              \end{align}
    \end{enumerate}
\end{proof}

% Class of 25/03/2024

\section{Noteworthy distributions}

\subsection{Uniform continuous distribution}

Let $a, b \in \R$ with $a < b$. $X$ is said to have a \emph{uniform continuous distribution} in the interval $[a, b]$ if its pdf is constant in the interval.

\begin{itemize}
    \item PDF:
          \begin{equation}
              \label{eq:uniform:pdf}
              f(x) = \frac{1}{b-a} I_{[a, b]}(x)
          \end{equation}
    \item CDF:
          \begin{equation}
              F(x) = \int_{-\infty}^x f(u) \dd{u} = \begin{cases}
                  0               & x < a        \\
                  \frac{x-a}{b-a} & a \leq x < b \\
                  1               & x \geq b
              \end{cases}
          \end{equation}
    \item Expectation:
          \begin{equation}
              E(X) = \int_{-\infty}^{+\infty} x f(x) \dd{x} = \int_a^b \frac{x}{b-a} \dd{x} = \frac{1}{b-a} \left[\frac{x^2}{2}\right]_a^b = \frac{a+b}{2}
          \end{equation}
    \item Variance:
          \begin{equation}
              V(X) = E(X^2) - E(X)^2 = \frac{b^2 + ab + a^2}{3} - \frac{(a+b)^2}{4} = \frac{(b-a)^2}{12}
          \end{equation}
\end{itemize}

\subsection{Exponential distribution}

We have one parameter $\lambda > 0$.

\begin{itemize}
    \item PDF:
          \begin{equation}
              f(x) = \lambda e^{-\lambda x} I_{[0, +\infty)}(x)
          \end{equation}
    \item CDF:
          \begin{equation}
              F(x) = \int_{-\infty}^x f(u) \dd{u} = (1 - e^{-\lambda x})I_{[0, +\infty)}(x)
          \end{equation}

          We know that $F(x) = P(X \leq x)$, then it's easy to remember that $P(X > x) = e^{-\lambda x}$.
    \item Expectation:
          \begin{equation}
              E(X) = \int_{-\infty}^{+\infty} (1 - F(x)) \dd{x} = \int_0^{+\infty} \lambda e^{-\lambda x} \dd{x} = \frac{1}{\lambda}
          \end{equation}
    \item Variance:
          \begin{equation}
              V(X) = E(X^2) - E(X)^2 = \frac{1}{\lambda^2}
          \end{equation}

          $E(X)^2$ can be computed by integrating by parts.
\end{itemize}

The exponential distribution is related to some discrete distributions:
\begin{itemize}
    \item Similarly to the geometric distribution, the exponential distribution is \emph{memoryless}, that is
          \begin{equation}
              P(X > x + z \mid X > z) = P(X > x)
          \end{equation}

          We can also prove that the exponential distribution is the continuous counterpart of the geometric distribution.

    \item
          The exponential distribution is also the distribution of the time between two events in a Poisson distribution.

          \begin{proof}
              We can prove this by considering $X$ the time of the next arrival and $N_t$ the number of arrivals in an interval of length $t$.
              Then $X \sim \Exponential(\lambda)$ and $N_t \sim \Poisson(t \lambda)$.
              Moreover, $P(X > t) = P(N_t = 0)$, that is, the probability of the next visit being after $t$ is the same as the probability of having no visits in the interval $t$.
              Then $P(X > t) = e^{-\lambda t}$.
          \end{proof}
\end{itemize}

\begin{example}
    We have a rate of 20 people per hour.

    Let $N_t$ be the number of people entering in a certain time interval $t$, then $N_t \sim \Poisson(t \lambda)$.
    We want to find the probability that the time between two arrivals is less than 3 minutes.

    Note that $N_{60} \sim \Poisson(20)$, that is, $\lambda = \frac{1}{3}$.

    Then the time between two arrivals $X \sim \Exponential\left(\frac{1}{3}\right)$, and $P(X < 3) = 1 - e^{-\lambda \cdot 3} = 1 - e^{-1}$.
\end{example}

\subsection{Normal or Gaussian distribution}
\label{sec:normal_distribution}

We have two parameters $\mu \in \R$ and $\sigma > 0$.

\begin{itemize}
    \item PDF:
          \begin{equation}
              f(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
          \end{equation}
    \item CDF:
          \begin{equation}
              F(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(u-\mu)^2}{2\sigma^2}} \dd{u}
          \end{equation}

          It is possible to prove that this is not a primitive function, hence it is not possible to express it in terms of elementary functions, but it is possible to compute that $\lim_{x \to +\infty} F(x) = 1$, but we will cover this in Analysis.
    \item Expectation:
          \begin{align}
              E(X) & = \int_{-\infty}^{+\infty} x \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \dd{x}                                                                                         \\
                   & = \int_{-\infty}^{+\infty} (z + \mu) \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{z^2}{2\sigma^2}} \dd{z}                                                                                       \\
                   & = \int_{-\infty}^{+\infty} z \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{z^2}{2\sigma^2}} \dd{z} + \mu \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{z^2}{2\sigma^2}} \dd{z} \\
                   & = 0 + \mu = \mu \label{eq:noteworthy:gaussian_1}
          \end{align}
          where $z = x - \mu$.

          In \autoref{eq:noteworthy:gaussian_1} we used the fact that the first integral contains an odd function, and the second integral is the pdf of a normal distribution hence it is equal to 1.

    \item Variance:
          \begin{align}
              V(X) & = \int_{-\infty}^{+\infty} (x - \mu)^2 \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \dd{x}                                                               \\
                   & = \int_{-\infty}^{+\infty} z^2 \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{z^2}{2\sigma^2}} \dd{z}                                                                             \\
                   & = \frac{1}{\sqrt{2\pi} \sigma}(-\sigma^2) \int_{-\infty}^{+\infty} z \left(- \frac{z}{\sigma^2}\right) e^{-\frac{z^2}{2\sigma^2}} \dd{z}                                  \\
                   & = -\frac{\sigma}{\sqrt{2\pi}} \left( \left[ z e^{-\frac{z^2}{2\sigma^2}} \right]_{-\infty}^{+\infty} - \int_{-\infty}^{+\infty} e^{-\frac{z^2}{2\sigma^2}} \dd{z} \right) \\
                   & = \frac{\sigma}{\sqrt{2\pi}} \int_{-\infty}^{+\infty} e^{-\frac{z^2}{2\sigma^2}} \dd{z}                                                                                   \\
                   & = \sigma^2 \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{z^2}{2\sigma^2}} \dd{z}                                                                        \\
                   & = \sigma^2
          \end{align}
\end{itemize}

\subsubsection{Properties of the normal distribution}

% Class of 27/03/2024

\begin{theorem}[transformation of normal random variables]
    Let $X \sim \Normal(\mu, \sigma^2)$, $a, b \in \R$ with $a \neq 0$, then
    \begin{equation}
        Y = aX + b \sim \Normal(a\mu + b, a^2 \sigma^2)
    \end{equation}
\end{theorem}

\begin{proof}
    Let $g(x) = ax + b$ such that $Y = g(X)$.
    Then $g^{-1}(y) = \frac{y - b}{a}$, and $\dv{g^{-1}(y)}{y} = \frac{1}{a}$.

    By \autoref{thm:transformation_crv} we have that
    \begin{align}
        f_Y(y) & = f_X(g^{-1}(y)) \abs{\dv{g^{-1}(y)}{y}}                                                                                \\
               & = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{1}{2} \frac{\left(\frac{1}{a}(y-b)- \mu\right)^2}{\sigma^2}} \abs{\frac{1}{a}} \\
               & = \frac{1}{\sqrt{2\pi} a \sigma} e^{-\frac{1}{2} \frac{(y - a\mu - b)^2}{a^2 \sigma^2}}
    \end{align}
    which is the pdf of a Normal distribution with parameters $a\mu + b$ and $a^2 \sigma^2$.
\end{proof}

\begin{definition}[standard normal distribution]
    The \emph{standard normal distribution} is a normal distribution with parameters $\mu = 0$ and $\sigma = 1$. It is denoted as $Z \sim \Normal(0, 1)$.
\end{definition}

\begin{lemma}
    For any $X \sim \Normal(\mu, \sigma^2)$, then $Z = \frac{X - \mu}{\sigma} \sim \Normal(0, 1)$.
\end{lemma}

\begin{proof}
    \begin{align}
        E(Z) & = E\left(\frac{X - \mu}{\sigma}\right) = \frac{1}{\sigma} (E(X) - \mu) = 0 \\
        V(Z) & = V\left(\frac{X - \mu}{\sigma}\right) = \frac{1}{\sigma^2} V(X) = 1
    \end{align}
\end{proof}

\begin{remark}[calculating the CDF of a normal distribution]
    We will need to use tables to get the value of the CDF of a normal distribution, as it is not possible to express it in terms of elementary functions.
    The tables are usually given for the standard normal distribution, where $\varphi(x)$ is the PDF and $\Phi(x)$ is the CDF of the standard normal distribution.
    Usually these table only contains the values of $\Phi(x)$ for $x \geq 0$, to get the value for $x < 0$ we use the symmetry of the normal distribution: $\Phi(x) = 1 - \Phi(-x)$.
\end{remark}

\begin{example}
    Let $X \sim \Normal(167, \sigma = 3)$, we want to find $P(X > 170)$.

    \begin{equation}
        P(X>170) = P(\frac{X - 167}{3} > \frac{170 - 167}{3}) = P(Z > 1) = 1 - P(Z \leq 1)
    \end{equation}
    and from the table we get that $P(Z \leq 1) = 0.8413$, hence $P(X > 170) = 1 - 0.8413 = 0.1587$.
\end{example}

\subsection{Gamma distribution}

Before defining the gamma distribution we need to define the gamma function:
\begin{equation}
    \Gamma(k) = \int_0^{+\infty}x^{k-1}e^{-x} \dd{x}
\end{equation}
this function is constructed in such way to be basically an extension of the factorial function to real numbers.

The gamma distribution has two parameters, $k, \lambda \in \R$, with $k > 0$ and $\lambda > 0$.

\begin{itemize}
    \item PDF:
          \begin{equation}
              f(x) = \frac{\lambda^k}{\Gamma(k)} x^{k-1} e^{-\lambda x} I_{(0, +\infty)}(x)
          \end{equation}
          note that if $k = 1$ we get the exponential distribution.
    \item Expectation:
          \begin{align}
              E(X) & = \int_{-\infty}^{+\infty} x \frac{\lambda^k}{\Gamma(k)} x^{k-1} e^{-\lambda x} I_{(0, +\infty)}(x) \dd{x}               \\
                   & = \int_{0}^{+\infty} \frac{\lambda^k}{\Gamma(k)} x^{k} e^{-\lambda x} \dd{x}                                             \\
                   & = \frac{\Gamma(k+1)}{\Gamma(k) \lambda} \int_{0}^{+\infty} \frac{\lambda^{k+1}}{\Gamma(k+1)} x^{k} e^{-\lambda x} \dd{x} \\
                   & = \frac{k}{\lambda}
          \end{align}
          where we used the propriety of the $\Gamma$ function that $\Gamma(k) = (k-1) \Gamma(k-1)$, and reconstructed the PDF of a gamma distribution with parameters $k+1, \lambda$.

          By the same calculations we can find that $E(X^2) = \frac{k(k+1)}{\lambda^2}$.

    \item Variance:
          \begin{equation}
              V(X) = E(X^2) - E(X)^2 = \frac{k}{\lambda^2}
          \end{equation}
\end{itemize}

\subsubsection{Properties of the gamma distribution}

\begin{theorem}
    Let $X \sim \GammaD(k, \lambda)$, then $Y = \lambda X \sim \GammaD(k, 1)$.
\end{theorem}

\begin{proof}
    Let $g(x) = \lambda x$ such that $Y = g(X)$.
    Then $g^{-1}(y) = \frac{y}{\lambda}$, and $\dv{g^{-1}(y)}{y} = \frac{1}{\lambda}$.

    \begin{align}
        f_Y(y) & = f_X(g^{-1}(y)) \abs{\dv{g^{-1}(y)}{y}}                                                                                           \\
               & = \frac{1}{\Gamma(k)} \left(\frac{y}{\lambda}\right)^{k-1} e^{-y} I_{(0, +\infty)}\left(\frac{y}{\lambda}\right) \frac{1}{\lambda} \\
               & = \frac{1}{\Gamma(k)} y^{k-1} e^{-y} I_{(0, +\infty)}(y)                                                                           \\
    \end{align}
    which is the PDF of a gamma distribution with parameters $k, 1$.
\end{proof}

\begin{theorem}
    Let $X \indep Y$ with $X \sim \GammaD(k_X, \lambda)$ and $Y \sim \GammaD(k_Y, \lambda)$, then $X + Y \sim \GammaD(k_X + k_Y, \lambda)$.
\end{theorem}

\begin{example}
    We have 20 customers per hour, we want to find the waiting time in minutes for the third customer.

    Let $N_{60} \sim \Poisson(60\lambda)$ be the number of customers in an hour, then $\lambda = \frac{1}{3}$.
    We have that $X \sim \GammaD(3, \lambda)$, and the $E(x) = \frac{3}{\lambda} = 9$ minutes.
\end{example}

\begin{theorem}
    Let $Z \sim \Normal(0, 1)$, then $Z^2 \sim \GammaD\left(\frac{1}{2}, \frac{1}{2}\right)$.
\end{theorem}

\begin{proof}
    We start by calculating the CDF of $Z^2$.
    \begin{align}
        P(X \leq x) & = P(Z^2 \leq x) = P(-\sqrt{x} \leq Z \leq \sqrt{x})                  \\
                    & = 2P(0 \leq Z \leq \sqrt{x})                                         \\
                    & = 2\int_0^{\sqrt{x}} \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}} \dd{z} \\
    \end{align}

    Then we differentiate the CDF with respect to $x$ to get the PDF of $Z^2$.

    \begin{align}
        f_X(x) & = \frac{2}{\sqrt{2\pi}} e^{-\frac{x}{2}} \frac{1}{2\sqrt{x}}                                                                          \\
               & = \frac{1}{\sqrt{2\pi}} x^{-\frac{1}{2}} e^{-\frac{x}{2}}                                                                             \\
               & = \left(\frac{1}{2}\right)^\frac{1}{2} \frac{1}{\Gamma\left(\frac{1}{2}\right)} x^{-\frac{1}{2}} e^{-\frac{x}{2}} I_{(0, +\infty)}(x)
    \end{align}
    where we used the propriety of the $\Gamma$ function that $\Gamma\left(\frac{1}{2}\right) = \sqrt{\pi}$.

    This is the PDF of a gamma distribution with parameters $\frac{1}{2}, \frac{1}{2}$.
\end{proof}

\begin{lemma}
    Let $Z_1, \ldots, Z_n \indep \Normal(0, 1)$, then $Z_1^2 + \ldots + Z_n^2 \sim \GammaD\left(\frac{n}{2}, \frac{1}{2}\right)$.
\end{lemma}

\subsection{Cauchy distribution}

The Cauchy distribution looks similar to the Normal distribution even though it goes to $0$ more slowly.

\begin{itemize}
    \item PDF:
          \begin{equation}
              f(x) = \frac{1}{\pi (1 + x^2)}
          \end{equation}
          We see that as $x \to \pm \infty$, $f(x) \sim x^{-2}$, hence $\varphi(x) = \o(f(x))$, where $\varphi(x)$ is the gaussian density.
    \item CDF:
          \begin{equation}
              F(X) = \int_{-\infty}^{x} \frac{1}{\pi(1+u^2)} \dd{u} = \frac{1}{\pi} \left[\arctan u\right]_{-\infty}^x = \frac{1}{2} + \frac{\arctan x}{\pi}
          \end{equation}
    \item Expectation and variance: since $x f(x) \sim x^{-1}$ as $x \to \pm \infty$ the expectation does not exist. All the absolute moments $E(\abs{X}^k)$ are also infinite and and the variance also does not exist.
\end{itemize}

\subsection{Beta distribution}

The beta distribution is used for random variables that take values in $[0, 1]$.
It has two parameters $a, b \gt 0$.

\begin{itemize}
    \item PDF:
          \begin{equation}
              f(x) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}I_{(0,1)}(x)
          \end{equation}
    \item CDF: It can be proven that
          \begin{equation}
              \int_0^1 x^{a -1}(1-x)^{b-1} \dd{x} = \frac{\Gamma(a) \Gamma(b)}{\Gamma(a + b)}
          \end{equation}
          which implies that
          \begin{equation}
              \int_0^1 \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1} \dd{x}= 0
          \end{equation}
    \item Expectation:
          \begin{align}
              E(X) & = \int_0^1 x \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1} \dd{x}       \\
                   & = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \int_0^1 x^a (1-x)^{b-1} \dd{x}           \\
                   & = \frac{\Gamma(a + b) \Gamma(a+1) \Gamma(b)}{\Gamma(a) \Gamma(b) \Gamma(a + 1+ b)} \\
                   & = \frac{a}{a+b}
          \end{align}

          We can similarly compute the second moment to be
          \begin{equation}
              \label{eq:beta:second_moment}
              E(X^2) = \frac{a(a+1)}{(a+b)(a+ b+ 1)}
          \end{equation}
    \item Variance: by using \autoref{eq:beta:second_moment}
          \begin{align}
              V(X) & = \frac{a(a+1)}{(a+b)(a+ b+ 1)} - \frac{a^2}{(a + b)^2}               \\
                   & = \frac{a}{a+ b} \left(\frac{a + 1}{a + b + 1} - \frac{a}{a+b}\right) \\
                   & = \frac{a}{a+b} \frac{b}{(a+b)(a + b + 1)}                            \\
                   & = \frac{ab}{(a+b)^2(a+b+1)}
          \end{align}
\end{itemize}

Note that if $a = b = 1$ then the pdf becomes the pdf of the uniform distribution (see \autoref{eq:uniform:pdf}).
In other words, the uniform distribution on $[0, 1]$ is a special case of the beta distribution.

\section{Joint continuous random variables}

\subsection{Definition}

Let $X, Y$ be continuous random variables.
Then we call $(X, Y)$ a \emph{random vector} of dimension 2 and it takes values on an open subset of $\R^2$.

\begin{definition}[jointly continuous random variables]
    Let $X, Y$ be random variables. $X$ and $Y$ are jointly continuous if there exists a function $f(x, y)$ such that for every Borel set $C \subseteq \R^2$ we have
    \begin{equation}
        P\left((X,Y) \in C\right) = \iint_C f(x, y) \dd{x} \dd{y} \label{eq:joint:def}
    \end{equation}

    The function $f(x,y)$ is called the \emph{joint probability density function (pdf)} of $X$ and $Y$.
\end{definition}

Note that if $C = A \times B$ then \autoref{eq:joint:def} becomes
\begin{equation}
    P(X \in A, Y \in B) = \int_{x \in A} \int_{y \in B} f(x, y) \dd{x} \dd{y}
\end{equation}

\begin{definition}[cumulative distribution function]
    We define the cumulative distribution function (or cdf) as
    \begin{equation}
        F(x, y) = P(X \leq x, Y \leq y) = \int_{-\infty}^x \int_{-\infty}^y f(u, v) \dd{u} \dd{v}
    \end{equation}
\end{definition}

Moreover, if the joint pdf is continuous at $(x, y)$, we have
\begin{equation}
    f(x, y) = \pdv[2]{F(x,y)}{x}{y}
\end{equation}

\subsection{Joint marginal distributions}

\begin{definition}[joint marginal distributions]
    We define the marginal pdf w.r.t. $X$ and w.r.t. $Y$ as
    \begin{align}
        f_X(x) & = \int_{-\infty}^{+\infty} f_{X, Y}(x, y) \dd{y} \\
        f_Y(y) & = \int_{-\infty}^{+\infty} f_{X, Y}(x, y) \dd{x}
    \end{align}
\end{definition}

\subsection{Expectation of functions of random vectors}

We give the following theorems without proof.
On the book you can find the proof for \autoref{thm:joint:linearity_expectation}

\begin{theorem}[transformation of random vectors]
    Let $X, Y$ be random variables, $f(x,y)$ be their pdf, and $g: \R^2 \to \R$ be a measurable function.
    Then $g(X, Y)$ is integrable iff
    \begin{equation}
        \iint_{R^2} \abs{g(x, y)}f(x, y) \dd{x} \dd{y} < \infty
    \end{equation}

    In this case
    \begin{equation}
        E(g(X, Y)) = \iint_{\R^2} g(x, y)f(x, y) \dd{x} \dd{y}
    \end{equation}
\end{theorem}

\begin{remark}
    In general
    \begin{equation}
        E(g(X, Y)) \ne g(E(X), E(Y))
    \end{equation}

    However this becomes true if $g(x, y)$ is linear.
\end{remark}

\begin{theorem}[linearity of the expectation]
    \label{thm:joint:linearity_expectation}

    Let $X, Y$ be integrable jointly continuous random variables.
    Then $\forall a, b \in \R$, $aX + bY$ is integrable and
    \begin{equation}
        E(aX + bY) = aE(X) + bE(Y)
    \end{equation}
\end{theorem}

\subsection{Independence}

\begin{definition}[stochastic independence]
    Let $X, Y$ be random variables.
    We write $X \indep Y$ and we say $X$ and $Y$ are independent if, for every Borel set $A$ and $B$
    \begin{equation}
        P(X \in A, Y \in B) = P(X\in A) \cdot P(Y \in B)
    \end{equation}
\end{definition}

It can be proven that $X \indep Y$ iff the joint pdf can be written as
\begin{equation}
    f_{X, Y} (x, y) = f_X(x)f_Y(y) \label{eq:joint:pdf_indep}
\end{equation}

\begin{theorem}[a criterion for independence]
    Let $X, Y$ be random variables and $f(x,y)$ be their pdf.
    $X \indep Y$ iff the pdf can be written as
    \begin{equation}
        f_{X, Y}(x, y) = g(x)h(y)
    \end{equation}
    in this case we say that $f_{X, Y} (x, y)$ \emph{factorizes}.

    This is a more powerful version of \autoref{eq:joint:pdf_indep}.
\end{theorem}

\begin{proof}
    % TODO: add proof
    TODO.
\end{proof}

\begin{theorem}[transformation of independent variables]
    \label{thm:joint:transformation_independent}

    Let $X, Y$ be random variables such that $X \indep Y$.
    For any measurable $g: \R \to R$ and $h: \R \to R$, $g(X) \indep h(Y)$.
\end{theorem}

\begin{proof}
    % TODO: add proof
    TODO.
\end{proof}

\begin{theorem}[expectation of independent variables]
    Let $X, Y$ be integrable random variables such that $X \indep Y$. Then
    \begin{equation}
        E(XY) = E(X)E(Y)
    \end{equation}
\end{theorem}

\begin{proof}
    % TODO: add proof
    TODO.
\end{proof}

\subsection{Covariance and correlation}

\begin{definition}[covariance]
    \label{def:joint:covariance}
    Let $X, Y$ be square-integrable random variables.
    The covariance of $X$ and $Y$ is defined as
    \begin{equation}
        \cov(X, Y) = E((X - \mu_X)(Y - \mu_Y))
    \end{equation}
\end{definition}

\begin{definition}[correlation coefficient]
    With the context of \autoref{def:joint:covariance} we define the correlation coefficient as
    \begin{equation}
        \rho(X, Y) = \frac{\cov (X, Y)}{\sqrt{V(X)V(Y)}}
    \end{equation}
\end{definition}

% TODO: there's stuff missing here

\begin{proposition}[properties of covariance]
    \label{prop:joint:properties_covariance}

    The properties of covariance are the same as in the discrete case, therefore we will not prove them.

    \begin{enumerate}
        \item $\cov(X, Y) = E(XY) - E(X)E(Y)$
        \item $\cov(X,Y) = \cov(Y,X)$
        \item $\cov(aX + bY, Z) = a \cov(X, Z) + b \cov(Y, Z)$
    \end{enumerate}
\end{proposition}

\begin{proposition}
    We can prove using Cauchy-Schwartz inequality that
    \begin{equation}
        -1 < \rho(X, Y) < 1
    \end{equation}
\end{proposition}

% Class of 10/04/2024

\begin{proposition}[covariance of independent variables]
    Let $X \indep Y$. Then $X - \mu_x \indep Y -\mu_Y$ because of \autoref{thm:joint:transformation_independent}.
    We can apply \autoref{prop:joint:properties_covariance} to get
    \begin{align}
        \cov(X, Y) & = E((X - \mu_X)(Y - \mu_Y))    \\
                   & = E(X - \mu_X) E(Y - \mu_Y)    \\
                   & = (E(X) - \mu_X)(E(Y) - \mu_Y) \\
                   & = 0
    \end{align}
\end{proposition}

\begin{remark}
    While $\cov(X, Y) = 0 \impliedby X \indep Y$, the converse implication is not true.
    We can use $f(x, y) = I_{(0,1)}(x) I_{(1-x, x-1)}(y)$ as a counterexample.
\end{remark}

\subsection{Bivariate normal distribution}

This distribution takes 5 parameters: $\mu_X, \mu_Y, \sigma_X, \sigma_Y, \rho$.

The PDF is
\begin{equation}
    f_{X,Y}(x,y) = \frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1- \rho^2}} e^{-\frac{1}{2(1-\rho^2)} \left[ \left( \frac{x - \mu_X}{\sigma_X} \right)^2 -2 \rho \left( \frac{x-\mu_X}{\sigma_X} \right) \left( \frac{y-\mu_Y}{\sigma_Y} \right) + \left(\frac{y-\mu_Y}{\sigma_Y}\right)^2 \right]}
\end{equation}

The level sets of this function are ellipses and in particular, if $\rho = 0$ we get circles.

We can simplify the pdf by completing the square at the exponent with $\rho ^2 \left(\frac{y - \mu_Y}{\sigma_Y}\right)^2$. We get
\begin{align}
    f_{X,Y}(x,y) & = \frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1- \rho^2}} e^{-\frac{1}{2(1-\rho^2)} \left[ \left( \frac{x - \mu_X}{\sigma_X} - \rho \frac{y-\mu_Y}{\sigma_Y} \right)^2 + (1 - \rho^2) \left(\frac{y-\mu_Y}{\sigma_Y}\right)^2 \right]} \\
                 & = \frac{1}{\sqrt{2 \pi} \sigma_X \sqrt{1- \rho^2}} e^{-\frac{1}{2(1-\rho^2)} \left( \frac{x - \mu_X}{\sigma_X} - \rho \frac{y-\mu_Y}{\sigma_Y} \right)^2} \cdot
    \frac{1}{\sqrt{2 \pi} \sigma_Y} e^{-\frac{1}{2\cancel{(1-\rho^2)}} \cancel{(1 - \rho^2)} \left(\frac{y-\mu_Y}{\sigma_Y}\right)^2}
\end{align}
which is very similar to the gaussian distribution (\autoref{sec:normal_distribution}).
In fact, with some algebra we can show that the marginal distributions are indeed normal distributions:
\begin{align}
    X & \sim \Normal(\mu_X, \sigma_X^2) \\
    Y & \sim \Normal(\mu_Y, \sigma_Y^2)
\end{align}

Moreover, we can also show that $\rho$ is indeed the correlation coefficient:
\begin{align}
    \rho(X, Y) & = \frac{\cov(X, Y)}{\sigma_X \sigma_Y}                                                                                                                                                                                                                                                                                             \\
               & = \frac{E((X - \mu_X)(Y-\mu_Y))}{\sigma_X \sigma_Y}                                                                                                                                                                                                                                                                                \\
               & = E\left(\frac{(X - \mu_X)}{\sigma_X}\frac{(Y-\mu_Y)}{\sigma_Y}\right)                                                                                                                                                                                                                                                             \\
               & = \iint \frac{x - \mu_X}{\sigma_X} \frac{y - \mu_Y}{\sigma_Y} \frac{1}{\sqrt{2 \pi} \sigma_X \sqrt{1- \rho^2}} e^{-\frac{1}{2(1-\rho^2)} \left( \frac{x - \mu_X}{\sigma_X} - \rho \frac{y-\mu_Y}{\sigma_Y} \right)^2} \cdot \frac{1}{\sqrt{2 \pi} \sigma_Y} e^{-\frac{1}{2} \left(\frac{y-\mu_Y}{\sigma_Y}\right)^2} \dd{x} \dd{y} \\
               & = \iint z w\frac{1}{\sqrt{2 \pi} \sqrt{1- \rho^2}} e^{-\frac{1}{2(1-\rho^2)} \left( z - \rho w \right)^2} \cdot \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} w^2} \dd{z} \dd{w}                                                                                                                                                          \\
               & = \int w \frac{1}{\sqrt{2 \pi}}e^{-\frac{1}{2} w^2} \underbrace{\left[ \int z \frac{1}{\sqrt{2 \pi} \sqrt{1- \rho^2}} e^{-\frac{1}{2(1-\rho^2)} \left( z - \rho w \right)^2} \dd{z} \right]}_{\rho w} \dd{w} \label{eq:bivariate:1}                                                                                                \\
               & = \int \rho w^2 \frac{1}{\sqrt{2 \pi}}e^{-\frac{1}{2} w^2} \dd{w}                                                                                                                                                                                                                                                                  \\
               & = \rho \cdot 1 = \rho \label{eq:bivariate:2}
\end{align}
where in \autoref{eq:bivariate:1} we let $Z \sim \Normal(\rho w, 1-\rho^2)$ then we recognize the integral as $E(Z)$;
in \autoref{eq:bivariate:2} we let $W \sim \Normal(0,1)$ then we recognize the integral as $\rho \cdot E(W^2) = V(W) + \left(E(W)\right)^2 = 1 \cdot \rho$.

\begin{remark}
    If $(X, Y) \sim \BivariateNormal$ then $X \indep Y \iff \rho = 0$ (note that this is an if and only if, not only an imply).
\end{remark}

\begin{remark}
    While $(X, Y) \sim \BivariateNormal \implies X \sim \Normal, Y \sim \Normal$ the converse is not generally true.
\end{remark}

\subsection{Conditioning}

\begin{definition}[conditional probability]
    We define the conditional pdf as
    \begin{equation}
        f_{Y \mid X}(y \mid x) = \frac{f_{X, Y}(x, y)}{f_X(x)} \label{eq:joint:conditioning}
    \end{equation}
\end{definition}

This is similar to the definition in the discrete case, but now we need to pass to the limit:
\begin{equation}
    f_{Y \mid X}(y\ mid x) = \lim_{\substack{\Delta x \to 0 \\ \Delta y \to 0}} \frac{P\left(y \leq Y \leq y + \Delta y \bigm| x \leq X \leq x + \Delta x\right)}{\Delta y}
\end{equation}
this can be proven by substituting the definition of $f_{X, Y}(x,y)$ and $f_X(x)$ in \autoref{eq:joint:conditioning}.

% Class of 12/04/24

As we saw in the discrete case \autoref{eq:joint:conditioning} is a pdf as it respects $f_{Y \mid X}(y \mid x) \geq 0$ and $\int f_{Y \mid X}(y \mid x) \dd{y}$.
Note that this is a function of $y$ only, as $x$ is fixed.

Then we have
\begin{equation}
    P(Y \in A \mid X = x) = \int_A f_{Y \mid X}(y \mid x) \dd{y}
\end{equation}
and we can calculate the conditional cdf of $Y$ given $X = x$ as
\begin{equation}
    P(Y \leq y  \mid  X = x) = \int_{-\infty}^y f_{Y \mid X}(u \mid x) \dd{u}
\end{equation}

\subsubsection{Conditional expectation and variance}

\begin{definition}[conditional expectation]
    \begin{equation}
        E(Y \mid X = x) = \int y f_{Y  \mid  X}(y  \mid  x) \dd{y} = m_Y(x)
    \end{equation}
    where by $m_Y(x)$ we mean the regression function of $Y$ over $X$, which is a random variable.
\end{definition}

\begin{theorem}[properties of conditional expectation]
    \label{thm:condition:prop_expectation}
    \begin{align}
         & E(E(Y \mid X)) = E(Y) \label{eq:joint:expectation_1}         \\
         & E(g(X)E(Y \mid X)) = E(g(X)Y) \label{eq:joint:expectation_2}
    \end{align}
\end{theorem}

\begin{proof}
    \skiplineafterproof
    \begin{enumerate}[label=\roman*.]
        \item (\autoref{eq:joint:expectation_1})
              \begin{align}
                  E(E(Y \mid X)) & = E(m_Y(X))                                           \\
                                 & = \int m_Y(x) f_X(x) \dd{x}                           \\
                                 & = \iint y f_{Y \mid X}(y \mid x) \dd{y} f_X(x) \dd{x} \\
                                 & = \iint y f_{X, Y}(x, y) \dd{x} \dd{y}                \\
                                 & = E(Y)
              \end{align}
        \item (\autoref{eq:joint:expectation_2}) The proof is basically the same as above.
    \end{enumerate}
\end{proof}

\begin{definition}[conditional variance]
    \begin{equation}
        V(Y  \mid  X = x) = v_Y(x) = \int \left(y - m_Y(x) \right)^2 f_{Y \mid X}(y \mid x) \dd{y}
    \end{equation}
\end{definition}

\begin{theorem}[properties of the variance]
    \skiplineafterproof
    \begin{enumerate}[label=\roman*.]
        \item \begin{equation}
                  V(Y \mid X) = E(Y^2  \mid  X) - E\left((Y \mid X)\right)^2
              \end{equation}

        \item (variance decomposition)
              \begin{equation}
                  V(Y) = E(V(Y \mid X)) + V(E(Y \mid X))
              \end{equation}
    \end{enumerate}
\end{theorem}

\begin{example}[bivariate normal distribution]
    Given $(X, Y) \sim \BivariateNormal$ we want to compute $f_{Y \mid X}(y \mid x)$.

    As we saw, we can factorize the marginal pdf from the joint pdf of a bivariate normal.
    Then, by the definition of conditional probability we have that
    \begin{equation}
        f_{Y \mid X}(y \mid x) = \frac{1}{\sqrt{2 \pi} \sigma_Y \sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}\left( \frac{y-\mu_y}{\sigma_Y} - \rho \frac{x-\mu_X}{\sigma_X} \right)^2}
    \end{equation}
    which we can recognize as the pdf of a normal distribution:
    \begin{equation}
        Y \mid X \sim \Normal\left(\mu_Y + \rho\frac{\sigma_Y}{\sigma_X}(x - \mu_X), \sigma_Y^2\left(1-\rho^2\right)\right)
    \end{equation}

    Moreover, the regression function is linear:
    \begin{equation}
        m_Y(x) = E(Y \mid X = x) = \mu_Y + \rho \frac{\sigma_Y}{\sigma_X} (x - \mu_X)
    \end{equation}
    and the conditional variance is
    \begin{equation}
        v_Y(x) = \left(1- \rho^2\right) \sigma_Y^2 < \sigma_Y^2
    \end{equation}
\end{example}

\subsection{Transformations of random vectors}

\subsubsection{Transformations preserving dimension}

Let $Z = g(X, Y)$ and $W = h(X, Y)$, therefore we are going from 2 dimensions to 2 dimensions.
We will only consider the case in which $Z, W$ are continuous.
Let $T: \R^2 \to \R^2$ with components $g, h$.

\begin{equation}
    \label{eq:joint:transformation}
    P\left(T(X, Y) \in A\right) = P \left((X, Y) \in T^{-1}(A)\right) = \iint_{(x,y) \in T^{-1}(A)} f_{X, Y}(x, y) \dd{x} \dd{y}
\end{equation}

\begin{theorem}
    Let $X, Y$ be jointly continuous random variables taking values in an open set $U \subset R^2$ and with joint pdf $f_{X, Y}(x, y)$.
    Let $T:U \to V$ be a one-to-one mapping and let $T^{-1}$ be its inverse functions.
    Assume that $T^{-1}$ has continuous partial derivatives in $V$ and that the determinant of the Jacobian matrix of $T^{-1}$ is $\ne 0 \enspace \forall (z, w) \in V$.

    Then $(Z, W)$ is a continuous random vector with pdf
    \begin{equation}
        f_{Z, W}(z, w) = f_{X, Y}\left(T^{-1}(z, w)\right) \abs{\det J_{T^{-1}}(z, w)} I_V(z, w)
    \end{equation}
\end{theorem}

\begin{proof}
    The result follows from a change of variable in \autoref{eq:joint:transformation}.
    We won't prove it as we haven't seen it in Analysis yet.
\end{proof}

\subsubsection{Transformations with less dimensions}

If we are given just one transformation $Z = g(X, Y)$ doing the full procedure with double integrals may be more complicated than necessary.
Instead we can expand the transformation with some $W = h(X, Y)$ and then marginalize:
\begin{equation}
    f_Z(z) = \int f_{Z, W}(z, w) \dd{w}
\end{equation}
we have to choose $W$ in such a way that helps us do the calculations.

\begin{theorem}
    Let $X, Y$ be continuous random variables with joint pdf $f_{X, Y}(x, y)$ and let $Z = X + Y$.
    Then
    \begin{equation}
        f_Z(z) = \int_{-\infty}^{+\infty} f_{X, Y} (x, z-x) \dd{x}
    \end{equation}

    In particular, if $X \indep Y$
    \begin{equation}
        f_Z(z) = \int_{-\infty}^{+\infty} f_X(x) f_Y(z-x) \dd{x} = f_X \ast f_Y
    \end{equation}
\end{theorem}

\begin{proof}
    We get this result by applying what we wrote above where $Z = X + Y$ and $W = X$

    TODO: formalize this.
\end{proof}

% Class of 17/04/2024
\subsection{Mixing continuous and discrete}

Consider the case in which we know $f_X(x)$ and $f_{Y \mid X}(y \mid x)$.
Then we can write the joint probability as
\begin{equation}
    P(X\in A, Y \in B) = \int_A f_X(x) \left[\int_B f_{Y \mid X}(y \mid x) \dd{y} \right] \dd{x} = \int_A f_X(x) P(Y\in B \mid  X = x)
\end{equation}
just by applying the definition of conditioning and Fubini's theorem.

Applying the same reasoning we can get that this result holds even in the case of a general surface $C$, therefore
\begin{equation}
    P((X, Y) \in C) = \int P(Y \in C_x  \mid  X = x) f_X(x) \dd{x}
\end{equation}
where $P(Y \in C_x  \mid  X = x) = P((X, Y) \in C  \mid  X = x)$.

This holds even if $Y \mid X = x$ is discrete and $X$ is continuous (\autoref{eq:mixing:x_cont}), or if $X$ is discrete and $Y \mid X = x$ is continuous (\autoref{eq:mixing:x_disc}).
\begin{align}
    P(X \in A, Y \in B) & = \int_A \left[\sum_{y \in B} f_{Y  \mid  X}(y \mid  x)\right] f_X(x) \dd{x} \label{eq:mixing:x_cont} \\
    P(X \in A, Y \in B) & = \sum_{x \in A} \left[ \int_B f_{Y \mid X}(y \mid x) \dd{y} \right] f_X(x) \label{eq:mixing:x_disc}
\end{align}

Moreover, the proprieties of the conditional expectation (\autoref{thm:condition:prop_expectation}) are still valid in both cases.

\subsection{Random vectors}

We define a random vector as $\vec X =
    \begin{bmatrix}
        X_1    \\
        \vdots \\
        X_k
    \end{bmatrix}$
and $f_{\vec X}(\vec x)$ its pdf.
This notation is valid for discrete and continuous random variables.

To calculate the probability $P(\vec X \in A)$ we use the usual definition for the discrete case while in the continuous case we just stack $k$ integrals one inside the other:
\begin{equation}
    P(\vec X \in A = \idotsint f_{\vec X}(x_1, \ldots, x_k) \dd{x_1} \ldots \dd{x_k}
\end{equation}

The joint cdf will be
\begin{equation}
    F_{\vec X}(\vec x) = P(X_1 \leq x_1, \ldots X_k \leq x_k)
\end{equation}
and independence will be
\begin{equation}
    f_{\vec X} = \prod_{i = 1}^{k} f_{X_i}(x_i)
\end{equation}

The expectation vector is defined as
\begin{equation}
    E(X) = \begin{bmatrix}
        E(X_1) \\
        \vdots \\
        E(X_k)
    \end{bmatrix} = \vec \mu
\end{equation}

The variance is a matrix defined as
\begin{equation}
    V(\vec X) = \begin{bmatrix}
        V(X_1)         & \cov(X_1, X_2) & \dots  & \cov(X_1, X_k) \\
        \cov(X_2, X_1) & V(X_2)         &        & \vdots         \\
        \vdots         &                & \ddots &                \\
        \cov(X_k, X_1) & \dots          &        & V(X_k)
    \end{bmatrix} = \Sigma
\end{equation}
which is sometimes called the covariance matrix, as it contains all the covariance (recall that $\cov(X_i, X_i)= V(X_i)$).
Also note that $\Sigma$ is symmetric.

We can also arrange random variables in a matrix, giving us a random matrix.
Its expectation is
\begin{equation}
    E\left(\begin{bmatrix}
        X_{11} & \dots  & X_{1n} \\
        \vdots & \ddots & \vdots \\
        X_{k1} & \dots  & X{kn}
    \end{bmatrix}\right) =
    \begin{bmatrix}
        E(X_{11}) & \dots  & E(X_{1n}) \\
        \vdots    & \ddots & \vdots    \\
        E(X_{k1}) & \dots  & E(X{kn})
    \end{bmatrix}
\end{equation}

\begin{proposition}[variance matrix]
    \label{prop:gen:variance_mat}
    Let $\vec X =
        \begin{bmatrix}
            X_1    \\
            \vdots \\
            X_k
        \end{bmatrix}$ and $\vec \mu = E(\vec X)$.
    Then
    \begin{equation}
        V(\vec X) = E \left( \left(\vec X - \vec \mu\right) \left(\vec X - \vec \mu\right)^T \right)
    \end{equation}
\end{proposition}

\begin{proof}
    This just comes from the fact that $E((X_i - \mu_i)(X_j - \mu_j)) = \cov(X_i, X_j)$ (\autoref{def:joint:covariance}).
    Then each element of the resulting matrix is the covariance of $X_i, X_j$.
\end{proof}

\begin{theorem}[linearity of expectation matrix]
    Let $\vec X$ be a random matrix and $A, B$ be matrices of numbers.
    Then
    \begin{equation}
        E(A \vec X + B) = A E(\vec X) + B
    \end{equation}
\end{theorem}
\begin{proof}
    This is immediate from iterating the linearity of expectation on the values of the matrix.
    (Actually there's a bit more to it but you can read it on the book.)
\end{proof}

\begin{theorem}[proprieties of variance]
    Let $\vec X$ be a random vector, $A$ be a matrix, and $\vec b$ be a vector.
    Then
    \begin{align}
        V(\vec X)            & = E(\vec X \vec X^T) - \vec \mu \vec \mu^T \\
        V(A \vec X + \vec b) & = A V(\vec X) A^T
    \end{align}
\end{theorem}

\begin{proof}
    These properties both come from expanding \autoref{prop:gen:variance_mat} and performing some linear algebra properties such as $(AB)^T = B^T A^T$.
    I will not write the full proof here, check the book.
\end{proof}

\begin{proposition}[sign of the variance]
    Let $\vec X$ be a random vector and $\vec a$ a number vector.
    We know that
    \begin{equation}
        V(\vec a^T \vec X) = \vec a^T V(\vec X) \vec a \geq 0 \quad \forall \vec a
    \end{equation}
    since the variance is always positive.

    We say that the variance matrix is always \emph{positive semi-definite}.
    Moreover, $V(\vec X)$ is not positive definite only if there exists some vector $\vec a$ such that $\vec a^T \vec X = c$ constant and $P(\vec a^T \vec X) \ne 1$.
\end{proposition}

\begin{proposition}
    Let $\vec X$, $U \subseteq \R^k$ open, $P(\vec X \in U) = 1$, $T: U \to V$ be a one-to-one \emph{linear} transformation such that $T^{-1}$ is its inverse and $T^{-1}$ is of class $C^1$, and $\det J_{T^{-1}}(\vec y) \ne 0$ for all $\vec y \in V$.

    Then we know that in general
    \begin{equation}
        f_{\vec Y} = f_{\vec X}(T^{-1}(\vec y)) \abs{\det J_{T^{-1}}(\vec y)}
    \end{equation}
    but since $T$ is linear we can find its matrix $A$, and $J_{T^{-1}}(\vec y) = A^{-1} \ne 0$.
    This means that we can rewrite the general equation as
    \begin{align}
        f_{\vec Y} & = f_{\vec X}(A^{-1}(\vec y - \vec b)) \abs{\det A^{-1}}      \\
                   & = f_{\vec X}(A^{-1}(\vec y - \vec b)) \frac{1}{\abs{\det A}}
    \end{align}
\end{proposition}

\subsubsection{Multivariate normal distribution}

We write $\Normal(\vec \mu, \Sigma)$, where $\vec \mu \in \R^d$ and $\Sigma$ is a positive definite, symmetric $k \times k$ matrix.

The pdf of this distribution is
\begin{equation}
    f_{\vec X}(\vec x) = (2 \pi)^{-\frac{k}{2}} (\det \Sigma)^{-\frac{1}{2}} e^{-\frac{1}{2}(\vec x - \vec \mu)^T \Sigma^{-1} (\vec x - \vec \mu)}
\end{equation}

We call $\Normal(\vec 0, I)$ the \emph{standard multivariate normal} and its pdf becomes
\begin{align}
    f_{\vec X}(\vec x) & = (2 \pi)^{-\frac{k}{2}} (1)^{-\frac{1}{2}} e^{-\frac{1}{2}(\vec x - \vec \mu)^T (\vec x - \vec \mu)} \\
                       & = \frac{1}{(\sqrt{2 \pi})^k} e^{-\frac{1}{2} \sum_{i = 1}^k x_i^2}                                    \\
                       & = \prod_{i = 1}^k \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x_i^2}
\end{align}
that is, $X_1, \ldots, X_k \indep$ and $X_i \sim \Normal(0,1)$.

\begin{lemma}
    $X_1, \ldots, X_k \indep$ if and only if $\Sigma$ is diagonal.
\end{lemma}

\begin{proposition}
    Let $\vec X \sim \Normal(\vec \mu, \Sigma)$ and $\vec Y = A \vec X + \vec b$.
    Then
    \begin{equation}
        \vec Y \sim \Normal(A \vec \mu + \vec b, A \Sigma A^T)
    \end{equation}
\end{proposition}

\begin{proof}
    \begin{align}
        f_{\vec Y}(\vec y) & = (2 \pi)^{-\frac{k}{2}} (\det \Sigma)^{-\frac{1}{2}} e^{-\frac{1}{2}(A^{-1}(\vec y - \vec b) - \vec \mu)^T \Sigma^{-1} (A^{-1}(\vec y - \vec b) - \vec \mu)} \frac{1}{\abs{\det A}} \\
                           & = \frac{1}{\sqrt{(\det \Sigma)(\det A)(\det A)}} e^{-\frac{1}{2}(A^{-1}(\vec y - \vec b) - \vec \mu)^T \Sigma^{-1} (A^{-1}(\vec y - \vec b) - \vec \mu)} \frac{1}{\abs{\det A}}      \\
                           & = \frac{1}{\sqrt{\det(A \Sigma A^T)}} e^{-\frac{1}{2}(A^{-1}(\vec y - \vec b) - \vec \mu)^T (A \Sigma A^T)^{-1} (\vec y - \vec b) - \vec \mu)}
    \end{align}
\end{proof}

\begin{proposition}[properties of the multivariate normal]
    \label{prop:multivariate:prop}
    Define $\begin{bmatrix} \vec X \\ \vec Y \end{bmatrix}$ as
    \begin{equation}
        \begin{bmatrix}
            \vec X \\ \vec Y
        \end{bmatrix}
        =
        \begin{bmatrix}
            X_1 \\ \vdots \\ X_k \\ Y_1 \\ \vdots\\ Y_j
        \end{bmatrix}
        \sim \Normal\left(
        \begin{bmatrix}
            \vec \mu_x \\ \vec \mu_y
        \end{bmatrix},
        \begin{bmatrix}
            \Sigma_{XX} & \Sigma_{XY} \\
            \Sigma_{YX} & \Sigma_{YY}
        \end{bmatrix}
        \right)
    \end{equation}

    We give the following properties without proof:
    \begin{enumerate}[label=\roman*.]
        \item \begin{align}
                  X & \sim \Normal(\vec \mu_x, \Sigma_{XX}) \\
                  Y & \sim \Normal(\vec \mu_y, \Sigma_{YY})
              \end{align}
        \item \begin{equation}
                  \left( \vec Y \mid \vec X = \vec x \right) \sim \Normal \left( \vec \mu_Y + \Sigma_{YX} \Sigma_{XX}^{-1}(\vec x - \vec \mu x), \Sigma_{YY} \Sigma_{YX} \Sigma_{XX}^{-1} \Sigma_{XY} \right)
              \end{equation}
    \end{enumerate}
\end{proposition}

\begin{proposition}
    Let $\vec X \sim \Normal(\vec \mu, \Sigma)$ of length $k$ and $Y = A \vec X + \vec b$ where $A$ is a non-singular $k \times h$ matrix with $h < k$.
    Then
    \begin{equation}
        \vec Y \sim \Normal(A \vec \mu + \vec b, A \Sigma A^T)
    \end{equation}
\end{proposition}

\begin{proof}
    Since the first $h$ rows are linearly independent we can construct a space of dimension $k$ and complete the matrix with rows $k-h$ such that all of the rows are independent.

    Then we can write
    \begin{equation}
        \begin{bmatrix}
            \vec Y \\ \vec W
        \end{bmatrix} =
        \begin{bmatrix}
            A \\ B
        \end{bmatrix} \vec X +
        \begin{bmatrix}
            \vec b \\ \vec o
        \end{bmatrix}
    \end{equation}
    such that $\begin{bmatrix}
            A \\ B
        \end{bmatrix}$ is non-singular and applying \autoref{prop:multivariate:prop} we get the result.
\end{proof}

\begin{definition}[standard multivariate normal distribution]
    If $\vec \mu = \vec 0$ and $\Sigma = I$ we say that $\Normal(\vec \mu, \Sigma)$ is standard.
\end{definition}

\begin{proposition}[standardizing]
    If $\Sigma$ is positive definite we can find a $\Sigma^{\frac{1}{2}}$ symmetric such that $\Sigma = \Sigma^{\frac{1}{2}} \Sigma^{\frac{1}{2}}$ and $\left(\Sigma^{\frac{1}{2}}\right)^{-1} = \Sigma^{-\frac{1}{2}}$.

    Let $\vec Z = \Sigma^{-\frac{1}{2}} (\vec X -\vec \mu)$, then $\vec Z \sim \Normal(\vec 0, I)$.
    Moreover each $Z_i \sim \Normal(0, 1)$.
\end{proposition}

\begin{proof}
    For the expectation we have
    \begin{equation}
        E(\vec Z) = E(\Sigma^{-\frac{1}{2}} (\vec X -\vec \mu)) = \Sigma^{-\frac{1}{2}} E(\vec X -\vec \mu) = \vec 0
    \end{equation}
    and for the variance
    \begin{align}
        V(\vec Z) & = V(\Sigma^{-\frac{1}{2}} \vec X - \cancel{\Sigma^{-\frac{1}{2}} \vec \mu})                 \\
                  & = \Sigma^{-\frac{1}{2}} V(\vec X) \Sigma^{-\frac{1}{2}}                                     \\
                  & = \Sigma^{-\frac{1}{2}} \Sigma \Sigma^{-\frac{1}{2}}                                        \\
                  & = \Sigma^{-\frac{1}{2}} \Sigma^{\frac{1}{2}} \Sigma^{\frac{1}{2}} \Sigma^{-\frac{1}{2}} = I
    \end{align}

    Note that $\Sigma^{\frac{1}{2}}$ may not be unique.
\end{proof}

\section{Moment generating function}

\begin{definition}[moment generating function]
    Let $X$ be a random variable.
    Let $I = \{t \in \R : E(e^{tX}) < + \infty \}$.

    Then we can define $M_X(t): I \to \R$ as
    \begin{equation}
        M_X(t) = E(e^{tX})
    \end{equation}
    as a function of $t$.
    Note that this expectation always exist.

    In every case $0 \in I$ and it can be proven that $I$ is convex.
\end{definition}

\subsubsection{MGF of noteworthy distributions}

For the proof see the book.
\begin{itemize}
    \item ($X \sim {\operatorfont Bernoulli}(p)$)
          \begin{align}
              M_X(t) & = e^{0t} P(X = 0) + e^{1t} P(X = 1) \\
                     & = q + e^t p
          \end{align}
          where $I = \R$
    \item ($X \sim {\operatorfont Binomial}(n, p)$)
          \begin{equation}
              M_X(t) = (1-p + p e^t)^n
          \end{equation}
          where $I = \R$
    \item ($X \sim \Poisson(\lambda)$)
          \begin{equation}
              M_X(t) = e^{\lambda (e^t -1)}
          \end{equation}
          where $I = \R$
    \item ($X \sim \Gamma(k, \lambda)$)
          \begin{equation}
              M_X(t) = \frac{\lambda^k}{(\lambda - t)^k}
          \end{equation}
          where $I = (-\infty, \lambda)$
    \item ($X \sim \Normal(\mu, \sigma)$)
          \begin{equation}
              M_X(t) = e^{t \mu + \frac{t^2 \sigma^2}{2}}
          \end{equation}
          where $I = \R$
\end{itemize}

\subsection{Properties of the MGF}

\begin{align}
    E(e^{tX}) & = E(\sum_{k = 0}^\infty \frac{(tX)^k}{k!})    \\
              & = \sum_{k = 0}^{\infty} \frac{t^k E(X^k)}{k!} \\
              & = \sum_{k = 0}^{\infty} \frac{t^k m_k}{k!}
\end{align}

Note that this only works if we can \say{take out} the expectation from the sum.

\begin{theorem}
    If $\exists a > 0$ such that $(-a, a) \subset I$, then $E(\abs{X}^k) < +\infty$ $\forall k$ and
    \begin{equation}
        \label{eq:mgf:taylor}
        M_X(t) = \sum_{k = 0}^{\infty} \frac{t^k m_k}{k!}
    \end{equation}
    for all $t \in (-a, a)$.

\end{theorem}

Moreover, the coefficient of the power series are unique by the properties of the power series itself.
With these properties we can compute all the $k$ moments for any distribution.

Moreover, we see that \autoref{eq:mgf:taylor} is the taylor expansion of $M_X(t)$ centered at $0$. Then $m_k = M^{(k)}(0)$.

\begin{proposition}[the mgf characterizes the distribution]
    If $I \subset (-a, a)$ then there is a one-to-one mapping between $F_X$ (the cdf of $X$) and $M_X$.

    This means that if $M_X = M_Y$ then $X$ and $Y$ have the same distribution.
    We can say that the $M_X$ characterize the distribution of $X$.
\end{proposition}

\begin{proposition}[properties of the mgf]
    \skiplineafterproof
    \begin{enumerate}
        \item $M_{aX + b}(t) = e^{tb} M_X (at)$
        \item If $X \indep Y$ then $M_{X+Y}(t) = M_X(t) M_Y(t)$
    \end{enumerate}
\end{proposition}
\begin{proof}
    These follow quite easily from the substituting into the definition and the properties of the expectation.
    See the book for the full proof.
\end{proof}

\end{document}