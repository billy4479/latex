\documentclass[10pt]{extarticle}
\title{Computer Science 2 Notes}
\author{Giacomo Ellero}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{dirtytalk}
\usepackage{parskip}
\usepackage{mathrsfs}
\usepackage[many]{tcolorbox}
\usepackage{xparse}
\usepackage[a4paper,margin=1.5cm]{geometry}
\usepackage{bookmark}
\usepackage{bytefield}
\usepackage{minted}

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\newenvironment{absolutelynopagebreak}
  {\par\nobreak\vfil\penalty0\vfilneg
   \vtop\bgroup}
  {\par\xdef\tpd{\the\prevdepth}\egroup
   \prevdepth=\tpd}

\newtcolorbox{examplebox}[1]{colback=green!5!white,colframe=green!40!black,title={#1},fonttitle=\bfseries,parbox=false}
\newtcolorbox{notebox}[1]{colback=green!5!white,colframe=blue!40!black,title={Note: #1},fonttitle=\bfseries,parbox=false}
\newtcolorbox{bluebox}[1]{colback=green!5!white,colframe=blue!40!black,title={#1},fonttitle=\bfseries,parbox=false}

\begin{document}

\maketitle
\tableofcontents
\clearpage

\section{Class of 06/02/2024}

\subsection{Asymptotic notation}

\begin{bluebox}{Definition}
    If $\exists C \in \R^+$ and $N \in \N$ such that for two sequences $a_n, b_n > 0$
    we have that $a_n \leq Cb_n$ for all $n \geq N$, then we write $a_n = O(b_n)$ and we read \say{$a_n$ is big O of $b_n$}.
\end{bluebox}

These sequences describe the time it takes for an algorithm to solve a certain problem of size $n$.

\begin{examplebox}{Example}
    Let $a_n = n^2 + 2n + 1$. We will prove that $a_n = O(n^2)$.

    \begin{proof}
        We have that
        \begin{align*}
            a_n & = n^2 + 2n + 1        \\
                & \leq n^2 + 2n^2 + n^2 \\
                & = 4n^2
        \end{align*}
    \end{proof}

    As we can see just need to show that $C$ exists, we don't need to find the best one.
\end{examplebox}

Usually we don't use limits to prove that a sequence is big O of another, it is usually more convenient to proceed by inequalities.

\begin{notebox}{Logarithms}
    When we have sequences with logarithms we don't need to specify the basis,
    as the logarithm is a constant factor of another logarithm by the change of basis formula.
\end{notebox}

\subsubsection{Operations and other notation}

Let $a_n = O(c_n)$ and $b_n = O(d_n)$, then we have that

\begin{itemize}
    \item $a_n + b_n = O(\max\{c_n, d_n\})$
    \item $a_n \cdot b_n = O(c_n \cdot d_n)$
\end{itemize}

We also define the \say {opposite} of big O notation, the $\Omega$ notation:
$a_n$ is $\Omega(b_n)$ if $a_n \geq Cb_n$ for all $n \geq N$.

Moreover, if $a_n = O(b_n)$ and $a_n = \Omega(b_n)$ (for some different $C$ and $N$) then we write $a_n = \Theta(b_n)$.

\subsection{Randomness}

When we run an experiment we can define
\begin{itemize}
    \item an outcome $\omega_i$
    \item the value of the outcome $x_i$ (similar to a bet)
    \item the probability of the outcome $p_i$
\end{itemize}

We can also define the expected result $E(X)$ of the experiment as
$$
    E(X) = \sum ^n _{i = 1} x_i p_i
$$

\subsection{IEEE-754}

This is not in the syllabus but it's useful to know.

IEEE-754 is a standard for representing floating point numbers in computers.
We will discuss in particular the 32-bits representation but larger or smaller formats also exist.

We subdivide the 32 bits in 3 parts: 1 bit for the \textbf{sign}, 8 bits for the \textbf{exponent}, and 23 bits for the \textbf{fraction} or mantissa.

\begin{center}
    \begin{bytefield}[bitwidth=1.1em, bitheight=\widthof{~Sign~}]{32}
        \bitheader{0,8,31} \\
        \bitbox{1}{1}
        \bitboxes{1}{00100101}
        \bitboxes{1}{00110110101101001111011} \\
        \bitbox{1}{\rotatebox{90}{Sign}}
        \bitbox{8}{Exponent}
        \bitbox{23}{Fraction}
    \end{bytefield}
\end{center}

The number is interpreted according to the following formula:

$$
    n = (-1)^s \cdot 2^{e - 127} \cdot (1 + f)
$$

Basically the number is represented in scientific notation, in base 2.

Note that when converting the fraction part to decimal the powers of 2 are negative and decreasing
(i.e.: bit at 9 is $2^{-1}$, bit at 10 is $2^{-2}$, etc).

\section{Class of 08/02/2024}

\subsection{Introductory statements}

We will start by stating the following facts without providing a proof (they can be proven but in class we skipped them).

\begin{itemize}
    \item If $c \in \R^+$, then $g(n) = 1 + c + c^2 + \dots + c^n$ is $\Theta(1)$ if $c < 1$, $\Theta(n)$ if $c = 1$, and $\Theta(c^n)$ if $c > 1$.
    \item In any base $b \ge 2$ the sum of any 3 single-digit numbers is at most 2 digits long.
    \item $\forall n \in \N$ and any base $b$ there exists a power of $b$ in $[n, bn].$
\end{itemize}

\begin{notebox}{$\Theta(1)$}
    When we say that a function is $\Theta(1)$ we mean that it is bounded from above by a constant $C$ (since it is big O of 1)
    and bounded from below by a constant $c$ (since it is $\Omega(1)$).

    This usually means that $a_n \ne 0$ for all $n$.
\end{notebox}

\subsection{How to approach a algorithm problem}

We usually proceed by following these steps:
\begin{enumerate}
    \item Find an algorithm
    \item Prove that the algorithm is correct
    \item Calculate the time complexity of the algorithm
          \begin{enumerate}
              \item Can we do better?
          \end{enumerate}
\end{enumerate}

We will use the following steps to approach some classic problems such as addition and multiplication of binary numbers.

\begin{notebox}{Shifts}
    We refer to shifts as the operation of moving all the bits of a number to the left or to the right by a certain amount of positions.
    This operation mathematically corresponds to multiplying or dividing the number by a power of the base.

    $$
        x \ll n = x \cdot b^n
    $$

    The $\ll$ symbol denotes a left shift by $n$ positions, while the $\gg$ symbol denotes a right shift by $n$ positions.

    In our mathematical world each shift is $O(n)$, in the real world it is usually $O(1)$.
\end{notebox}

\subsection{Addition}

\underline{Problem}: Let $x, y$ be binary numbers of $n$ bits. We want to compute $x + y$.

We proceed by using the normal addition algorithm: this is a well know algorithm that we know is correct.

At each step we are adding at most 3 numbers (2 bits and a carry).
We know that the result of each step is at most 2 bits long: one bit goes for the result and the other goes for the carry.
This ensures that at the next step we will also be adding at most 3 numbers, hence each step is performed in a finite amount of time.

Since we are performing $n$ steps, the time complexity of this algorithm is $O(n)$.

\textit{Can we do better?} No, we can't, since we need to read all the bits of the input and this operation is already $O(n)$.

\subsection{Multiplication}

\underline{Problem}: Let $x, y$ be binary numbers of $n$ bits. We want to compute $x \cdot y$.

Again, we proceed using the normal multiplication algorithm that we know is correct.
The algorithm has the following parts:
\begin{enumerate}
    \item Write the multiplications of $x$ by each bit of $y$
    \item Sum all the results
\end{enumerate}

\textbf{Part 1}: Let $(s_n)$ be the sequence of the shifted values of $x$ and $p_n$ be the sequence of the partial products.

\begin{align*}
    s_0 & = x                         \\
    s_n & = s_{n-1} \ll 1             \\
    p_n & = \begin{cases*}
                0   & \text{if } y[n] = 0 \\
                s_n & \text{if } y[n] = 1
            \end{cases*}
\end{align*}

Choosing the right $p_n$ is $O(1)$, but computing $s_n$ is $O(n)$ hence this part is $O(n)$.

Note that we are \say{storing} the result of the previous shifts, so if we want to compute $s_5$, for example, we don't need to compute $s_4$ again.
Without this optimization the time complexity of this part would be $O(n^2)$.

\textbf{Part 2}:
In this step we are computing $\sum_{i = 0}^n p_n$.
These are sums of numbers of at most $2n$ bits, hence this part is $O(n)$.

\textbf{Conclusion}: The time complexity of the multiplication algorithm is $O(n) \cdot O(n) = O(n^2)$.

\subsubsection{Egyptian Multiplication}

This is an ancient algorithm that is used to multiply two numbers.
It works as follows:

\begin{enumerate}
    \item Write the two numbers in two columns
    \item Divide the first number by 2, floor the result and write it underneath it in the same column
    \item Multiply the second number by 2 and write the result underneath it in the same column
    \item When the first number is 1, sum all the numbers in the second column if the corresponding number in the first column is odd
\end{enumerate}

We can easily implement this algorithm in Python as follows:
\begin{minted}{python}
    def egyptian_multiplication(x, y):
        result = 0
        while x >= 1:       # This loop runs n times since x has n bits
            if x % 2 == 1:  # We always consider the worst case, hence this is always true
                result += y # Sums are O(n)
            x = x >> 2      # Shifts are O(n)
            y = y << 2      # Shifts are O(n)
        return result
\end{minted}

The complexity of the algorithm is $(O(n) + O(n) + O(n)) \cdot O(n) = O(n^2)$.

\subsubsection{Other multiplication algorithms}

Another algorithm we can use to implement multiplication works by dividing each number in half and recursively computing the result.

First we write the two numbers as
\begin{align*}
    x & = 2^{\frac{n}{2}}x_{\text{up}} + y_{\text{low}} \\
    y & = 2^{\frac{n}{2}}y_{\text{up}} + y_{\text{low}}
\end{align*}

Where $x_{\text{up}}$ and $y_{\text{up}}$ are the upper halves of $x$ and $y$ and $x_{\text{low}}$ and $y_{\text{low}}$ are the lower halves of $x$ and $y$.

Then we compute the result as
$$
    x \cdot y = 2^n x_{\text{up}}y_{\text{up}} + 2^{\frac{n}{2}}(x_{\text{up}}y_{\text{low}} + x_{\text{low}}y_{\text{up}}) + x_{\text{low}}y_{\text{low}}
$$

Each time we are performing 4 multiplication of half the size and adding them together.
The time this algorithm takes is
$$
    T(n) = 4T\left(\frac{n}{2}\right) + O(n)
$$

However if we keep expanding the recursion we will see that eventually we get to $T(1)$ which is $O(1)$
and we are left with $n$-many $O(n)$ terms, hence the time complexity of this algorithm is also $O(n^2)$.

\section{Class of 09/02/2024}

\subsection{Karatsuba's algorithm}

This is a multiplication algorithm that is based on the last algorithm we discussed in the last class with some variations.

The algorithm takes inspiration from the multiplication of two complex numbers:
\begin{align*}
    (a + bi)(c + di) & = (ac - bd) + (ad + bc)i   \\
    \implies bc + ad & = (a + b)(c + d) - ac - bd
\end{align*}

Applying this to the multiplication of two binary numbers we get
\begin{align*}
    x \cdot y & = x_{\text{up}} y_{\text{up}} 2^n
    + 2^\frac{n}{2}((x_{\text{up}} + x_{\text{low}})(y_{\text{up}} + y_{\text{low}}) - x_{\text{up}}y_{\text{up}} - x_{\text{low}}y_{\text{low}}) + x_{\text{low}}y_{\text{low}}
\end{align*}

By doing this we are performing 3 multiplications of half the size and 4 additions.
Since we saw how additions are faster than multiplications we can expect this algorithm to be faster than the previous one.

$$
    T(n) = 3T\left(\frac{n}{2}\right) + O(n)
$$

We can arrange the work in nodes of a tree:

\begin{center}
    \begin{tabular}{ |c|c|c|c| }
        \hline
        level  & nodes  & work per node    & total work                      \\
        \hline
        0      & 1      & $Cn$             & $Cn$                            \\
        1      & 3      & $\frac{n}{2}C$   & $\frac{3}{2}Cn$                 \\
        2      & 9      & $\frac{n}{4}C$   & $\frac{9}{4}Cn$                 \\
        \vdots & \vdots & \vdots           & \vdots                          \\
        $k$    & $3^k$  & $\frac{n}{2^k}C$ & $\left(\frac{3}{2}\right)^k Cn$ \\
        \hline
    \end{tabular}
\end{center}

We have that the total work is the sum of the total work at each level of the tree:
$$
    Cn\left(1+\frac{3}{2}+\left(\frac{3}{2}\right)^2+\dots+\left(\frac{3}{2}\right)^k\right)
$$

Compared to the \say{old} algorithm that has a total work that looks like

$$
    Cn\left(1+2+2^2+\dots+2^k\right)
$$

Through some algebra we can show that this sum is $3Cn^{\log_{2}3}$,
hence proving that the algorithm is $O(n^{\log_{2}3}) \sim O(n^{1.58})$.

\subsection{The master theorem}

\underline{Statement}: If $a > 0, b > 1, d \ge 0$, $n$ is a power of $b$, and
$T(n)$ is defined by induction as
\begin{align*}
    T(1) & = 1                                   \\
    T(n) & = aT\left(\frac{n}{b}\right) + O(n^d)
\end{align*}

Where
\begin{itemize}
    \item $a$ is the number of subproblems
    \item $b$ is the factor by which the problem size is reduced at each step
    \item $d$ is the exponent in the work done at each level
\end{itemize}

Then

\begin{align*}
    T(n) = \begin{cases}
               O(n^d)          & \text{if } d > \log_b a \\
               O(n^d \log n)   & \text{if } d = \log_b a \\
               O(n^{\log_b a}) & \text{if } d < \log_b a
           \end{cases}
\end{align*}

\begin{proof}
    We can write a table with the work to be done at each size of the problem

    \begin{center}
        \begin{tabular}{ |c|c|c|c| }
            \hline
            level  & \# of problems & size of each problem & total work                         \\
            \hline
            0      & 1              & $n$                  & $Cn^d$                             \\
            1      & $a$            & $\frac{n}{b}$        & $Ca\left(\frac{n}{b}\right)^d$     \\
            2      & $a^2$          & $\frac{n}{b^2}$      & $Ca^2\left(\frac{n}{b^2}\right)^d$ \\
            \vdots & \vdots         & \vdots               & \vdots                             \\
            $k$    & $a^k$          & $\frac{n}{b^k}$      & $Ca^k\left(\frac{n}{b^k}\right)^d$ \\
            \hline
        \end{tabular}
    \end{center}

    To find the total work we need to evaluate the following sum
    $$
        Cn^d\left(1+\left(\frac{a}{b^d}\right)+\left(\frac{a}{b^d}\right)^2+\dots+\left(\frac{a}{b^d}\right)^k\right)
    $$

    We can consider the following 3 cases:
    \begin{itemize}
        \item If $\frac{a}{b^d} = 1$, then $d = \log_b a$ and the sum becomes $ ??? $ and its complexity is $O(n^d \log n)$
        \item If $\frac{a}{b^d} < 1$, then $d > \log_b a$ and the sum becomes $ ??? $ and its complexity is $O(n^d)$
        \item If $\frac{a}{b^d} > 1$, then $d < \log_b a$ and the sum becomes $ ??? $ and its complexity is $O(n^{\log_b a})$
    \end{itemize}

    % TODO: complete the proof with picture and book
\end{proof}

\subsection{Introduction to sorting algorithms}

\subsubsection{Comparison based sorting}

Every comparison-based sorting algorithm
must do $\Omega(n \log n)$ comparison to sort $n$ distinct elements in the worst case.

\underline{Statement}: For any comparison-based algorith $A$, for all $n \ge 2$ there exists an input of size $n$
such that $A$ makes at least $\log_2(n!) = \Omega(n \log n)$ comparisons.

The $n!$ comes from the fact that the factorial represents all the possible permutations of $n$ numbers,
hence, when we are sorting, we are just finding the \say{correct} permutation that sorts the input.

The $\log_2$ comes from the fact that each comparison we are making a choice, which the end gives two outcomes,
each one with half the options available for the next permutation.
Since we are looking for the worst outcome we have that we stop after $\log_2$ of the number we started with.

\subsubsection{Sorting without comparisons}

This is kinda surprising but it is actually possible if we have some other informations about the input we are sorting.

For example, if we know that the input is a sequence of integers in a certain range we can use the \textbf{counting sort} algorithm.
This algorithm works by counting the occurrences of each number in the input and then writing them in the correct order.
This is a very fast algorithm compared to other generic sorting and takes $O(n)$ time.

Other examples of sorting algorithms that don't use comparisons are \textbf{radix sort} and \textbf{bucket sort}.

\subsection{Finding the median}

In general we can sort the list and find the median like that, which would be $O(n \log n)$.
It turns out that we can do better than that.

We will solve the more general problem that looks like this:
\begin{itemize}
    \item \textit{Input}: A list of numbers $S$; an integer $k$
    \item \textit{Output}: The $k$-th smallest element of $S$
\end{itemize}

In particular if $k = \frac{|S|}{2}$ we have that $k$ is the median.

\subsubsection{Description of the algorithm}

To do so we will use a recursion based algorithm:

\begin{enumerate}
    \item Select an element $V$ from $S$ at random
    \item We create 3 sub-lists:
          \begin{enumerate}
              \item $S_L = {x \in S : x < V}$
              \item $S_V = {x \in S : x = V}$
              \item $S_R = {x \in S : x > V}$
          \end{enumerate}
    \item We now call the algorithm recursively with the following parameters:
          \begin{enumerate}
              \item If $k \le |S_L|$ then we call the algorithm with $S_L$ and $k$
              \item If $|S_L| < k \leq |S_L| + |S_V|$ then we return $V$
              \item If $k > |S_L| + |S_V|$ then we call the algorithm with $S_R$ and $k - |S_L| - |S_V|$
          \end{enumerate}
\end{enumerate}

\subsubsection{The choice of V}

To calculate the time complexity of this algorithm we first have to choose a good $V$.
This is a hard task, because the \say{best} $V$ would be the median, but that's exacltly what we are trying to find;
on the other hand the \say{worst} $V$ would be the minimum or the maximum of the input, which would reduce the problem size by only 1.

Since we cannot choose a good $V$ we will choose a random $V$:
the odds of choosing the best or the worst cases are both extreamly unlikely,
hence we need to define a \say{good enough} $V$.

We consider a good enough $V$ if it lies between $\frac{1}{4}$ and $\frac{3}{4}$ of the input,
hence we can expect that the size of the sub-problems will be at most $\frac{3}{4}$ of the original problem.

This choice is arbirtraitly, we could have chosen any other fraction and the time complexity,
as we will see later, would have been the same.
We chose these fractions because in this way the input
is divided in 2 \say{bad} cases of size $\frac{1}{4}$ and a \say{good} case of size $\frac{1}{2}$,
hence the size of the \say{bad} part is the same as the size of the \say{good} part.

\subsection{Time complexity}

We can now compute the time complexity

$$
    T(n) = \underbrace{T\left(\frac{3}{4}n\right)}
    _{\text{expected reduction}} +
    \underbrace{O(n)}_{\text{compatisons with } V}
    + \underbrace{\gamma O(n)}_{\text{finding a good } V}
$$

Note that $\gamma$ is the byproduct of the random choice of $V$,
we don't know how many times we will have to choose $V$ to find a good one.

Although we cannot be sure about the exact time this algorithm takes,
we can compute the \textbf{expected} time.

We saw before how the probability of choosing a good $V$ is $\frac{1}{2}$.
We can calculate the expected time as follows:
we pick a random $V$ and

\begin{enumerate}
    \item If $V$ is good we are done
    \item If $V$ is bad we need to repeat the algorithm
\end{enumerate}

Hence we have that $E = 1 + \frac{1}{2} E$, thus $E = 2$.
We have


\begin{align*}
    E(T(n)) & = E\left(T\left(\frac{3}{4}n\right)\right) + O(n) + 2O(n) \\
            & = O(n) + O(n) + 2O(n)                                     \\
            & = O(n)
\end{align*}

\begin{notebox}{Other solutions}
    This algorithm is presented to illustrate that algorithms that make use of random variables exist and can be useful;
    in the case of this specific problem there exist better algorithms that can solve the problem in a deterministic way,
    such as the \textbf{median of medians} algorithm.
\end{notebox}


\end{document}