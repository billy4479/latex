\documentclass[12pt]{extarticle}

\setlength{\headheight}{16pt} % ??? we do what fancyhdr tells us to do  

\title{Advanced Programming and Optimization}
\author{Giacomo Ellero}
\date{a.y. 2024/2025}

\usepackage{preamble}
%\usepackage{preamble_code}

\renewcommand{\vec}[1]{\uvec{#1}}

\begin{document}

\oldfirstpage

\section{Introduction}

This course will all be about
\begin{align}
	\max f(x), & \text{subject to}                          \\
	g_i(x)     & \leq b_i \quad \text{for } i = 1, \dots, m \\
	x          & \in \R^n
\end{align}
where $f, g_1, \dots, g_m: \R^n \to \R$ are linear functions and $b_1, \dots, b_m \in \R$.
This is the standard \emph{linear programming} problem and can be solved in polynomial time.

However we are also going to be looking at problems where $x \in \Z^n$ (integers).
This is a NP hard problem.

The last part of the course we will have the case where $f, g_1, \dots, g_m$ are not linear
but they are \emph{convex} instead. This also is a hard problem.

\section{Linear programming}

Recall that even though we have stated the problem with less-than-or-equal, we can just multiply
the constraints by $-1$ to flip the inequality if needed.
Similarly we can multiply $f$ by $-1$ to change $\min$ to $\max$ or viceversa.

In linear programming our constraint make a polygon in an $n$-dimensional space.
If we let $\vec a$ be the vector of the coefficients of $f$ the problem reduces to finding the
point which is further away from the origin perpendicular to $\vec a$.

It is not always possible to find an unique solution to a linear programming problem: sometimes
the problem has multiple solutions, it is undecidable, or unbounded.

\subsubsection{Examples}

\begin{example}{Flow}{}
	Recall that the flow problem we saw in CS2 can be written as a linear programming problem:
	each edge has a constraint of $-c_i \leq e_i \leq c_i$ (where $c_i$ is the capacity of each edge)
	and each node has an additional constraint of flow preservation.
	We can maximize the output from the source or the input to the destination.
\end{example}

\begin{example}{Icecream production}{}
	Each month $i$ has a certain icecream demand $d_i$.
	Changing the production of icecream has a cost $a_1$ and the cost of storing icecream we pay $a_2$.
	What is the optimal amount of icecream to be produced?
\end{example}

\begin{proof}[Solution]
	For each month $i$ we want
	\begin{equation}
		x_i + s_{i-1} - s_i = d_i
	\end{equation}
	where $x_i$ is the production of icecream and $s_i$ is the amount we have in storage.

	Our objective will be
	\begin{equation}
		\min a_1 \sum \abs{x_i - x_{i-1}} + a_2 \sum s_i
	\end{equation}
	but this is not a linear function!

	Instead we introduce two new variables $y_i$ which represents the increase in the production
	and $z_i$ which represents the decrease in the production.
	We can now add the constraint
	\begin{align}
		x_i - {x_{i-1}} & = y_i - z_i \\
		y_i             & \geq 0      \\
		z_i             & \geq 0
	\end{align}
	and our objective becomes
	\begin{equation}
		\min a_1 \sum y_i + a_1 \sum z_i + a_2 \sum s_i
	\end{equation}
	which is indeed linear.
	Moreover, we notice that an optimal solution either has $y_i = 0$ or $z_i = 0$,
	hence the problem is equivalent to the first one.
\end{proof}

\subsection{0-sum games}

A 0-sum game has the following properties:
\begin{itemize}
	\item There is a finite set of strategies
	\item Assume player $A$ plays move $i$ and player $B$ plays $j$,
	      then the game pays $m_{ij}$ to player $A$ and $-m_{ij}$ to player $B$.
\end{itemize}
Therefore we can consider the game to be a matrix $M$.

\subsubsection{Example: Colonel Blotto}

\paragraph{Modelling the game}
There are two armies ($B$ and $E$) which are fighting at $3$ mountain passes ($a, b, c$).
$B$ and $E$ have $5$ regiments each.

What each player has to decide is how to partition the regiments:
we will have $k, \ell, m$ such that $k \leq \ell \leq m$ and $k+\ell + m = 5$;
then each group $k, \ell, m$ will be randomly assigned to a mountain pass.

The player who has more regiments in a certain pass will win the pass,
in particular if the number of regiments is the same we will have a draw.
The player who wins more passes wins the game.

We will model the payoff as the probability of winning minus the probability of losing.

\paragraph{Example game 1}
Assume that $B$ plays $i = (0,0,5)$ and $E$ plays $j = (5, 0, 0)$.
Then we have that
\begin{equation}
	m_{ij} = \frac{1}{3} \cdot 0 + \frac{2}{3} \cdot 0 = 0
\end{equation}
where the first probability represents the case where the two armies meet, therefore we have a draw;
the second probability is for all other cases, where one army wins and the other one loses,
therefore we again get a draw.

\paragraph{Example game 2}
Assume that $B$ plays $i = (0,1,4)$ and $E$ plays $j = (5, 0, 0)$.
Then we have that
\begin{equation}
	m_{ij} = \frac{1}{3} \cdot 1 + \frac{2}{3} \cdot 0 = \frac{1}{3}
\end{equation}
similarly to the case above.

\paragraph{How to choose the best strategy}
In this way we can construct a matrix with all the payoffs, however how can we choose the best one?
We could assume that the opponent chooses a strategy at random, therefore we should choose the
strategy with the highest expected payoff, but the enemy is also a skilled player, therefore he
won't choose a strategy at random.

Players will choose based on the worst possible outcome, but then the worst case scenario will
always realize.
Moreover, even if each player knew that the other one is playing the \say{optimal} strategy,
the best option would still be to play the same strategy.
This is called \emph{pure Nash equilibrium}.

\begin{definition}{Pure Nash equilibrium}{pure-nash-eq}
	There exists a pair of pure strategies $(i, j)$ which are the best responses to each other.
\end{definition}

\subsubsection{Rock-Paper-Scissors}

\paragraph{Normal game}
This game does not have a Nash equilibrium, however we can have a mixed strategy.

\begin{definition}{Mixed Nash equilibrium}{mixed-nash-eq}
	$(\tilde x, \tilde y)$ is a mixed Nash equilibrium if each player's response $\alpha(\tilde y)$
	and $\beta(\tilde x)$ is such that
	\begin{equation}
		\beta(\tilde x) = x^T M y = \alpha(\tilde y)
	\end{equation}
\end{definition}

The payoff $(x, y)$ will be
\begin{equation}
	\sum_{i, j} m_{ij} \cdot P(A \text{ plays } i, B \text{ plays } j)
	= \sum_{i, j} m_{ij} x_i y_i = x^T M y
\end{equation}

Therefore in this case, the mixed Nash equilibrium is to choose a strategy uniformly at random:
even if the opponent knew my strategy, there is no better way to respond.

\paragraph{Easter Bunny and Santa Claus}
Assume now that Santa Claus and the Easter Bunny play a game of R-P-S,
however BS doesn't know how to use scissors.

Assume SC chooses strategy $x$, we claim then for each mixed strategy $y$ of the EB,
there exists a pure strategy $y'$ with at least the same payoff.

Indeed EB can achieve
\begin{equation}
	\min \{ \underbrace{x_p - x_s}_{\text{EB plays rock}},
	\underbrace{ -x_r + x_s}_{\text{EB plays paper}}\} = \beta(x)
\end{equation}
Meanwhile SC will max the worst case scenario:
\begin{equation}
	x_0 = \max_{x} \min \{ \underbrace{x_p - x_s}_{\text{EB plays rock}} ,
	\underbrace{ -x_r + x_s}_{\text{EB plays paper}}\}
	= \max_x \beta(x)
	= \max_x \min_y x^T M y
\end{equation}

Therefore SC strategy can be solved through a linear program:
\begin{align}
	\max             x_0 & \quad \text{subject to}                    \\
	x_0                  & \leq x_p - x_s          \label{eq:bunny-r} \\
	x_0                  & \leq -x_r + x_s         \label{eq:bunny-p} \\
	x_p + x_r + x_s      & = 1                                        \\
	x_p, x_r, x_s        & \geq 0
\end{align}
where \cref{eq:bunny-r} is what happens when EB plays rock, \cref{eq:bunny-p} is when EB plays paper
and the other two constraints are there to make sure that $x$ is a valid mixed strategy.
Putting this program into a solver we get that the optimal solution is $\tilde x = (0, 2/3, 1/3)$
and pays $1/3$.

Similarly, for the EB we can write another linear program.
\begin{equation}
	y_0 = \min_{y} \max \{ \underbrace{y_p}_{\text{SC plays rock}} ,
	\underbrace{y_r},
	\underbrace{ -y_r + y_p}_{\text{EB plays paper}}\}
	= \min_x \alpha(x)
	= \min_x \max_y x^T M y
\end{equation}
Hence
\begin{align}
	\min y_0  & \quad \text{subject to} \\
	y_0       & \leq y_p                \\
	y_0       & \leq y_r                \\
	y_0       & \leq -y_r + y_p         \\
	y_p + y_r & = 1                     \\
	y_p, y_r  & \geq 0
\end{align}
and solving the program gives that the optimal solution is $\tilde y = (1/3, 2/3)$ which pays $1/3$
(therefore EB loses).
Note that this is the same win that SC gets.
We have
\begin{equation}
	\tilde x_0 = \beta(\tilde x) \leq \tilde x^T M \tilde y \leq \alpha(\tilde y) = \tilde y_0
\end{equation}
therefore $(\tilde x, \tilde y)$ is a mixed Nash equilibrium.

\begin{theorem}{Von Neumann}{vn}
	In a finite zero-sum game $x_0 = y_0$.
	Therefore it does not matter who chooses the strategy first.
\end{theorem}


\subsubsection{AA}

We want to find a line $y = ax + b$ which separates two kind of points $p$ and $q$
such that $p$ are above the line and $q$ are below.
We try to write this as a linear problem
\begin{align}
	y(p_i) > ax(p_i) + b \quad \forall i = 1, \dots, m \\
	y(q_i) < ax(q_i) + b \quad \forall i = 1, \dots, m
\end{align}

However, this is not a linear program, because the inequality are strict.
We can fix this by introducing another variable $\delta$ and a constraint $\delta \geq 0$,
moreover we also need to $\max$ is, since, ideally we'd like for $\delta$ to be $> 0$ but, again,
we are not allowed to have strict inequalities.
\begin{align}
	\max \delta & \text{ subject to}                                   \\
	y(p_i)      & > ax(p_i) + b + \delta \quad \forall i = 1, \dots, m \\
	y(q_i)      & < ax(q_i) + b + \delta \quad \forall i = 1, \dots, m \\
	\delta      & \geq 0
\end{align}

\subsection{Standard form for linear programs}
\subsubsection{Affine Geometry}

Let $L$ be a linear subspace of $\R^d$.
Then $A = a + L$ with $a \in \R$ is a shifted linear subspace: we call it an \emph{affine space}.

Firstly we note that $A = a + L = x + L$ $\forall x \in A$: indeed if $x \in A$ then $x-a \in L$,
then for $y \in a + L$ we have that $y - a - (x - a) \in L$ giving $y \in x + L$.

We also define the \emph{affine hull} of $X \subseteq \R^d$ to be the intersection of all affine
subspaces containing $X$.

\begin{proposition}{}{}
	The affine hull of $X \subseteq \R^d$ can be written as
	\begin{equation}
		\mathrm{aff. hull}(X) =
		\left\{ \sum \alpha_i x_i \left| \substack{x_i \in X \\ \sum \alpha_i = 1} \right. \right \}
	\end{equation}
\end{proposition}
\begin{proof}
	First we prove that the set of affine combinations is included in the affine hull.
	By definition of affine hull we have $X \subseteq A = x_1 + L$.
	Moreover
	\begin{equation}
		\sum x_i \alpha_i = x_1 + \left(\sum \alpha_i (x_i - x_1)\right) \in x_1 + L = A
	\end{equation}

	Then we prove that affine hull is included in the set of affine combinations.
\end{proof}

Now we consider the dimensions of these spaces:
\begin{itemize}
	\item $\dim(L) =$ max number of linear independent vectors in $L$.
	\item $\dim(A) = \dim(L)$.
	\item $\dim(X) = \dim(\mathrm{aff. hull}(X))$.
	\item $\dim(\varnothing) = -1$
\end{itemize}
take these as definitions.

In particular a point is an affine space with dimension 0, a line has dimension 1,
a plane has dimension 2, and an hyperplane in $\R^d$ has dimension $d-1$.
Moreover, note that hyperplanes can be written as
\begin{equation}
	\{ x: \R^d \mid a^T x = b \}
\end{equation}

Observe that affine spaces spaces $A$ of dimensions $k \leq d-1$ are intersections of hyperplanes
(proof in lecture notes).

We call \emph{halfspace} the set
\begin{equation}
	\{x \in \R ^d : a^T x \leq b \}
\end{equation}


\subsubsection{Standard form}

\begin{definition}{Standard form}{lp-standard}
	Given $A \in \R^{m\cross n}, b \in \R^m, c \in \R^n$
	a linear program where the variables are $n$ dimensional and there are $m$ constraints
	can be written as
	\begin{align}
		\max c^T x & \text{ subject to} \\
		Ax \leq    & b                  \\
		x \in \R^n
	\end{align}
\end{definition}

Moreover any LP can be written in this form by adding another constraint when we have equalities,
flipping the signs of $\geq$ and of the objective function if needed.

We note that this is equivalent to finding an intersection between halpspaces, and when we require
$Ax = b$ we are instead finding an intersection between hyperplanes.

\subsubsection{Convex geometry}

We define the convex hull of a set $X$ as the intersection of all convex sets containing $X$.
Moreover we define a convex combination as $\sum \alpha_i x_i$ such that $x_i \in X$ and
$\sum \alpha_i = 1$.

The convex hull of $X$, i.e. $\mathrm{conv}(X)$ has the following properties:
\begin{itemize}
	\item $\mathrm{conv}(X)$ is convex itself
	\item $\mathrm{conv}(X)$ is the set of convex combinations of elements of $X$, that is
	      \begin{equation}
		      \mathrm{conv}(X) =
		      \left\{ \left.\sum_{i = 1}^k \alpha_i x_i \,\right|\, k \in \N^+ , x_i \in X, \sum_{i = 1}^k \alpha_i = 1 \right\}
	      \end{equation}
	\item $\mathrm{conv}(X) \subseteq X$
	\item For $X$ finite $\mathrm{conv}(X)$ is compact (closed and bounded)
\end{itemize}

\begin{theorem}{Caratheodory}{caratheodory}
	Let $X \subseteq \R^n$ and $d = \dim X$.
	Then
	\begin{equation}
		\mathrm{conv}(X) =
		\left\{ \left.\sum_{i = 1}^{d+1} \alpha_i x_i \,\right|\, x_i \in X, \sum_{i = 1}^k \alpha_i = 1 \right\}
	\end{equation}
\end{theorem}

\begin{proof}
	We know that there exists $k \in \N^+$ such that the theorem is true, let $k$ be the smallest one.
	If $k \leq d + 1$ we are done as we can set the remaining coefficients to $0$.

	We will show by contradiction that it is not possible that $k > d + 1$.
	Let $\{x_1, \dots, x_k\} = X'$: they are affine dependant;
	then there exists $\beta \neq 0 \in \R^k$ s.t. $\sum \beta_i x_i = 0$ and $\sum \beta_i = 0$.

	We know that for any $x \in X$ we can find $\alpha_i$ such that $x = \sum^k_{i = 1} \alpha_i x_i$.
	Let $\gamma = \max\{\frac{\alpha_i}{\beta_i} \mid i = 1, \dots, k_i, \beta_i < 0\}$
	and $\alpha_i' = \alpha_i - \gamma \beta_i$, such that $\alpha_i \geq 0$ and there exists exactly
	one $i^*$ such that $\alpha_{i^*}' = 0$.

	Note that
	\begin{equation}
		\sum \alpha_i' = \underbrace{\sum \alpha_i}_{=1} + \underbrace{\sum \gamma \beta_i}_{= 0} = 1
	\end{equation}
	But then $\sum \alpha_i' x_i = x$ since $\sum \beta_i x_i = 0$ and by our choice of $\alpha_i'$
	we know that $\alpha_{i^*}' = 0$ therefore we can remove the $i^*$-th term of the combination
	without changing the result.
	In this way we decreased $k$ by $1$.
	We can continue iteratively until $k = d+1$.
\end{proof}

\begin{theorem}{Strong convex separation}{strong-conv-sep}
	Let $C, D \subseteq \R^d$ be non-empty, convex, closed, and disjoint,
	moreover $C$ is bounded and therefore compact.
	Then, there is a hyperplane $\{x \mid a^T x = b\}$ such that
	$C \subseteq \{ \mid a^T x < b\}$ and $D \subseteq \{ x \mid a^T x > b\}$.
\end{theorem}

\begin{proof}
	Find $c \in C, d \in D$ such that $\norm{c-d}$ is minimal.
	Note that we can do this only if $C$ and $D$ are compact.
	If $D$ is also compact we are done.

	If $D$ is unbounded we choose $c'\in C, d' \in D$ and $\beta = \norm{c' - d'}$,
	$\alpha = \mathrm{diam}(C)$ (the diameter of $C$).
	Then consider the ball $B$ defined as $B = \{x \mid \norm{c' - x} \leq \alpha+\beta\}$.
	The set $D'= D \cap B$ is now compact and for $x \in C$ and $y \in D$, if $\norm{x - y} \leq \beta$,
	$y \in D'$:
	\begin{equation}
		\norm{c'-y} \leq \norm{c' - x} + \norm{x - y} \leq \alpha + \beta
	\end{equation}
	Now we choose $(c, d) = \argmin_{c \in C, d \in D'} \{\norm{c - d}\}$.

	We now choose an hyperplane using the parameters $a, b$:
	\begin{equation}
		a \coloneq d - c \quad \text{and} \quad b \coloneq \frac{a^T d - a^T c}{2}
	\end{equation}
	then
	\begin{equation}
		a^Td - a^T c = a^T a = \norm{a}^2 = \norm{d - c}^2 \geq 0
		\implies a^T d > a^T c
	\end{equation}

	Now we want to prove that $\forall x \in C: a^Tx < b$ and $\forall y \in D: a^T y > b$.
	We show that $a^T x \leq a^T c < b$ and we can do the same procedure over $D$.

	By contradiction assume that $a^Tx > a^Tc$.
	We have $a^T(x-c)>0$ which means that the angle $d, c, x$ is sharp (acuto) and we can draw
	a right-angle triangle we can form with $x' \in \mathrm{conv}(\{c, x\})$ and
	$\norm{x'-d}<\norm{c-d}$, but this contradicts the fact that $c$ is minimal.
\end{proof}


\begin{theorem}{Weak convex separation}{weak-conv-sep}
	Let $C, D$ be convex and disjoint.
	Then there is an hyper plane $\{x \mid a^T x= b\}$ such that
	$C \subseteq \{ \mid a^T x \leq b\}$ and $D \subseteq \{ x \mid a^T x \geq b\}$.
\end{theorem}

\begin{proof}
	Consider $\{0\}$ and $S = \{x - y \mid x \in C, y \in D\}$.
	Let $x - y \in S$ and $x' - y' \in S$, then take their convex combination:
	\begin{equation}
		t(x - y) + (1-t)(x'-y') =
		\underbrace{tx + (1-t)x'}_{\in C} - \underbrace{(ty + (1-t)y')}_{\in D} \in S
	\end{equation}
	Now we have a few possible cases:
	\begin{enumerate}
		\item $0 \notin \mathrm{cl}(S)$ ($0$ is in the closure of $S$).

		      We apply \cref{thm:strong-conv-sep} to $\{0\}$ and $\mathrm{cl}(S)$.
		      We have
		      \begin{equation}
			      0 = a^T 0 < b , a^T(x-y) \enspace \forall x-y \in S
			      \implies a^T x > a^T y \enspace \forall x \in C, d \in D
		      \end{equation}

		\item $0 \in \mathrm{cl}(S)$.

		      Then $0$ is in the boundary of $\mathrm{cl}(S)$.
		      \begin{enumerate}
			      \item If $\mathrm{int}(S) = \varnothing$, then $S$ belongs to a hyperplane
			            $\{x \mid a^T = 0 \}$ which we can use as separator: we have
			            $a^T x =  a^T y$ for all $x \in C, y \in D$.

			      \item If $\mathrm{int}(S) \neq \varnothing$ we define
			            $S_{-\varepsilon} = \{s \in S \mid B_\varepsilon(s) \subseteq S\}$
			            for some $\varepsilon > 0$.
			            Then $\mathrm{cl}(S_{-\varepsilon})$ is closed, convex and does not contain $0$,
			            therefore we can apply \cref{thm:strong-conv-sep}
			            and get some $a_\varepsilon, b_\varepsilon$ such that
			            $a_\varepsilon^T s > 0$ for all $s \in S_{-\varepsilon}$.

			            Now we normalize $a_\varepsilon$ such that $\norm{a_\varepsilon} = 1$
			            and choose an infinite sequence $\varepsilon_n$
			            which converges to $0$ as $n\to\infty$.
			            Since is bounded ($\norm{a_{\varepsilon_k}} = 1$) the sequence $a_{\varepsilon_n}$
			            contains a subsequence converging to $a\neq 0$.

			            TODO: FINISH HERE
		      \end{enumerate}
	\end{enumerate}
\end{proof}

\end{document}
