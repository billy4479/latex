\documentclass[12pt]{extarticle}

\title{Advanced Programming and Optimization Algorithms \\ Homework 04}
\author{Giacomo Ellero}
\date{12/05/2025}

\usepackage{preamble_base}
\usepackage{preamble_math}

\renewcommand{\vec}[1]{\bvec{#1}}

\numberwithin{equation}{section}

\begin{document}

\maketitle

\section*{Exercise 1}
\stepcounter{section}

By definition of ProjGD we have
\begin{equation}
	x_{t+1} = \argmin_{x\in X} \norm{x - x_t + \gamma \nabla f(x_t)}^2
\end{equation}
then if we have obtained $x_{t+1} = x_t$ it means that the expression
$\norm{x - x_t + \gamma \nabla f(x_t)}^2$ is minimized $x = x_t$.

Notice that
\begin{equation}
	\norm{\cancel {x_t} - \cancel{x_t} + \gamma \nabla f(x_t)}^2 =
	\norm{\gamma \nabla f(x_t)}^2
\end{equation}
but the minimum that the norm can attain is $0$ and this happens only when its argument is also $0$.
Therefore, we necessarily have that
\begin{equation}
	\gamma \nabla f(x_t) = 0 \implies \nabla f(x_t) = 0
\end{equation}
since $\gamma > 0$ by definition.

This, by the first characterization of convexity, means that $x_t$ is the minimizer of $f$ over $X$.

\section*{Exercise 2}
\stepcounter{section}

We solve this problem using Lagrange multipliers.
The lagrangian is
\begin{equation}
	L(x, \lambda) = \sum_{i = 1}^n x_i \log x_i +
	\lambda \left( \left(\sum_{i = 1}^n x_i \right) - 1 \right)
\end{equation}
and its partial derivatives are
\begin{align}
	\dv{L}{x_i}     & = \log x_i + 1 + \lambda                  & \forall i \in \{1, \dots, n \} \\
	\dv{L}{\lambda} & = \left( \sum_{i = 1}^{n} x_i \right) - 1
\end{align}

We set them to zero to obtain from the first equations that $x_i = e^{-1-\lambda}$,
which is constant for all $i \in [n]$, and from the last one
\begin{equation}
	\sum_{i = 1}^{n} x_i = 1 \implies n e^{-1-\lambda} = 1 \implies x_i
	= \frac{1}{n} \enspace \forall i \in [n]
\end{equation}

\section*{Exercise 3}
\stepcounter{section}

We want to show that
\begin{equation}
	x^T y \leq \norm{x}_\infty \cdot \norm{y}_1
\end{equation}

To do so we expand the definition of norm and vector product:
\begin{equation}
	\sum_{i} x_i y_i \leq \max_{i} \{ \abs{x_i} \} \sum_{i}^{} \abs{y_i}
\end{equation}

We now consider each component separately
\begin{gather}
	x_i \cdot y_i \leq \max_i \{\abs{x_i}\} \cdot \abs{y_i} \\
	\frac{x_i}{\max_i \{ \abs{x_i} \}} \cdot \frac{y_i}{\abs{y_i}} \leq 1 \\
	\mathrm{sgn} (x_i) \cdot \frac{\abs{x_i}}{\max_i \{ \abs{x_i} \}} \cdot \mathrm{sgn}(y_i) \leq 1 \\
	\mathrm{sgn} (x_i y_i) \cdot \frac{\abs{x_i}}{\max_i \{ \abs{x_i} \}} \leq 1
\end{gather}

Then if $\mathrm{sgn}(x_i y_i) = -1$ we are done since we will end up with a negative term on the
left which is for sure $\leq 1$. If $\mathrm{sgn}(x_i y_i) = 0$ we are also done since everything on
the left becomes $0$. If $\mathrm{sgn}(x_i y_i) = 1$ we get that
\begin{equation}
	\frac{\abs{x_i}}{\max_i \{ \abs{x_i} \}} \leq 1 \label{eq:ex-3}
\end{equation}
which is indeed true by the definition of $\max$.
We then sum the inequalities for all $i$ to obtain the result.

Note that when $\norm{x}_\infty \leq 1$ and $x = \mathrm{sgn}(y)$ we are in the situation of
\cref{eq:ex-3}. Moreover $\abs{x_i} = 1$ for all $i$ (by the definition of $\mathrm{sgn}$), which
means that $\max_i \{ \abs{x_i} \} = 1$, attaining equality.



\end{document}
