\documentclass[12pt]{extarticle}

\usepackage{preamble}

\title{Mathematical Analysis 2 Notes, Partial 2}
\date{Semester 2, 2023/2024}

\setlength{\headheight}{15pt} % ??? we do what fancyhdr tells us to do

\renewcommand{\vec}[1]{\uvec{#1}}
\newcommand{\Hg}{{\operatorfont Hg}}

\begin{document}

\firstpage

% \chapter{Functions from \texorpdfstring{$\R^d$}{Rd} to \texorpdfstring{$\R^p$}{Rp}}

\section{Continuity}

\begin{definition}[continuity]
    A function $f: D \subseteq \R^d \to \R^p$ is continuous at $\vec{x}_0$ if
    \begin{equation}
        \forall \varepsilon > 0,
        \quad
        \exists \delta > 0,
        \quad
        \forall \vec{x} \in D,
        \quad
        \underbrace{\norm{\vec{x} - \vec{x}_0}}_{\text{norm in } \R^d} < \delta
        \implies
        \underbrace{\norm{f(\vec{x}) - f(\vec{x}_0)}}_{\text{norm in } \R^p} < \varepsilon
    \end{equation}
\end{definition}

\begin{proposition}[sequential characterization]
    A function $f: D \subseteq \R^d \to \R^p$ is continuous at $\vec{x}_0$ iff for every sequence $\vec{x}_n \in D$ which converges to $\vec{x}_0$, the sequence $f(\vec{x}_n)$ converges to $f(\vec{x}_0)$.
\end{proposition}

\begin{proposition}[composition of continuous functions]
    Let $f: D \subseteq \R^d \to \R^p$ and $g: U \subseteq \R^p \to \R^q$ be continuous at $\vec{x}_0$ and $f(\vec{x}_0)$ respectively such that if $\vec{x} \in D$ then $f(\vec{x}) \in U$ and $g \circ f$ is well defined.
    Then $g \circ f$ is continuous at $\vec{x}_0$.
\end{proposition}

\begin{proof}
    Informally we can say

    \begin{align}
        g(f(\lim \vec{x}_n)) = g(\lim f(\vec{x}_n)) = \lim g(f(\vec{x}_n))
    \end{align}

    For a more formal proof you can see the proof we did for parametric curves in the first partial.
\end{proof}

\begin{proposition}[componentwise continuity]
    Let $f: D \subseteq \R^d \to \R^p$ and write
    \begin{equation}
        f = \begin{pmatrix}
            f_1    \\
            f_2    \\
            \vdots \\
            f_p
        \end{pmatrix}
    \end{equation}
    where each $f_j : D \subseteq \R^d \to \R$.

    Then $f$ is continuous at $\vec{x}_0$ iff each $f_j$ is continuous at $\vec{x}_0$.
\end{proposition}

\begin{remark}
    This is more or less the definition of continuity for parametric curves.
\end{remark}

\begin{proof}
    \skiplineafterproof
    \begin{description}
        \item[$\implies$] ($f$ continuous at $\vec{x}_0 \implies \forall j, \enspace f_j$ continuous at $\vec{x}_0$)

              Fix $j$ and let $\left(\vec{y}_n\right)$ be a sequence that converges to $\vec{x}_0$.
              Since $f$ is continuous $f(\vec{y}_n) = \left(f_1(\vec{y}_n), f_2(\vec{y}_n), \ldots, f_p(\vec{y}_n)\right)$ converges to $f(\vec{x}_0) = \left(f_1(\vec{x}_0), f_2(\vec{x}_0), \ldots, f_p(\vec{x}_0)\right)$.

              In particular, the sequence $\left(f_j(\vec{y}_n)\right)$ converges to $f_j(\vec{x}_0)$.

        \item[$\impliedby$] ($\forall j, \enspace f_j$ continuous at $\vec{x}_0 \implies f$ continuous at $\vec{x}_0$)

              Let $\left(\vec{y}_n\right)$ be a sequence that converges to $\vec{x}_0$. Then, since $f_j$ is continuous, for every $j$, the sequence $f_j(\vec{y}_n)$ converges to $f_j(\vec{x}_0)$.

              So the sequence $f(\vec{y}_n) = \left(f_1(\vec{y}_n), \ldots, f_p(\vec{y}_n)\right)$ converges to $f(\vec{x}_0) = \left(f_1(\vec{x}_0), \ldots, f_p(\vec{x}_0)\right)$.
    \end{description}
\end{proof}

\section{Differentiability}

\begin{definition}[partial derivatives]
    Let $f: D \subseteq \R^d \to \R^p$ and $\vec{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{pmatrix} \in D$.

    We define, if exists,
    \begin{equation}
        \pdv{f_j}{x_i}()(\vec{x}) = \lim_{h \to 0} \frac{f_j(x_1, \ldots, x_{i-1}, x_i + h, x_{i+1}, \ldots, x_d) - f_j(x_1, \ldots, x_d)}{h} \in \R
    \end{equation}
    for all $i = 1, 2, \ldots, d$ and $j = 1, 2, \ldots, p$.

    Moreover we define $\pdv{f}{x_i}()(\vec{x})$, if exists, as
    \begin{equation}
        \pdv{f}{x_i}()(\vec{x}) = \begin{pmatrix}
            \pdv{f_1}{x_i}()(\vec{x}) \\
            \pdv{f_2}{x_i}()(\vec{x}) \\
            \vdots                    \\
            \pdv{f_p}{x_i}()(\vec{x})
        \end{pmatrix} \in \R^p
    \end{equation}
\end{definition}

\begin{definition}[Jacobian matrix]
    Let $f: D \subseteq \R^d \to \R^p$ and $\vec{x} \in D$ such that all the partial derivatives exist at $\vec{x}$.
    Then the Jacobian matrix of $f$ at $\vec{x}$, written as $J_f(\vec{x})$, is the $p \times d$ matrix such that
    \begin{equation}
        J_f(\vec{x})_{ij} = \pdv{f_j}{x_i}()(\vec{x})
    \end{equation}

    That is
    \begin{equation}
        J_f(\vec{x}) = \begin{pmatrix}
            \pdv{f_1}{x_1}()(\vec{x}) & \pdv{f_1}{x_2}()(\vec{x}) & \cdots & \pdv{f_1}{x_d}()(\vec{x}) \\
            \pdv{f_2}{x_1}()(\vec{x}) & \pdv{f_2}{x_2}()(\vec{x}) & \cdots & \pdv{f_2}{x_d}()(\vec{x}) \\
            \vdots                    & \vdots                    & \ddots & \vdots                    \\
            \pdv{f_p}{x_1}()(\vec{x}) & \pdv{f_p}{x_2}()(\vec{x}) & \cdots & \pdv{f_p}{x_d}()(\vec{x})
        \end{pmatrix}
    \end{equation}

    Note that the $i$-th column of the matrix is $\pdv{f}{x_i}()(\vec{x}) \in \R^p$.
\end{definition}

\begin{example}[change of variables to polar coordinates]
    Let $f: \R^2 \to \R^2$ be
    \begin{equation}
        f(x, y) = \begin{pmatrix}
            x \cos y \\
            x \sin y
        \end{pmatrix}
    \end{equation}
    that is $f_1(x, y) = x \cos y$ and $f_2(x, y) = x \sin y$.

    Then the Jacobian matrix of $f$ is
    \begin{equation}
        J_f(x, y) = \begin{pmatrix}
            \pdv{f_1}{x}() & \pdv{f_1}{y}() \\
            \pdv{f_2}{x}() & \pdv{f_2}{y}()
        \end{pmatrix} = \begin{pmatrix}
            \cos y & -x \sin y \\
            \sin y & x \cos y
        \end{pmatrix}
    \end{equation}
\end{example}

\begin{example}[Jacobian of $f: \R^2 \to \R$]
    If $f: \R^2 \to \R$ then the Jacobian matrix of $f$ is a $1 \times 2$ matrix, that is a row vector.
\end{example}

\begin{remark}
    The Jacobian is the transpose of the gradient, that is, $J_f(x, y) = \left(\nabla f(x, y)\right)'$.
\end{remark}

\begin{lemma}[linear applications]
    If $L: \R^d \to \R^p$ is a linear application then $L$ then there exists an unique matrix $M$ of size $p \times d$ such that $L(\vec{x}) = M \vec{x}$ for all $\vec{x} \in \R^d$.
    If we write $M = \begin{pmatrix} \vec{m}_1 & \vec{m}_2 & \cdots & \vec{m}_d \end{pmatrix}$ where $\vec{m}_i \in \R^p$ is the $i$-th column of $M$ for all $i = 1, 2, \ldots, d$, then
    \begin{equation}
        L(\vec{x} = M\vec{x} = M \begin{pmatrix}
            x_1    \\
            x_2    \\
            \vdots \\
            x_d
        \end{pmatrix} = h_1 \vec{m}_1 + h_2 \vec{m}_2 + \cdots + h_d \vec{m}_d
    \end{equation}
\end{lemma}
\begin{remark}
    We can calculate $\vec{m}_i = L(\vec{e}_i)$ where $\vec{e}_i$ is the $i$-th vector of the canonical basis of $\R^d$ for all $i = 1, 2, \ldots, d$.
\end{remark}

We can now intuitively define what differentiability means before giving a formal definition.
Just by extending the definition of differentiability of functions from $\R^2$ to $\R$ we can define the Taylor expansion as follows:

For $d \geq 2, p = 1$ we get
\begin{equation}
    f(\vec{x} + \vec{h}) = f(\vec{x}) + h_1 \pdv{f}{x_1}()(\vec{x}) + h_2 \pdv{f}{x_2}()(\vec{x}) + \cdots + h_d \pdv{f}{x_d}()(\vec{x}) + \o(\vec{h})
\end{equation}

Then to go to $p \geq 1$ we just repeat the process for each component of $f$.
That is, for each $j = 1, 2, \ldots, p$ we have
\begin{equation}
    f_j(\vec{x} + \vec{h}) = f_j(\vec{x}) + \sum_{i=1}^d h_i \pdv{f_j}{x_i}()(\vec{x}) + \o(\vec{h})
\end{equation}

This is the fully-expanded first order Taylor polynomial of $f$ at $\vec{x}$.
Now we can combine everything in matrix form and we get that the derivative part (which is just a linear transformation) becomes
\begin{equation}
    \sum_{i = 1}^d h_i \pdv{f}{x_i}()(\vec{x}) = M \vec{h}
\end{equation}
for some matrix $M$, but this $M$ is just the Jacobian matrix of $f$ at $\vec{x}$, so we can write
\begin{equation}
    f(\vec{x} + \vec{h}) = f(\vec{x}) + J_f(\vec{x}) \vec{h} + \o(\vec{h})
\end{equation}

\begin{definition}[differentiability]
    Let $f: D \subseteq \R^d \to \R^p$, with $D$ open, and $\vec{x}_0 \in D$.
    We say that $f$ is differentiable at $\vec{x}_0$ if all the partial derivatives exist at $\vec{x}_0$
    and $\exists r > 0$ such that for all $\vec{h} \in B_c(\vec{0}, r)$ we have $\vec{x}_0 + \vec{h} \in D$ and
    \begin{equation}
        f(\vec{x}_0 + \vec{h}) = f(\vec{x}_0) + J_f(\vec{x}_0) \vec{h} + \o(\vec{h})
    \end{equation}
\end{definition}

\begin{remark}
    This is an equality in $\R^p$, $\o(\vec{h})$ means that each component is $\o(h)$, and $J_f(\vec{x}_0) \vec{h}$ is the matrix-vector product.
\end{remark}

\begin{definition}[differential]
    We call differential of $f$ at $\vec{x}_0$ (if exists) the linear transformation
    \begin{equation}
        \vec{h} \mapsto J_f(\vec{x}_0) \vec{h}
    \end{equation}
    we denote it as $Df_{\vec{x}_0}(\vec{h})$.
\end{definition}

\begin{proposition}[differentiability, second definition]
    Let $f: D \subseteq \R^d \to \R^p$, with $D$ open, and $\vec{x}_0 \in D$.
    $f$ is differentiable iff there exists a linear transformation $L: \R^d \to \R^p$ such that $\exists r > 0$ such that for all $\vec{h} \in B_c(\vec{0}, r)$ we have $\vec{x}_0 + \vec{h} \in D$ and
    \begin{equation}
        f(\vec{x}_0 + \vec{h}) = f(\vec{x}_0) + L(\vec{h}) + \o(\vec{h})
    \end{equation}
    in this case necessarily $L = Df_{\vec{x}_0}$.
\end{proposition}

\begin{proof}
    \skiplineafterproof
    \begin{description}
        \item[$\implies$] (differentiability $\implies$ linear transformation)
              This is the easy part, just take $L = Df_{\vec{x}_0}$.
        \item[$\impliedby$] (linear transformation $\implies$ differentiability)
              Let $L(\vec{h}) = M \vec{h}$ for some matrix $M$.
              We saw before that the column $m_i = L(\vec{e}_i)$, then let $\vec{h} = h \vec{e}_i$ and by linearity we have
              \begin{equation}
                  f(\vec{x}_0 + h \vec{e}_i) = f(\vec{x}_0) + h L(\vec{e}_i) + \o(h)
              \end{equation}
              dividing by $h$ we get
              \begin{equation}
                  \frac{f(\vec{x}_0 + h \vec{e}_i) - f(\vec{x}_0)}{h} = L(\vec{e}_i) + \frac{\o(h)}{h}
              \end{equation}
              and taking the limit as $h \to 0$ we get that $\pdv{f}{x_i}()(\vec{x}_0)$ exists and is equal to $L(\vec{e}_i)$.
              Then the matrix representing $L$ has columns that are the partial derivatives of $f$ at $\vec{x}_0$, that is $M = J_f(\vec{x}_0)$.
    \end{description}
\end{proof}

\begin{proposition}[differentiability implies continuity]
    If $f: D \subseteq \R^d \to \R^p$ is differentiable at $\vec{x}_0$ then $f$ is continuous at $\vec{x}_0$.
\end{proposition}

\begin{theorem}[sufficient condition for differentiability]
    Let $f: D \subseteq \R^d \to \R^p$, $D$ open. If $\pdv{f_j}{x_i}()$ exist and are continuous in $D$ for all $i = 1, 2, \ldots, d$ and $j = 1, 2, \ldots, p$ then $f$ is differentiable in $D$.
    In this case we say that $f$ is of class $C^1$.
\end{theorem}
\begin{remark}
    This is a sufficient condition, but not a necessary one.
\end{remark}

\begin{proof}
    We will not prove these results because the proof is quite long and we already did it in the case of functions from $\R^2$ to $\R$.
\end{proof}

\begin{example}
    Consider a function $f: \R \to \R^p$ (a parametric curve) defined as
    \begin{equation}
        f : t \mapsto \begin{pmatrix}
            f_1(t) \\
            f_2(t) \\
            \vdots \\
            f_p(t)
        \end{pmatrix}
    \end{equation}

    Then
    \begin{equation}
        J_f(t) = \begin{pmatrix}
            f'_1(t) \\
            f'_2(t) \\
            \vdots  \\
            f'_p(t)
        \end{pmatrix} = f'(t)
    \end{equation}
    and the Taylor expansion of $f$ is
    \begin{equation}
        f(t + h) = f(t) + h f'(t) + \o(h)
    \end{equation}
    which looks a lot like what we saw in Analysis 1.
\end{example}
\begin{remark}
    Let $\vec{a}, \vec{b} \in \R^d$, then $\vec{a} \cdot \vec{b} = \vec{a}^T \vec{b}$, where the first one is a dot product and the second one is a matrix multiplication.
\end{remark}

\begin{example}[tangent plane to a parametric surface]
    Let $f: \R^2 \to \R^3$ be a parametric surface, then its image is
    \begin{equation}
        f = \{ f(u, v) : \R^2 \}
    \end{equation}

    The tangent plane is the plane passing through $f(u_0, v_0)$ and normal to
    \begin{equation}
        \pdv{f}{u}() \times \pdv{f}{v}()
    \end{equation}
    if the cross product is not zero.

    The equation of the plane is
    \begin{equation}
        (\vec{x} - f(u_0, v_0)) \cdot \left( \pdv{f}{u}() \times \pdv{f}{v}() \right) = 0
    \end{equation}

\end{example}

% Class of 25/03/2024

\begin{remark}
    Remember that if $L: \R^d \to \R^p$ and $A: \R^p \to \R^q$ are linear transformations represented by the matrices $M_L$ and $M_A$ respectively, then the composition $A \circ L$ is a linear map represented by the matrix $M_A M_L$.
\end{remark}

\begin{theorem}[chain rule]
    Let $f: D \subseteq \R^d \to \R^p$ and $g: U \subseteq \R^p \to \R^q$ such that $D, U$ are open, and $f(D) \subseteq U$ so that $g \circ f$ is well defined.
    If $f$ is differentiable at $\vec{x}_0$ and $g$ is differentiable at $f(\vec{x}_0)$ then $g \circ f$ is differentiable at $\vec{x}_0$ and
    \begin{equation}
        \underbrace{J_{g \circ f}(\vec{x}_0)}_{q \times d \approx (g \circ f)'} =
        \underbrace{J_g(f(\vec{x}_0))}_{q \times p \approx g' \circ f}
        \underbrace{J_f(\vec{x}_0)}_{p \times d \approx f'}
    \end{equation}
    which is quite similar to the chain rule for functions from $\R$ to $\R$.

    For the differential we get
    \begin{equation}
        D(g \circ f)_{\vec{x}_0} = Dg_{f(\vec{x}_0)} \circ Df_{\vec{x}_0}
    \end{equation}
    the equivalence of the two formulas is given by the remark above.
\end{theorem}

\begin{proof}
    This is just the idea of the proof, for a more rigorous one check the book.

    We have
    \begin{enumerate}[label=(\roman*.)]
        \item \label{itm:diff:chain_rule_1} $f(\vec{x}_0 + \vec{h}) \approx f(\vec{x}_0) + J_f(\vec{x}_0) \vec{h}$
        \item \label{itm:diff:chain_rule_2} $g(f(\vec{x}_0 + \vec{k})) \approx g(f(\vec{x}_0)) + J_g(f(\vec{x}_0)) J_f(\vec{x}_0) \vec{k}$
    \end{enumerate}

    then

    \begin{align}
        g(f(\vec{x}_0 + \vec{h})) & \stackrel{\text{\ref{itm:diff:chain_rule_1}}}{\approx} g(f(\vec{x}_0) + J_f(\vec{x}_0) \vec{h})                     \\
                                  & \stackrel{\text{\ref{itm:diff:chain_rule_2}}}{\approx} g(f(\vec{x}_0)) + J_g(f(\vec{x}_0)) (J_f(\vec{x}_0) \vec{h}) \\
                                  & = g(f(\vec{x}_0)) + \underbrace{(J_g(f(\vec{x}_0)) J_f(\vec{x}_0)) \vec{h}}_{\text{differential}}
    \end{align}

    In the rigorous proof we will remove the $\approx$ and deal with the $\o$ terms.
\end{proof}

\begin{example}[fully expanded form]
    Let $f: \R^p \to \R^q$ and $g: \R^d \to \R^p$ be differentiable functions.
    Then $J_{g \circ f}(\vec{x})$ is a $q \times d$ matrix where
    \begin{equation}
        J_{g \circ f}(\vec{x})_{ki} =
        \pdv{(g \circ f)_k}{x_i}() = \sum_{j=1}^p \pdv{g_k}{y_j}()(f(\vec{x})) \cdot \pdv{f_j}{x_i}()(\vec{x})
    \end{equation}
\end{example}

\begin{example}{polar coordinates}
    Let $f: \R^2 \to \R$ and $g(r, \theta) = f(r \cos \theta, r \sin \theta)$.
    Then $g = f \circ h$ where $h: \R^2 \to \R^2$ is $h(r, \theta) = (r \cos \theta, r \sin \theta)$.

    We can compute $\nabla g$, or equivalently $J_g$, by the chain rule:

    We have
    \begin{align}
        J_f & = \begin{pmatrix}
                    \pdv{f}{x}() & \pdv{f}{y}()
                \end{pmatrix}  \\
        J_h & = \begin{pmatrix}
                    \cos \theta & -r \sin \theta \\
                    \sin \theta & r \cos \theta
                \end{pmatrix}
    \end{align}
    then
    \begin{align}
        J_g & = J_f(h) \cdot J_h                  \\
            & = \begin{pmatrix}
                    \pdv{f}{x}()(h) & \pdv{f}{y}()(h)
                \end{pmatrix}
        \begin{pmatrix}
            \cos \theta & -r \sin \theta \\
            \sin \theta & r \cos \theta
        \end{pmatrix}
    \end{align}

    That is
    \begin{align}
        \pdv{g}{r}()(r, \theta)      & = \cos \theta \pdv{f}{x}()(r \cos \theta, r \sin \theta) + \sin \theta \pdv{f}{y}()(r \cos \theta, r \sin \theta)      \\
        \pdv{g}{\theta}()(r, \theta) & = -r \sin \theta \pdv{f}{x}()(r \cos \theta, r \sin \theta) + r \cos \theta \pdv{f}{y}()(r \cos \theta, r \sin \theta)
    \end{align}

\end{example}

\begin{proposition}[class of composition]
    Let $f: D \subseteq \R^d \to \R^p$ and $g: U \subseteq \R^p \to \R^q$ be of class $C^1$.
    Then $g \circ f$ is of class $C^1$.
\end{proposition}

\begin{proposition}[invariant of the dimension by diffeomorphism]
    Let $f: U \subseteq \R^d \to V \subseteq \R^p$ be a bijective function with $U, V$ open. We want to check whether $d = p$.

    \begin{itemize}
        \item If $f$ is \emph{linear}, then $d = p$.
        \item If $f$ is \emph{continuous}, bijective, and $f^{-1}$ is continuous then $d = p$.
        \item If $f$ is bijective and $f$ and $f^{-1}$ are \emph{differentiable} then $d = p$.
        \item In \emph{general} $d \neq p$.
    \end{itemize}
\end{proposition}

\begin{proof}
    \skiplineafterproof
    \begin{description}
        \item[General case]
              Let $f : (0,1) \to (0,1)^2$ such that $f$ maps the the even decimal digits of $x$ to the first component and the odd decimal digits to the second component.
              Then $f$ is bijective but $d = 1$ and $p = 2$.

        \item[Linearity]
              See linear algebra, any invertible matrix is also a square matrix.

        \item[Continuity]
              This proof is quite hard and we will not do it here.

        \item[Differentiability]
              Let $g = f^{-1}$ so that $f(g(\vec{y})) = \vec{y}$ for $\vec{y} \in V$ and $g(f(\vec{x})) = \vec{x}$ for $\vec{x} \in U$.
              Then fix a point $\vec{x} \in U$ and let $\vec{y} = f(\vec{x})$.
              Now apply the chain rule to get

              \begin{align}
                  J_f(\vec{x}) \cdot J_g(f(\vec{x})) & = I_{p\times p} \\
                  J_g(\vec{y}) \cdot J_f(g(\vec{y})) & = I_{d\times d}
              \end{align}

              Then $J_f(\vec{x})$ and $J_g(\vec{y})$ are the inverse of each other, so they must be square matrices of the same size.

              Moreover, if we have the proof for the continuity case we can just use the fact that differentiability implies continuity.
    \end{description}
\end{proof}

\begin{theorem}[schwartz]
    \label{thm:diff:schwartz}
    If $f$ is of class $C^k$, the order in which you take derivatives does not matter.
\end{theorem}

% TODO: there is something missing here

\begin{proof}
    TODO
\end{proof}

% Class of 15/04/2024
\begin{definition}[Hessian matrix]
    Let $f: D \subseteq \R^d \to \R$, with $D$ open and $f$ of class $C^2$.
    The Hessian matrix of $f$ at $\vec{x}_0 \in D$, written $\Hg(\vec{x}_0)$, is the $\vec{d} \times \vec{d}$ matrix in which the $(i, j)$ entry is $\pdv[2]{f}{x_i}{x_j}()(\vec{x}_0)$.
    By \autoref{thm:diff:schwartz} this is a symmetric matrix.
\end{definition}

\subsection{Taylor expansions of second order}

\begin{theorem}[second order taylor expansions]
    Let $f: D \subseteq \R^d \to \R$, with $D$ open and $f$ of class $C^2$. Let $\vec x_0 \in D$.
    Then
    \begin{equation}
        f(\vec x_0 + \vec h) = f(\vec x_0) + \left( \sum^d_{i = 1} h_i \pdv{f}{x_i}()(\vec x_0) \right) + \left(\frac{1}{2} \sum^d_{i = 1} \sum^d_{j = 1} h_i h_j \pdv[2]{f}{x_i}{x_j}()(\vec x_0) \right) + \o\left(\norm{\vec h}^2\right)
    \end{equation}

    Here a function $g = \o\left(\norm{\vec h}^2\right)$ if
    \begin{equation}
        \lim_{h \to \vec 0} \frac{g(\vec h)}{\norm{\vec h}^2} = 0
    \end{equation}
\end{theorem}

\begin{example}[$d = 2$]
    All the partial derivatives are evaluated at $(x_0, y_0)$.
    \begin{align}
        f(x_0 + h, y_0 + k) & = f(x_0, y_0) + h \pdv{f}{x} + k \pdv{f}{y}                                                                              \\
                            & + \frac{1}{2} \left( h^2 \pdv[2]{f}{x} + 2hk \pdv[2]{f}{x}{y} + k^2 \pdv[2]{f}{y} \right) \label{eq:taylor:second_order} \\
                            & + \o \left( \norm{(h, k)}^2 \right)
    \end{align}
\end{example}

\begin{remark}
    The second order term (\autoref{eq:taylor:second_order}) can be written as
    \begin{equation}
        \frac{1}{2} \vec{h} \cdot \left( \Hg (\vec x_0) \vec h\right) = \frac{1}{2} \vec h^T \Hg(\vec x_0) \vec h
    \end{equation}

    Therefore the second order expansion could be written \say{compactly} as
    \begin{equation}
        f(\vec x_0 + \vec h) = f(\vec x_0) + \vec h \cdot \nabla f(\vec x_0) + \frac{1}{2} \vec h^T \Hg(\vec x_0) \vec h + \o(\norm{\vec h}^2)
    \end{equation}
\end{remark}

\begin{proof}
    Let $g(t) = f(\vec x_0 + t \vec h)$, so that $g(0) = f(\vec x_0)$ and $g(1) = f(\vec x_0 + \vec h)$.
    Then
    \begin{align}
        g'(t)  & = \sum^d_{i = 1} h_i \pdv{f}{x_i}()(\vec x_0 + t \vec h)                                                                                                                         \\
        g''(t) & = \sum^d_{i = 1} h_i \underbrace{\left[ \sum^d_{j = 1} h_j \pdv{x_j}\left( \pdv{f}{x_i}()(\vec x_0 + t\vec h) \right) \right]}_{\dv{t}\left( \pdv{f}{x_i} ()(x_0 + t h) \right)} \\
               & = \sum^d_{i = 1} \sum^d_{j = 1} h_i h_j \pdv[2]{f}{x_j}{x_i}()(\vec x_0 + t \vec h)
    \end{align}

    Using the fact that $g(1) = g(0) + g'(0) + \frac{1}{2} g''(t)$ we have
    \begin{equation}
        \underbrace{f(\vec x_0 + \vec h)}_{g(1)} = \underbrace{f(\vec x_0)}_{g(0)} + \underbrace{\sum^d_{i = 1} h_i \pdv{f}{x_i}()(\vec x_0)}_{g'(0)} + \frac{1}{2} \underbrace{\sum^d_{i = 1} \sum^d_{j = 1} h_i h_j \pdv[2]{f}{x_j}{x_i}()(\vec x_0 + t \vec h)}_{g''(t)}
    \end{equation}
    for some $t \in (0, 1)$.

    Then we call the \textit{reminder} the function
    \begin{equation}
        \Delta(\vec h) = f(\vec x_0 + \vec h) - f(\vec x_0) + \sum^d_{i = 1} h_i \pdv{f}{x_i}()(\vec x_0) - \frac{1}{2} \sum^d_{i = 1} \sum^d_{j = 1} h_i h_j \pdv[2]{f}{x_j}{x_i}()(\vec x_0)
    \end{equation}

    We now want to prove that $\Delta(\vec h) = \o(\norm {\vec h}^2)$. But $\Delta(\vec h)$ can be written as
    \begin{equation}
        \Delta(\vec h) = \frac{1}{2}  \sum^d_{i = 1} \sum^d_{j = 1} h_i h_j \left( \pdv[2]{f}{x_j}{x_i}()(\vec x_0 + t \vec h)- \pdv[2]{f}{x_j}{x_i}()(\vec x_0)\right)
    \end{equation}
    for some $t \in (0, 1)$.

    Now we fix $\varepsilon > 0$. By continuity of the second derivatives, we can find $\delta > 0$ such that if $\norm {\vec{h}} \leq \delta$ then $ \pdv[2]{f}{x_j}{x_i}()(\vec x_0 + t \vec h)- \pdv[2]{f}{x_j}{x_i}()(\vec x_0) \leq \varepsilon$ for any $i, j$.
    So we have that
    \begin{equation}
        \abs{\Delta(\vec h)} \leq \frac{1}{2}  \sum^d_{i = 1} \sum^d_{j = 1} \abs{h_i h_j} \varepsilon
    \end{equation}

    Finally, $h_i \leq \norm{\vec{h}}$, $h_j \leq \norm{\vec{h}}$ so $h_i h_j \leq \norm{\vec{h}}^2$.

    We conclude that if $\norm{h} \leq \delta$, then
    \begin{equation}
        \abs{\Delta(\vec h)} \leq^2 \frac{d}{2} \norm{h}^2 \varepsilon
    \end{equation}
    where the $d^2$ term comes from the fact that we have a sum of $d^2$ terms.
    This is enough to conclude that
    \begin{equation}
        \lim_{h \to 0} \frac{\abs{\Delta(\vec h)}}{\norm{\vec h}^2}
    \end{equation}
\end{proof}

\subsection{Minimum and maximum}

\begin{definition}[minimum and maximum]
    Let $f: D\subseteq \R^d \to \R$ then
    \begin{itemize}
        \item $\vec x_0$ is a \emph{global minimum} if $\forall \vec x \in D$, $f(\vec x) \geq f(\vec x_0)$
        \item $\vec x_0$ is a \emph{global maximum} if $\forall \vec x \in D$, $f(\vec x) \leq f(\vec x_0)$
        \item $\vec x_0$ is a \emph{local minimum} if $\exists r > 0$ such that $\forall \vec x \in D \cap B_c(\vec x_0, r)$, $f(\vec x) \geq f(\vec x_0)$
        \item $\vec x_0$ is a \emph{local maximum} if $\exists r > 0$ such that $\forall \vec x \in D \cap B_c(\vec x_0, r)$, $f(\vec x) \leq f(\vec x_0)$
    \end{itemize}

    We can also define the \emph{strict} version of these points which have a $<$ instead of $\leq$ and a $>$ instead of $\geq$.
\end{definition}

\begin{definition}[critical point]
    A point $\vec x_0 \in \mathring{D}$ where $f$ is differentiable and $\nabla f(\vec x_0)= \vec 0$ is called a critical point.
\end{definition}

\subsubsection{Necessary condition}

\begin{proposition}[necessary contidion for local extrema]
    Let $f: D\subseteq \R^d \to \R$, $x_0 \in \mathring{D}$.
    If $\vec x_0$ is a point of local extremum and the directional derivative in the direction $\vec u \in \R^d$ exists, then $\pdv{f}{u}()(\vec x_0) = 0$.
    In particular if $f$ is differentiable at $x_0$, then $\nabla f(\vec x_0) = 0$.
\end{proposition}

\begin{remark}
    This is the extension of Fermat's theorem to higher dimension.
\end{remark}

\begin{proof}
    If $x_0$ is a local minimum and $\pdv{f}{u}()(\vec x_0)$ exists consider $g: t \mapsto f(\vec x_0, t \vec u)$.
    Then $g$ has a local minimum at $t = 0$ so $g'(0) = 0$ (by Fermat's theorem in 1D),
    but $g'(0) = \pdv{f}{u}()(\vec x_0) = 0$

    If $f$ is differentiable $\pdv{f}{u}()(\vec x_0) = \nabla f (\vec x_0) \cdot \vec u$, therefore $\nabla f(\vec x_0)$ should be equal to $\vec 0$.

    We have just proven that local extrema are critical points.
\end{proof}

\subsubsection{Sufficient condition}

In 1D we used to check second derivatives, so now we have to check the Hessian matrix.

Since we are in a critical point second order Taylor expansion looks like
\begin{equation}
    f(\vec x_0 + \vec h) = f(\vec x_0) + \frac{1}{2} \vec h^T H \vec h + \o(\norm{\vec h}^2)
\end{equation}
where $H = \Hg(\vec x_0)$.
Then $f(\vec x_0 + \vec h) - f(\vec x_0) \sim \vec h^T H \vec h$ so it should have the sign of $\vec h^T H \vec h$.

\begin{definition}[\say{sign} of a matrix]
    Let $H$ be a $d\times d$ symmetric matrix. We say $H$ is
    \begin{itemize}
        \item \emph{positive definite} if $\vec h^T H \vec h > 0$ for $\vec h \in \R^d \setminus \{0\}$;
        \item \emph{negative definite} if $\vec h^T H \vec h < 0$ for $\vec h \in \R^d \setminus \{0\}$;
        \item \emph{indefinite} if $\vec h^T H \vec h$ takes both strictly negative and strictly positive values.
    \end{itemize}
\end{definition}

\begin{theorem}[spectral theorem (yay!)]
    Let $H$ be a symmetric matrix with real (not in $\C$) coefficients.
    Then $H$ is diagonalizable in an orthonormal basis.
    Moreover:
    \begin{enumerate}[label=\roman*.]
        \item $H$ is positive definite if all the eigenvalues are strictly positive
        \item $H$ is negative definite if all the eigenvalues are strictly negative
        \item $H$ is indefinite if some eigenvalues are strictly positive and some are strictly negative
    \end{enumerate}
\end{theorem}
\begin{proof}
    See linear algebra D:
\end{proof}

\begin{theorem}[sufficient condition for local extrema]
    Let $f: D\subseteq \R^d \to \R$ of class $C^2$ and $\vec x_0$ a critical point.
    Let $H = \Hg(\vec x_0)$.
    Then
    \begin{enumerate}[label=\roman*.]
        \item If $H$ is positive definite then $\vec x_0$ is a strict local minimum
        \item If $H$ is negative definite then $\vec x_0$ is a strict local maximum
        \item If $H$ is indefinite then $\vec x_0$ is not a local minimum nor a local maximum: we call it a \say{saddle point}
    \end{enumerate}
\end{theorem}

\begin{lemma}[$d = 2$]
    Let $H$ be a $2 \times 2$ matrix:
    \begin{enumerate}[label=\roman*.]
        \item If $\det(H) > 0$ and $\tr(H) > 0$ then $H$ is positive definite
        \item If $\det(H) > 0$ and $\tr(H) < 0$ then $H$ is negative definite
        \item If $\det(H) < 0$ then $H$ is indefinite
    \end{enumerate}
\end{lemma}

\begin{proof}
    Let $\lambda, \nu$ be eigenvalues of $H$, then
    \begin{equation}
        \begin{cases}
            \det(H) = \lambda \nu \\
            \tr(H) = \lambda + \nu
        \end{cases}
    \end{equation}

    If $\det(H) > 0$ both eigenvalues have the same sign and the sign is the same as $\tr{H} = \lambda + \nu$.
    If $\det(H) < 0$ then one of the eigenvalues is $> 0$ and the other one is $< 0$.
\end{proof}

\section{Intro to integration}

\subsubsection{Motivation}

We are anticipating some results of this topic because we will need them in other classes, such as probability and physics.
For example:
\begin{itemize}
    \item Let $f(x, y)$ be the probability density function of a pair of random variables, then
          \begin{equation}
              \iint_D f(x, y) \dd{x} \dd{y} = P((X, Y) \in D)
          \end{equation}
    \item Computing the gaussian integral by using double integrals
          \begin{equation}
              \int_{-\infty}^{+\infty} e^{-x^2} \dd{x} = \sqrt{\pi}
          \end{equation}
\end{itemize}

\subsection{Definition}

The intuition is that we want to calculate the volume under the graph of a function $f: D \subseteq \R^2 \to \R$.
We write it as
\begin{equation}
    \iint_D f(x, y) \dd{x} \dd{y}
\end{equation}
where $D$ is a region in the plane in which we want to calculate the volume.

As with normal integrals, if $f$ takes negative values we will subtract the volume under the graph from the volume above the graph.

Note that $D$ is not always a rectangle, so even defining the domain of integration is not trivial.

\subsubsection{Proprieties}

We have the following properties of double integrals:

\begin{itemize}
    \item Linearity:
          \begin{equation}
              \iint_D (af + bg) \dd{x} \dd{y} = a \iint_D f \dd{x} \dd{y} + b \iint_D g \dd{x} \dd{y}
          \end{equation}
    \item Domain decomposition: if $D = D_1 \cup D_2$ and $D_1 \cap D_2 = \varnothing$ then
          \begin{equation}
              \iint_D f \dd{x} \dd{y} = \iint_{D_1} f \dd{x} \dd{y} + \iint_{D_2} f \dd{x} \dd{y}
          \end{equation}
    \item Monotonicity: if $f \leq g$ then
          \begin{equation}
              \iint_D f \dd{x} \dd{y} \leq \iint_D g \dd{x} \dd{y}
          \end{equation}
    \item If $f = 1$ then
          \begin{equation}
              \iint_D 1 \dd{x} \dd{y} = \operatorfont{area}(D)
          \end{equation}
\end{itemize}

\subsection{Computational techniques}

\subsubsection{Fubini's iterated integrals}

The idea is to slice the volume in an infinite number of 2D areas and sum them up.

\begin{theorem}[Fubini]
    Let $D = [a, b] \times [c, d]$ and $f: D \to \R$ be continuous.
    Then
    \begin{equation}
        \iint_D f = \int_c^d \left( \int_{b}^{a} f(x, y) \dd{x} \right) \dd{y}
    \end{equation}

    If $D$ is not a rectangle the bound of integrations of the inner integral will be functions of $y$.
    \begin{equation}
        \iint_D f = \int_{c}^{d} \left( \int_{g(y)}^{h(y)} f(x, y) \dd{x} \right) \dd{y}
    \end{equation}
\end{theorem}

\begin{remark}
    We can also switch the order of integration, but we need to be careful with the bounds.
\end{remark}

\subsubsection{Change of variables}

Let $f: D \subseteq \R^2 \to \R$ and $\varphi: U \subseteq \R^2 \to \R^2$ be a bijective function such that $\varphi(D) = U$.

\begin{equation}
    \iint_D f(x, y) \dd{x} \dd{y} = \iint_U f(\varphi(u, v)) \abs{\det J_{\varphi}(u, v)} \dd{u} \dd{v}
\end{equation}

A common change of variables is to polar coordinates, where we have $\varphi(r, \theta) = (r \cos \theta, \allowbreak r \sin \theta)$, and $\det J_{\varphi}(r, \theta) = r$.

% Class of 18/04/2024

\section{Path integrals}

Let $f: D \subseteq \R^d \to R^p$, $\gamma: I:\subseteq \R \to \R^d$.
Then we write
\begin{eqnarray}
    \int_\gamma f
\end{eqnarray}
which is the path integral of $f$ along the path $\gamma$.
This could represent a length if $p = 1$ or a work along a trajectory with $p = d = 3$.

Note that the path integrals do not depend on the speed at which the path is traveled.

Moreover we can extend the fundamental theorem of calculus: $g = \int_\gamma \nabla g$.

\subsection{Scalar fields}

\begin{definition}[path integral over a scalar field]
    \label{def:integrals:path_over_scalar}
    Let $f: D \subseteq \R^d \to \R$ continuous, $\gamma: I \subseteq \R \to \R^d$ of class $C^1$, $I = [a, b]$ is an interval and $\gamma(I) \subseteq D$.
    Then we define
    \begin{equation}
        \int_\gamma f \dd{s} = \int_a^b f(\gamma(t)) \norm{\gamma'(t)} \dd{t}
    \end{equation}

    If $\gamma$ is continuous and piecewise $C^1$ we can split the interval in $a = a_1 \leq a_2 \leq \dots \leq a_n = b$ such that in each interval $[a_i, a_{i+1}]$ $\gamma$ is of class $C^1$.
    Then we can write
    \begin{equation}
        \int_{\gamma} f \dd{s} = \sum^{n -1}_{i = 1} \int^{a_{i+1}}_{a_i} f(\gamma(t)) \norm{\gamma'(t)} \dd{t}
    \end{equation}
\end{definition}

\begin{remark}
    The function $t \mapsto f(\gamma(t)) \norm{\gamma'(t)}$ is continuous if $f$ is continuous and $\gamma$ is $C^1$.
\end{remark}

The meaning of the $\norm{\gamma'(t)}$ is that we are splitting the path into many small linear segments and use $\gamma'$ to find the directions of these segments.

\begin{proposition}
    Let $f, \gamma$ as in \autoref{def:integrals:path_over_scalar}.
    For $N \in \N^+$ we look at the partition $[a, b]$ in $N$ intervals of the same length.

    Let $t_i = a + \frac{i}{N}(a-b)$
    then
    \begin{equation}
        \int_\gamma f \dd{s} = \lim_{N \to +\infty} \sum_{i = 0}^{N-1} f(\gamma(t_i)) \norm{\gamma(t_{i+1}) - \gamma(t_i)}
    \end{equation}
\end{proposition}

\begin{proof}
    From Analysis 1 we can use the definition of Riemann sum to get
    \begin{equation}
        \int_a^b f(\gamma(t)) \norm{\gamma'(t)} \dd{t} = \lim_{N \to +\infty} \sum_{i = 0}^{N-1} f(\gamma(t_i)) \norm{\gamma(t_i)} (t_{i+1} - t_i)
    \end{equation}
    but as $t_{i+1} - t_{i}$ goes to $0$ we have that $\gamma'(t_i) \approx \frac{\gamma(t_{i+1}) - \gamma{t_i}}{t_{i+1} - t_i}$ so
    $\norm{\gamma'(t_i)} (t_{i+1} - t_i) \approx \norm{\gamma(t_{i+1}) - \gamma(t_i)}$.

    In lecture notes there is a more formal proof where we use $\varepsilon$ instead of $\approx$.
\end{proof}

Moreover the $\dd{s}$ is
\begin{equation}
    \dd{s} = \norm{\gamma'(t)} \dd{t}
\end{equation}
which can be read as the change of speed times the increase in time, which is what makes the integral invariant to the speed.

\subsubsection{Independance of the parametrization}

\begin{definition}[diffeomorphism]
    Let $I, J \subseteq \R$ and $k \geq 1$.
    $\varphi$ is a $C^k$-diffeomorphism if it is a function $\varphi: I \to J$ which is bijective, of class $C^k$, with an inverse of class $C^k$.
\end{definition}

\begin{proposition}
    If $\varphi: I \to J$ is bijective and of class $C^k$, then $\varphi$ is a $C^k$-diffeomorphism if and only if $\varphi'$ does not vanish.
\end{proposition}

\begin{definition}
    Let $\gamma: I \to \R^d$, $\omega: J \to \R^d$ both of class $C^k$.
    We say that $(I, \gamma)$ and $(J, \omega)$ represent the same $C^k$-oriented curve id there exists a $C^k$-diffeomorphism $\varphi: I \to J$ increasing, with $\forall t \in I, \gamma(t) = \omega(\varphi(t))$.

    That is, a $C^k$-oriented curve is an equivalence class of $(I, \gamma)$ with $\gamma: I \to \R^d$ of class $C^k$.
\end{definition}

\begin{proposition}
    \label{prop:path_int:scalar_indep_speed}
    If $(I, \gamma)$ and $(J, \omega)$ represent the same $C^1$-oriented curve and if $f: D \subset \R^d \to \R$  is a continuous function with $\Im(\gamma) \subseteq D$, then
    \begin{equation}
        \int_\gamma f \dd{s} = \int_\omega f \dd{s}
    \end{equation}
\end{proposition}

\begin{proof}
    Let $I = [a, b]$, $\varphi: I \to J$ a $C^1$ diffeomorphism such that $\gamma = \omega \circ \varphi$. Then $J = [\varphi(a), \varphi(b)]$.
    Note that
    \begin{equation}
        \gamma'(t) = \varphi'(t)\omega'(\varphi(t))
    \end{equation}

    We substitute into the integral
    \begin{align}
        \int_\gamma f \dd{s} & = \int_a^b f(\gamma (t)) \norm{\gamma'(t)} \dd{t}                                                                                        \\
                             & =\int_a^b f\left(\omega\left(\varphi (t)\right)\right) \norm{\omega'\left(\varphi(t)\right)} \varphi'(t) \dd{t} \label{eq:path_int:norm} \\
                             & = \int_{\varphi(a)}^{\varphi(b)} f\left(\omega (r)\right) \norm{\omega'(r)} \dd{r} \label{eq:path_int:change_var}                        \\
                             & = \int_\omega f \dd{s}
    \end{align}

    Where in \autoref{eq:path_int:change_var} we performed a change of variable with $r = \varphi(t)$ and $\dd{r} = \varphi'(t) \dd{t}$.

    Moreover, we used the fact that $\varphi'(t) > 0$ both in \autoref{eq:path_int:norm} (in order to take it out of the norm) and when we defined the interval $J$ (otherwise it would be inverted).
    Note that if $\varphi'(t) < 0$ the result would still be valid since the two minus signs would cancel.
\end{proof}

\subsubsection{Length of a curve}

\begin{definition}[lentgth of a curve]
    If $\gamma: I \to \R^d$ of class $C^1$, the length of $\phi$ is
    \begin{equation}
        \int_\gamma 1 \dd{s} = \int_I \norm{\gamma'(t)} \dd{t}
    \end{equation}
\end{definition}

\begin{lemma}
    If $(I, \gamma)$ and $(J, \omega)$ represent the same oriented curve they have the same length.
\end{lemma}

\begin{example}[length of $y = x^2$ between $(0,0)$ and $(1,1)$]
    Let $\gamma(t) = \begin{pmatrix}
            t \\ t^2
        \end{pmatrix}, t \in [0, 1]$,
    then $\gamma'(t) = \begin{pmatrix}
            1 \\ 2t
        \end{pmatrix}$
    so $\norm{\gamma'(t)} = \sqrt{1+(2t)^2}$.
    Then the length of $\gamma$ is
    \begin{align}
        {\operatorfont length}(\gamma) & = \int_0^1 \sqrt{1 + 4t^2} \dd{t}                                        \\
                                       & = \frac{1}{2} \int_0^2 \sqrt{1+x^2} \dd{x}                               \\
                                       & = \frac{1}{2} \int_0^{\sinh^{-1}(2)} \sqrt{1+\sinh^2(u)} \cosh(u) \dd{u} \\
                                       & = \frac{1}{2} \int_0^{\sinh^{-1}(2)} \cosh^2(u) \dd{u}                   \\
                                       & = \frac{1}{8} \int_0^{\sinh^{-1}(2)} (e^{2u} + e^{-2u} + 2) \dd{u}       \\
                                       & = \text{something}
    \end{align}

    As you can see, shit gets hard.
\end{example}

\begin{example}[length of ellipse]
    Let $\gamma(t) = \begin{pmatrix}
            a \cos (t) \\
            b \cos (t)
        \end{pmatrix}, t \in[0, 2\pi]$.
    Then
    \begin{equation}
        {\operatorfont length}(\gamma) = \int_0^{2\pi} \sqrt{(a \sin(t))^2 + (b \cos (t))^2} \dd{t}
    \end{equation}
    this is very hard and it's even possible to prove that this integral cannot be expressed with usual functions
\end{example}

\begin{definition}[normal parametrization]
    A $C^1$ parametric curve $(I, \gamma)$ has a normal parametrization if $\norm{\gamma'(t)} = 1$ for any $t \in I$.
\end{definition}

\begin{proposition}
    Let $(I, \gamma)$ be a $C^k$ parametric curve with $k \geq 1$ such that all points are regular ($\gamma'(t) \neq \vec 0$ for all $t \in I$).
    Then there exists a $(J, \omega)$ with a normal parametrization representing $\gamma$.
\end{proposition}

\begin{proof}
    To simplify let $I = [a, b]$. Let $\varphi(t) = \int_a^t \norm{\gamma'(t)}\dd{t}$ which is the length of the curve between $\gamma(a)$ and $\gamma(t)$.
    Then $\varphi$ is of class $C^1$ and $\varphi'(t) = \norm{\gamma'(t)} > 0$ and $\varphi$ is a $C^1$ diffeomorphism (it is possible to prove that if $\gamma$ is $C^k$ then also $\varphi$ is $C^k$).

    TODO: complete here
\end{proof}

\subsection{Vector fields}

\begin{definition}[path integrals of a vector field]
    \label{def:path_int:vec_field}
    Let $f: D \subseteq \R^d \to \R^d$ be a continuous vector field, $\gamma: I \subseteq \R \to \R^d$ of class $C^1$ with $\gamma(I) \subseteq D$, where $I = [a, b]$.
    Then we define
    \begin{equation}
        \int_\gamma f \cdot \dd{\vec{s}} = \int_a^b f(\gamma(t)) \cdot \gamma'(t) \dd{t}
    \end{equation}
\end{definition}

\begin{remark}
    By expanding the dot product we get
    \begin{equation}
        \int_\gamma f \cdot \dd{\vec{s}} = \sum_{i = 0}^d \int_a^b f_i(\gamma(t))\gamma_i'(t) \dd{t}
    \end{equation}
\end{remark}

\begin{proposition}
    Let $f, \gamma$ as in \autoref{def:path_int:vec_field}.
    Let $t_0 = a, t_1, \dots, t_N = b$ be a partition of $[a, b]$ with constant time step.
    Then
    \begin{equation}
        \int_\gamma f \cdot \dd{\vec s} = \lim_{N \to \infty} \sum_{i = 0}^{N -1} f(\gamma(t_i)) \cdot \left( \gamma(t_{i+1} - \gamma_{t_i}) \right)
    \end{equation}
\end{proposition}

\begin{remark}
    In physics we could have that $f$ is a force, $\gamma$ is the position, and the path integral is the work of the force.
\end{remark}

\begin{proposition}
    Let $f: D \subseteq \R^d \to \R^d$, with $f$ of class $C^0$.
    Let $(I, \gamma)$, $(J, \omega)$ representing the same $C^1$-oriented curve with $\gamma(I) \subseteq D$.
    Then
    \begin{equation}
        \int_\gamma f \cdot \dd{\vec s} = \int_\omega f \cdot \dd{\vec s}
    \end{equation}
\end{proposition}

\begin{remark}
    The orientation of $\gamma$ and $\omega$ matters: if the orientation is different the sign of the integral will be different.
\end{remark}

\begin{proof}
    This is just an adaptation of the proof of \autoref{prop:path_int:scalar_indep_speed}.
\end{proof}

\begin{proposition}[triangle inequality for path integrals]
    \label{prop:path_int:triangle_ineq}
    Let $f, \gamma$ as in \autoref{def:path_int:vec_field}, then
    \begin{equation}
        \abs{\int_\gamma f \cdot \dd{\vec s}}  \leq \int_\gamma \norm{f} \dd{s} \leq {\operatorfont Length}(\gamma) \sup_D \norm{f}
    \end{equation}
\end{proposition}

\begin{proof}
    \begin{align}
        \abs{\int_\gamma f \cdot \dd{\vec s}} & = \abs{\int_a^b f(\gamma(t)) \cdot \gamma'(t) \dd{t}}                                          \\
                                              & \leq \int_a^b \abs{f(\gamma(t)) \cdot \gamma'(t) \dd{t}} \label{eq:path_int:triangle_triangle} \\
                                              & \leq \int_a^b \norm{f(\gamma(t))} \norm{\gamma'(t)} \dd{t} \label{eq:path_int:triangle_cauchy} \\
                                              & \leq \sup_D \norm{f} \int_a^b \norm{\gamma'(t)} \dd{t} \label{eq:path_int:triangle_bound}
    \end{align}
    where in \autoref{eq:path_int:triangle_triangle} we used the triangle inequality for integrals, in \autoref{eq:path_int:triangle_cauchy} we used Cauchy-Schwartz inequality, and in \autoref{eq:path_int:triangle_bound} we bounded $\norm{f(\gamma(t))}$ by $\sup_D \norm{f}$.
\end{proof}

\begin{theorem}[fundamental theorem of calculus for path integrals]
    Let $g: D \subseteq \R^d \to \R$, with $D$ open and $g$ of class $C^1$.
    Let $\gamma: I \subseteq \R \to \R^d$, with $I = [a, b]$, $\gamma$ of class $C^1$ and $\gamma(I) \subseteq D$
    Then
    \begin{equation}
        \int_\gamma \nabla g \cdot \dd{\vec s} = g(\gamma(b))-g(\gamma(a))
    \end{equation}
\end{theorem}

\begin{proof}
    \begin{align}
        \int_\gamma g \cdot \dd{\vec s} & = \int_a^b \nabla g(\gamma(t)) \cdot \gamma'(t) \dd{t}                                                \\
                                        & = \int_a^b h'(t) \dd{t} \quad \text{where } h(t) = g(\gamma(t)) \label{eq:path_int:fundamental_chain} \\
                                        & = h(b) - h(a) \label{eq:path_int:fundamental_fundamental}                                             \\
                                        & = g(\gamma(b)) - g(\gamma(a))
    \end{align}
    where in \autoref{eq:path_int:fundamental_chain} we applied the chain rule, and in \autoref{eq:path_int:fundamental_fundamental} we applied the fundamental theorem of calculus for 1D integrals.
\end{proof}

\begin{corollary}
    If $\gamma$ is a closed curve, that is $\gamma(a) = \gamma(b)$, then
    \begin{equation}
        \int_\gamma \nabla g \cdot \dd{\vec s} = 0
    \end{equation}

    If this condition holds we can let $f = \nabla g$ and we say that $f$ is a \emph{gradient field}.
\end{corollary}

\begin{proof}
    As $\gamma(a) = \gamma(b)$ then $g(\gamma(b)) - g(\gamma(a)) = 0$.
\end{proof}

\begin{corollary}
    If $g: D \subseteq \R^d \to \R$, with $g$ of class $C^1$ and $\sup_D \norm{\nabla g} < +\infty$ and $D$ convex then $g$ is Lipchitz:
    \begin{equation}
        \abs{g(\vec y) - g(\vec x)} \leq L \norm{\vec x - \vec y} \quad \text{with } L = \sup_D \norm{\nabla g}
    \end{equation}
\end{corollary}

\begin{proof}
    Let $\gamma$ be a parametrization of the straight line from $\vec x$ to $\vec y$.
    Then $g(\vec y) - g(\vec x) = \int_\gamma \nabla g \cdot \dd{\vec s}$
    So
    \begin{equation}
        \abs{g(\vec y) - g(\vec x)} = \abs{\int_\gamma \nabla g \cdot \dd{\vec s}} \leq L \cdot {\operatorfont Length}(\gamma) = \norm{\vec x - \vec y}
    \end{equation}
    by \autoref{prop:path_int:triangle_ineq}.
\end{proof}

\begin{lemma}
    Let $f$ be a constant function equal to $\vec u \in \R^d$.
    Then for any $\gamma$ of class $C^1$, $\gamma: [a, b] \to \R^d$,
    \begin{equation}
        \int_\gamma f \cdot \dd{\vec s} = \vec u \cdot (\gamma(b) - \gamma(a))
    \end{equation}
\end{lemma}
\begin{proof}
    We have that $f: \vec x \mapsto \vec u$ so $f = \nabla g$ with $g: \vec x \mapsto \vec u \cdot \vec x$.
    Then
    \begin{equation}
        \int_\gamma f \cdot \dd{\vec s} = g(\gamma(b)) - g(\gamma(a)) = \vec u \cdot \gamma(b) - \vec u \cdot \gamma(a) = \vec u \cdot (\gamma(b) - \gamma(a))
    \end{equation}

    Note that we had to find the right $g$ such that the vector field is the gradient of this potential $g$.
\end{proof}

\section{Integral of functions of two variables}

\subsection{Topological preliminaries}

\begin{definition}[compact set]
    In $\R^d$ a set $K \subseteq \R^d$ is called compact if it is \emph{bounded} and \emph{closed}.
\end{definition}
\begin{remark}
    Bounded means that $\exists R > 0$ such that $\forall \vec x \in K$, $\norm{\vec x} \leq R$.
\end{remark}

\begin{theorem}[bolzano-weierstrass]
    Let $K \in \R^d$ be a compact set and let $\left(\vec x_n\right)_{n \in \N}$ be a sequence in $K$.
    Then there exists $\varphi: \N \to \N$ strictly increasing such that $\left(\vec x_{\varphi(n)}\right)_{n \in \N}$ converges to $\vec x \in K$.
\end{theorem}

\begin{proof}
    For simplicity we assume $d = 2$. Let $\varphi_n$ be a sequence with components $x_n$ and $y_n$.
    Since $K$ is bounded $\norm{(x, y)} \leq R$ and for any $x_n, y_n$ we have that $\abs{x_n} \leq R, \abs{y_n} \leq R$.
    As $x_n$ is bounded in $\R$ we can use Bolzano-Weierstrass theorem in $\R$ to find a sequence $\varphi_1 : \N \to \N$ strictly increasing such that $x_{\varphi_1(n)}$ converges to $x$.
    Let $z_n = y_{\varphi_1(n)}$. Then $z_n$ is bounded in $\R$ and we can apply Bolzano-Weierstrass again to find $\varphi_2$.
    Then $\varphi = \varphi_1 \circ \varphi_2$ is the sequence such that $(x_{\varphi(n)}, y_{\varphi(n)})$ converges to $(x, y)$.
    As $K$ is closed $(x, y) \in K$.
\end{proof}

\begin{definition}[uniform conrinuity]
    Let $f: D \subseteq \R^d \to \R$. We say that $f$ is uniformly continuous in $D$ if
    \begin{equation}
        \forall \varepsilon > 0, \enspace \exists \delta > 0, \enspace \forall \vec x, \vec y \in D, \norm{\vec x - \vec y} \leq \delta \implies \abs{f(\vec x) - f(\vec y)} \leq \varepsilon
    \end{equation}

    The difference with \say{normal} continuity is that it is valid around any $\vec x \in D$, not just around some $\vec x_0$.
\end{definition}

\begin{proposition}[sequential characterization of uniform continuity]
    Let $f: D \subseteq \R^d \to \R$. $f$ is uniform continuous if and only if for every pair of $x_n, y_n$ of sequences in $D$, if $\lim_{n \to + \infty} (x_n - y_n) = \vec 0$ then $\lim_{n \to +\infty} [f(x_n) - f(y_n)] = 0$.
    Note that $x_n, y_n$ don't need to converge.
\end{proposition}

\begin{example}
    Consider $f: x \mapsto x^2$ with $x \in \R$. $f$ is continuous but not uniformly continuous.
    Consider $x_n = n$ and $y_n = n + \frac{1}{n}$, then $\lim (x_n - y_n) = 0$ but $\lim [f(x_n) - f(y_n)] = \lim (n^2 - n^2 - 2 - \frac{1}{n^2}) = -2$.
\end{example}

\begin{proof}
    \skiplineafterproof
    \begin{itemize}
        \item[$\implies$] (assume $f$ is uniformly continuous).
              Take $x_n, y_n$ such that $(x_n - y_n) \to \vec 0$.
              Fix $\varepsilon > 0$ and as $f$ is uniform continuous we can find $\delta > 0$ such that $\abs{f(\vec x) - f(\vec y)} \leq \varepsilon$ if $\norm{\vec x - \vec y} \leq \delta$.
              As $(x_n - y_n) \to \vec 0$ there exists $N$ such that $\abs{\vec x_n - \vec y_n} \leq \delta$ for all the $n \geq N$. So $\abs{f(\vec x_n) - f(\vec y_n)} \leq \varepsilon$ for $n \geq N$.
        \item[$\impliedby$] (assume that $\forall \vec x_n, \vec y_n$ where $\vec x_n - \vec y_n \to \vec 0$ implies $f(\vec x_n) - f(\vec y_n) \to 0$).
              By contradiction assume that $f$ is not uniformly continuous, that means we can find some $\varepsilon > 0$ such that for any $\delta$ the difference between the images is more than $\varepsilon$.
              Fix such $\varepsilon$ and consider $\delta = \frac{1}{n}$, then there exists $(\vec x_n, \vec y_n)$ such that $\norm{\vec x_n - \vec y_n} \leq \frac{1}{n}$ and $\abs{f(\vec x_n) - f(\vec y_n)} > \varepsilon$.
              But then $\abs{f(\vec x_n) - f(\vec y_n)} > \varepsilon > 0$ for every $n$ which means we cannot have $\lim \abs{f(\vec x_n) - f(\vec y_n)} = 0$ and we have reached a contradiction.
    \end{itemize}
\end{proof}

\begin{theorem}[heine]
    A continuous function defined ove a compact set is uniformly continuous.
\end{theorem}

\begin{proof}
    Let $f: K \subseteq \R^d \to \R$ with $K$ compact and $f$ continuous.
    By contradiction assume that $f$ is not uniformly continuous.
    This means that there exist two sequences $x_n, y_n$ such that $\lim (\vec x_n - \vec y_n) = \vec 0$ but $f(x_n) - f(y_n)$ does not converge to $0$.
    As $K$ is compact there exists $\varphi: \N \to \N$ strictly increasing and $\vec x \in K$ such that $\lim \vec x_{\varphi(n)} = \vec x$.
    We have that $\lim \vec y_{\varphi(n)} = \lim (\vec y_{\varphi(n)} - \vec x_{\varphi(n)} + \vec x_{\varphi(n)}) = \vec 0 + \vec x$.
    The by continuity of $f$ at $\vec x$ we have that $\lim f(\vec x_{\varphi(n)}) = \vec x = \lim f(\vec y_{\varphi(n)})$ therefore $f(\vec x_{\varphi(n)}) - f(\vec y_{\varphi(n)}) = 0$.
\end{proof}

\begin{theorem}[weierstrass]
    Let $f: K \subseteq \R^d \to \R$ with $K$ compact and $f$ continuous.
    Then $f$ is bounded and there exists $\vec x_\text{min}$ and $\vec x_\text{max}$ such that $f(\vec x_\text{min}) = \inf_K f$ and $f(\vec x_\text{max}) = \sup_K f$, that is, the infimum and supremum are attained.
\end{theorem}

\subsection{Definition of integral}

Consider $f: D \subseteq \R^2 \to \R$, we want to compute the volume under the graph of $f$.
We write
\begin{equation}
    \iint_D f(x, y) \dd{x} \dd{y}
\end{equation}

Note that even the domain can be complicated, as $D$ could be a rectangle but also some more complex shape.
We will start by considering rectangular domains of the form $D = [a, b] \times [c, d]$.

\begin{definition}[partition and step size]
    For an interval $[a, b]$ a partition $\mathcal{P}$ of $[a, b]$ is a set of ordered points
    \begin{equation}
        x_0 = a < x_1 < \dots < x_{n-1} < x_n = b
    \end{equation}
    Moreover the \emph{step size} of the partition is
    \begin{equation}
        \Delta(\mathcal{P}) = \sup_i (x_i - x_{i-1})
    \end{equation}
\end{definition}

\begin{definition}[rectangular partition]
    Let $\mathcal{P}_1$ be a partition of $[a, b]$ of the form of $x_0, \ldots, x_N$ and $\mathcal{P}_2$ be a partition of $[c, d]$ of the form of $y_0, \ldots, y_M$.
    We call $\mathcal{P}_1 \otimes \mathcal{P}_2$ the partition of $D=[a, b] \times [c, d]$ into rectangles:
    \begin{equation}
        R_{ij} = [x_{i-1}, x_i] \times [y_{j-1}, y_j] \enspace \text{for } \substack{i \in \{1, \ldots, N\} \\ j \in \{1, \ldots, M\}}
    \end{equation}
\end{definition}

\begin{definition}[upper and lower riemann sums]
    Let $f : D \to \R$ bounded and partitions $\mathcal{P_1}, \mathcal{P_2}$ of $D$.
    The upper and lower sums are
    \begin{align}
        L(f, \mathcal{P}_1 \otimes \mathcal{P}_2) & = \sum_{i = 1}^N \sum_{j = 1}^M \mathcal{A}(R_{ij}) \left( \inf_{R_{ij}} f \right) \\
        U(f, \mathcal{P}_1 \otimes \mathcal{P}_2) & = \sum_{i = 1}^N \sum_{j = 1}^M \mathcal{A}(R_{ij}) \left( \sup_{R_{ij}} f \right)
    \end{align}
    Note that $L(f, \mathcal{P}_1 \otimes \mathcal{P}_2) \leq U(f, \mathcal{P}_1 \otimes \mathcal{P}_2)$ by definition.
\end{definition}

\begin{definition}[integral over a rectangle]
    Let $D=[a, b] \times [c, d]$ and $f : D \to \R$ bounded.
    We say that $f$ is Riemann integrable if
    \begin{equation}
        \sup_{\mathcal P_1, \mathcal P_2}         L(f, \mathcal P_1 \otimes \mathcal P_2) = \inf_{\mathcal P_1, \mathcal P_2} U (f, \mathcal P_1 \otimes \mathcal P_2)
    \end{equation}
    in this case the common value of the $\sup$ and the $\inf$ is
    \begin{equation}
        \iint_D f(x, y) \dd{x} \dd{y}
    \end{equation}
\end{definition}

\begin{remark}[orientation]
    For 2D integrals there is no notion of orientation of the domain like we had in 1D.
    We will only consider rectangles with $a \leq b, c \leq d$.
\end{remark}

\subsection{Theorems}

\begin{proposition}[continuity implies integrability]
    Let $D=[a, b] \times [c, d]$ and $f : D \to \R$ continuous.
    Then $f$ is Riemann integrable over $D$.
\end{proposition}

\begin{proof}
    Fix $\varepsilon > 0$, we want to conclude that $U(f, \mathcal P_1 \otimes \mathcal P_2) - L(f, \mathcal P_1 \otimes \mathcal P_2) \leq \varepsilon$.
    As $D$ is compact, $f$ is uniformly continuous and bounded, so there exists $\delta > 0$ such that if $(x, y), (x', y')$ such that $\norm{(x, y) - (x', y')} \leq \delta$ then $\abs{f(x, y) - f(x', y')}\leq \varepsilon$.
    Choose $\mathcal P_1, \mathcal P_2$ partitions of $[a, b]$ and $[c, d]$ of step $\leq \delta / \sqrt{2}$. If $R = R_{ij} \in \mathcal P_1 \otimes \mathcal P_2$ then, for $(x, y), (x', y') \in R_{ij}$, we have $\abs{f(x, y) - f(x', y')}\leq \varepsilon$ so that $(\sup_{R_{ij}} f) - (\inf_{R_{ij}} f) \leq \varepsilon$.
    By multiplying by $\mathcal A(R_{ij})$ and summing we obtain the result.
\end{proof}

\begin{definition}[integrability in a non-rectangular domain]
    Let $D\subset \R^2$ bounded and $f:D \to \R$ a bounded function.
    Define a new function
    \begin{equation}
        \widetilde{f} : (x, y) \in \R^2 \mapsto \begin{cases}
            f(x, y) & \text{if } (x, y) \in D \\
            0       & \text{otherwise}
        \end{cases}
    \end{equation}
    and let $\widetilde{D}$ be a rectangle large enough to contain $D$.
    We say that $f$ is Riemann integrable over $D$ if $\widetilde{f}$ is Riemann integral over $\widetilde{D}$. and in this case
    \begin{equation}
        \iint_D f(x, y) \dd{x} \dd{y} = \iint_{\widetilde{D}} \widetilde{f}(x, y)\dd{x}\dd{y}
    \end{equation}
    Note that $\widetilde{f}$ is almost never continuous.
\end{definition}

\begin{definition}[Jordan measurable domains]
    Let $D \subseteq \R^2$ bounded and $I_D$ be the indicator of $D$.
    Then $D$ is Jordan measurable if $I_D$ is Riemann integrable, and in this case we write the area of $D$ as
    \begin{equation}
        \mathcal A(D) = \iint_{\widetilde{D}} I_D(x, y) \dd{x} \dd{y}
    \end{equation}
\end{definition}

\begin{proposition}
    Let $\psi, \varphi: [a, b] \to \R$ continuous such that $\varphi \leq \psi$.
    Let
    \begin{equation}
        D = \left\{(x, y) \in \R \middle| \substack{a \leq x \leq b \\ \varphi(x) \leq y \leq \psi(x)}\right\}
    \end{equation}
    Then $D$ is Jordan measurable.
\end{proposition}

\begin{theorem}
    Let $D \subseteq \R^2$ be compact and Jordan measurable.
    Let $f: D \to \R$ be continuous.
    Then $f$ is Riemann integrable over $D$.
\end{theorem}

\begin{proposition}
    Let $D \subseteq \R^2$ be bounded, $f, g : D \to \R$ Riemann integrable and $a, b \in \R$.
    \begin{enumerate}[label=\roman*.]
        \item (linearity) $af + bg$ is Riemann integrable and
              $\iint_D (af + bg) = a \iint_D f + b \iint_D g$
        \item (positivity) If $f \geq 0$ over $D$ then $\iint_D f \geq 0$
        \item (monotonicity) If $f \leq g$ over $D$ then $\iint_D f \leq \iint_D g$
    \end{enumerate}
\end{proposition}

\subsection{Fubini's theorem}

\begin{theorem}[Fubini's theorem for rectangles]
    Let $D = [a, b] \times [c, d]$ and $f: D \subseteq \R^2 \to \R$ continuous.
    Then $y \mapsto \int_a^b f(x, y) \dd{x}$ and $x \mapsto \int_c^d f(x, y) \dd{y}$ are continuous and
    \begin{equation}
        \iint_D f(x, y) \dd{x}\dd{y} = \int_c^d \left(\int_a^b f(x,y) \dd{x}\right) \dd{y} = \int_a^b \left(\int_c^d f(x,y) \dd{y}\right) \dd{x}
    \end{equation}
\end{theorem}
\begin{proof}
    Let us first prove that $g(y) = \int_a^b f(x, y) \dd{x}$ and $h(x) = \int_c^d f(x, y) \dd{y}$ are continuous.

    Fix $\varepsilon > 0$. As $f$ is uniformly continuous (since $D$ is compact and $f$ is continuous), there is $\delta > 0$ such that
    \begin{equation}
        \norm{(x,y)- (x',y')} \leq \delta \implies \abs{f(x, y) - f(x', y')} \leq \varepsilon
    \end{equation}
    so if $y, y'$ are such that $\abs{y-y'} \leq \delta$ then $\abs{f(x,y) - f(x,y')} \leq \varepsilon$ for any $x$.
    We have
    \begin{align}
        \abs{g(y) - g(y')} & = \abs{\int_a^b f(x,y) - f(x,y') \dd{x}}             \\
                           & \leq \int_a^b \abs{f(x, y) - f(x, y')}               \\
                           & \leq \int_a^b \varepsilon \dd{x} = (b-a) \varepsilon
    \end{align}
    that is, if $\abs{y-y'} \leq \delta$ then $\abs{g(y) - g(y')} \leq (b-a)\varepsilon$, which means $g$ is uniformly continuous.
    We can repeat the argument for $h(x)$.

    Consider now the partitions $\mathcal P_1$ of $[a, b]$ and $\mathcal P_2$ of $[c, d]$.
    Then consider the rectangles $R_{ij} = [x_{i-1}, x_i] \times [y_{i-1}, y]$: let $l_{ij} = \inf_{R_{ij}} f$, $u_{ij} = \sup_{R_{ij}} f$ and $\mathscr A(R_{ij}) = (y_i -y{i-1}) (x_i - x_{i-1})$.

    We can now write the Riemann sums as
    \begin{align}
        L(f, \mathcal P_1 \otimes \mathcal P_2) & = \sum_{j = 1}^M \sum_{i = 1}^N (y_i -y{i-1}) (x_i - x_{i-1}) l_{ij} \\
                                                & = \sum_{j = 1}^M (y_i -y{i-1}) \sum_{i = 1}^N (x_i - x_{i-1}) l_{ij}
    \end{align}
    note that for every $y \in [y{i-1}, y_i]$, $(x_i - x_{i-1}) l_{ij} \leq \int_{x_{i-1}}^{x_i} f(x, y)\dd{y}$ because $f(x, y) \geq l_{ij}$.
    If we take the sum we get that \begin{equation}
        \sum_{i = 1}^N (x_i -x_{i-1})l_{ij} \leq \int_a^b f(x, y) \dd{x}
    \end{equation}
    As this is valid for any $y$ we can write
    \begin{equation}
        \sum_{i = 1}^N (x_i -x_{i-1})l_{ij} \leq \min_{y \in [y_{i-1}, y_i]} \int_a^b f(x, y) \dd{x}
    \end{equation}
    but then we can write the lower sum as
    \begin{align}
        L(f, \mathcal P_1 \otimes \mathcal P_2) & \leq \sum_{j = 1}^M (y_i -y{i-1}) \sum_{i = 1}^N \min_{y \in [y_{i-1}, y_i]} \int_a^b f(x, y) \dd{x} \\
                                                & \leq \int_c^d g(y) \dd{y}                                                                            \\
                                                & = \int_c^d \left( \int_a^b f(x, y) \dd{x} \right) \dd{y}
    \end{align}

    By a similar argument we can get that
    \begin{equation}
        U(f, \mathcal P_1 \otimes \mathcal P_2) \geq \int_c^d \left( \int_a^b f(x, y) \dd{x} \right) \dd{y}
    \end{equation}
    therefore, combing the two inequalities we get
    \begin{equation}
        L(f, \mathcal P_1 \otimes \mathcal P_2) \leq \int_c^d \left( \int_a^b f(x, y) \dd{x} \right) \dd{y} \leq U(f, \mathcal P_1 \otimes \mathcal P_2)
    \end{equation}
    moreover
    \begin{equation}
        \sup_{\mathcal P_1, \mathcal P_2} L(f, \mathcal P_1 \otimes \mathcal P_2) \leq \int_c^d \left( \int_a^b f(x, y) \dd{x} \right) \dd{y} \leq \inf_{\mathcal P_1, \mathcal P_2} U(f, \mathcal P_1 \otimes \mathcal P_2)
    \end{equation}
    and since $f$ is continuous we get the result.
\end{proof}

\begin{corollary}
    Assume that $f(x, y) = g(x)h(y)$ with $g:[a,b] \to \R$ and $h:[c,d] \to \R$.
    Then
    \begin{equation}
        \iint_D f(x, y) \dd{x}\dd{y} = \left(\int_a^b g(x)\dd{x}\right) \left(\int_c ^d g(y)\dd{y}\right)
    \end{equation}

    This result is particularly useful in probability.
\end{corollary}

\begin{proof}
    \begin{align}
        \iint_D f(x, y) \dd{x}\dd{y} & = \int_a^b \left(\int_c^d g(x)h(y) \dd{y}\right) \dd{x}              \\
                                     & = \int_a^b g(x) \left(\int_c^d h(y) \dd{y}\right) \dd{x}             \\
                                     & = \left(\int_a^b g(x)\dd{x}\right) \left(\int_c ^d g(y)\dd{y}\right)
    \end{align}
\end{proof}

\begin{theorem}[Fubini general form]
    Let $\varphi, \phi: [a, b] \to \R$ continuous such that $\varphi \leq \psi$.
    Let $D = \left\{ (x, y) : a \leq x \leq b, \varphi(x) \leq y \leq \psi(x) \right\}$.
    Then, if $f: D \to \R$ is continuous we have
    \begin{equation}
        \iint_D f(x, y) \dd{x} \dd{y} = \int_a^b \left(\int_{\varphi(x)}^{\psi(x)} f(x, y) \dd{y}\right) \dd{x}
    \end{equation}
\end{theorem}

\subsection{Change of variable}

\begin{theorem}
    Let $\varphi: \tilde{U} \subseteq \R^2 \to D \subseteq \R^2$ with $\tilde{U}$ open, $\varphi$ of class $C^1$ and injective over $\tilde{U}$.
    Let $U$ be a Jordan measurable domain included in $\tilde{U}$.
    Define $D = \varphi(U)$.
    Assume that $J_\varphi (u,v)$ is invertible for any $(u, v) \in U$.
    Then $D$ is Jordan measurable and for any continuous function $f:D \to \R$
    \begin{equation}
        \iint_D f(x, y) \dd{x}\dd{y} = \iint_U f(\varphi(u, v)) \abs{\det J_{\varphi}(u, v)} \dd{u} \dd{v}
    \end{equation}
\end{theorem}
\begin{proof}
    This is just the idea of the proof.

    Consider $R_{ij}$ the rectangles that partition $U$, with $i \in \{1, \ldots,N\}, j \in \{1, \ldots,M\}$.
    We have that
    \begin{equation}
        \int_D f(x, y) \dd{x} \dd{y} \approx \sum_{i,j} f_{ij} \mathscr A (\varphi(R_{ij}))
    \end{equation}
    then each area is approximately equal to the area given by the parallelogram given by the linear approximations.
    We have that the two sides of the parallelogram will be
    \begin{gather}
        (u_i - u_{i-1}) \pdv{\varphi}{u}()(u_{i-1}, v_{j-1})
        (v_j, v_{j-1}) \pdv{\varphi}{v}()(u_{i-1}, v_{j-1}) \\
    \end{gather}
    and we know that the area of the parallelogram can be calculated with the determinant:
    \begin{align}
        \mathscr A(\varphi(R_{ij})) & = \abs{\det \begin{pmatrix}
                                                          (u_i - u_{i-1}) \pdv{\varphi}{u}()(u_{i-1}, v_{j-1}) \\
                                                          (v_j, v_{j-1}) \pdv{\varphi}{v}()(u_{i-1}, v_{j-1})
                                                      \end{pmatrix}}               \\
                                    & = (v_j, v_{j-1})(u_i - u_{i-1}) \abs{\det \begin{pmatrix}
                                                                                        \pdv{\varphi}{u}()(u_{i-1}, v_{j-1}) \\
                                                                                        \pdv{\varphi}{v}()(u_{i-1}, v_{j-1})
                                                                                    \end{pmatrix}} \\
                                    & = \mathscr A(R_{ij}) \abs{\det J_\varphi}
    \end{align}

    Then we can write
    \begin{align}
        \int_D f(x, y) \dd{x} \dd{y} & \approx \sum_{i,j} f_{ij} \mathscr A (\varphi(R_{ij}))                         \\
                                     & \approx \sum_{i,j} f_{ij} \mathscr A(R_{ij}) \abs{\det J_\varphi}              \\
                                     & \approx \sum_{i,j} f(\varphi(u_i, v_j)) \abs{\det J_\varphi}\mathscr A(R_{ij}) \\
                                     & \approx \int_U f(\varphi(u_i, v_j)) \abs{\det J_\varphi} \dd{u} \dd{v}
    \end{align}
\end{proof}

\begin{corollary}[linear $\varphi$]
    Let $M$ be a $2\times 2$ invertible matrix.
    Let $U \subseteq \R^2$ Jordan measurable and $D = \left\{ M \begin{pmatrix} u \\ v\end{pmatrix}, (u, v) \in U \right\}$.
    Then for any $f: D \to \R$ continuous
    \begin{equation}
        \iint_D f(x, y) \dd{x}\dd{y} = \abs{\det M} \iint_D f\left(M\begin{pmatrix} u \\ v\end{pmatrix}\right) \dd{u} \dd{v}
    \end{equation}
\end{corollary}
\begin{proof}
    Let $\varphi(u, v) = M\begin{pmatrix} u \\ v\end{pmatrix}$. Then $J_\varphi(u, v) = M$ for any $(u, v)$.
    So $abs{\det J_\varphi (u, v)} = \abs{\det M}$ and we can apply the theorem.
\end{proof}

\end{document}