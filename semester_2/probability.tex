\documentclass[10pt]{extarticle}
\title{Probability Notes}
\author{Giacomo Ellero}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{dirtytalk}
\usepackage{parskip}
\usepackage{mathrsfs}
\usepackage[many]{tcolorbox}
\usepackage{xparse}
\usepackage[a4paper,margin=1.5cm]{geometry}
\usepackage{bookmark}
\usepackage{cancel}

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\newcommand{\indep}{\perp \!\!\! \perp}

\newenvironment{absolutelynopagebreak}
  {\par\nobreak\vfil\penalty0\vfilneg
   \vtop\bgroup}
  {\par\xdef\tpd{\the\prevdepth}\egroup
   \prevdepth=\tpd}

\newtcolorbox{examplebox}[1]{colback=green!5!white,colframe=green!40!black,title={#1},fonttitle=\bfseries,parbox=false}
\newtcolorbox{notebox}[1]{colback=blue!5!white,colframe=blue!40!black,title={Note: #1},fonttitle=\bfseries,parbox=false}
\newtcolorbox{bluebox}[1]{colback=blue!5!white,colframe=blue!40!black,title={#1},fonttitle=\bfseries,parbox=false}
\newtcolorbox{warningbox}[1]{colback=orange!5!white,colframe=orange!90!black,title={Warning: #1},fonttitle=\bfseries,parbox=false}

\begin{document}

\maketitle
\tableofcontents
\clearpage

\section{Class of 05/02/2024}

\subsection{Interpretations of probability}

Probability has had many interpretations over the years, here's some of them:
\begin{enumerate}
    \item \textbf{Classical interpretation}: Games of chances, all the outcomes are equally likely.
          We can calculate probability as the number of favorable outcomes over the total.
          This only works on finite spaces with perfect symmetries.
    \item \textbf{Frequency interpretation}: This is the probability calculated in the long run by repeating the experiment infinitely many times.
          This only works if the experiment is repeatable. What if we cannot take the limit?
    \item \textbf{Subjective interpretation}: How much are people willing to bet on something to happen?
\end{enumerate}

\subsection{Sample spaces and outcomes}

In an \textbf{experiment} we call \textbf{sample space} $\Omega$ the set of all possible outcomes,
while a singular outcome is denoted as $\omega$.
$\Omega$ can be finite, countable or uncountable.

\begin{examplebox}{Example}
    \begin{itemize}
        \item Rolling a die: $\Omega = \{1, 2, 3, 4, 5, 6\}$
              \begin{itemize}
                  \item This is sample space is finite.
              \end{itemize}
        \item Tossing a coin until heads comes up: $\Omega = \{H, TH, TTH, \dots, TTT\dots\}$
              \begin{itemize}
                  \item This is sample space is not finite, but it is countable.
              \end{itemize}
        \item Dart in a circular target of radius $r$: $\Omega = \{(x, y) \in \R^2 : x^2 + y^2 \le r\}$
              \begin{itemize}
                  \item This is sample space is uncountable.
              \end{itemize}
    \end{itemize}
\end{examplebox}

\begin{notebox}{impossible $\ne$ probability 0}
    Despite some elements having probability 0, it doesn't mean they are impossible.

    \begin{itemize}
        \item In the dart example, the probability of hitting a single point is 0 since $\Omega$ is uncountable, but it's not impossible.
        \item In the coin example, the probability of getting an infinite number of tails is 0, but it's not impossible.
    \end{itemize}

    In particular note that if $\Omega$ is uncountable every singular outcome has probability 0.
\end{notebox}

\subsection{Events}

An \textbf{event} is a subset of the sample space of all the outcomes such that the event occurs.
We denote it as $A \subseteq \Omega$.

In particular:
\begin{itemize}
    \item The \textbf{empty set} $\varnothing$ is the impossible event.
    \item The \textbf{whole sample space} $\Omega$ is the certain event.
\end{itemize}

We can define the following operations on events:
\begin{itemize}
    \item \textbf{Complement}: $A^c$, the event occurs if $A$ doesn't.
    \item \textbf{Union}: $A \cup B$, the event occurs if $A$ \textit{or} $B$ occurs.
    \item \textbf{Intersection}: $A \cap B$, the event occurs if $A$ \textit{and} $B$ occur.
    \item \textbf{Difference}: $A \setminus B$, the event occurs if $A$ occurs but $B$ doesn't.
    \item \textbf{Symmetric difference}: $A \triangle B$, the event occurs if $A$ or $B$ occur but not both.
\end{itemize}

Moreover we say that $A$ \textbf{depends} of $B$ if $A \subseteq B$. This means that if $B$ occurs, then $A$ also occurs.

\begin{examplebox}{Example}
    Say we toss a coin twice, then the sample space is $\Omega = \{HH, HT, TH, TT\}$.

    We define the following events:
    \begin{itemize}
        \item $A = \{\text{first toss is a head}\} = \{HH, HT\}$
        \item $B = \{\text{2 times heads}\} = \{HH\}$
    \end{itemize}

    We have that $A \subseteq B$, so $A$ depends on $B$.
\end{examplebox}

\begin{notebox}{singleton events}
    Events that contain only one outcome are called \textbf{singleton events}.
\end{notebox}

\subsection{Sigma-algebras}

We usually denote a sigma-algebra as $\F$ and it's a subset of the power set of $\Omega$.
Every sigma-algebra, by definition, has the following properties:
\begin{enumerate}
    \item $\Omega \in \F$
    \item If $A \in \F$, then $A^c \in \F$
    \item If $A_1, A_2, \dots \in \F$, then $\bigcup_{i=1}^\infty A_i \in \F$
\end{enumerate}

\begin{notebox}{notation}
    We introduce the following notation:
    $$
        \bigcap_{i=1}^\infty A_i \qquad \text{ and } \qquad \bigcup_{i=1}^\infty A_i
    $$

    These symbols just denote the intersection and the union respectively for a infinite, countable set of events.
\end{notebox}

\subsubsection{Proprieties of sigma-algebras}

Moreover, we can prove the following properties:
\begin{enumerate}
    \item $\varnothing \in \F$
    \item $\F$ is closed under finite unions
    \item $\F$ is closed under finite and countable intersections
\end{enumerate}

\begin{proof}
    According to the properties of sigma-algebras we have that:

    \begin{enumerate}
        \item $\Omega \in \F$ by $(1)$, so $\Omega^c = \varnothing \in \F$, by $(2)$.
        \item Let $A_1 \cup A_2 \cup \dots \cup A_n$, we extend it by adding $\varnothing$ to the end of the sequence.
              Then this is in $\F$ by $(3)$.
        \item According to De Morgan's laws we have:
              $$
                  \bigcap_{i=1}^n A_i = \left(\left(\bigcap_{i=1}^n A_i\right)^c\right)^c = \left(\bigcup_{i=1}^n A_i^c\right)^c
              $$
              Since the intersection is in $\F$ by $(3)$ and the complement is in $\F$ by $(2)$, then the union is also in $\F$.
    \end{enumerate}
\end{proof}

\begin{notebox}{Reminder of De Morgan's Laws}
    $$
        \left(\bigcup_{i=1}^n A_i\right)^c = \bigcap_{i=1}^n A_i^c
        \qquad \text{ and } \qquad
        \left(\bigcap_{i=1}^n A_i\right)^c = \bigcup_{i=1}^n A_i^c
    $$
\end{notebox}

\subsubsection{Borel sigma-algebra}

Suppose $\Omega$ is uncountable and we want to define a sigma-algebra on it.

We start by choosing a class $\mathcal C$ of subsets of $\Omega$ that we want to include in the sigma-algebra.
Then we define the sigma-algebra generated by $\mathcal C$ as the smallest sigma-algebra that contains $\mathcal C$.

In particular Borel sigma-algebra is the sigma-algebra generated by the open sets of $\R$:
$$
    \mathcal B (\R) = \sigma(\{(a, b) : -\infty < a < b < \infty\})
$$

This definition can be extended to $\R^n$ by taking the gaussian product sigma-algebra:
$$
    \mathcal B (\R^n) = \sigma(\{(a_1, b_1) \times \dots \times (a_n, b_n) : -\infty < a_i < b_i < \infty\})
$$

\section{Class of 07/02/2024}

Let $\Omega$ be a sample space and $\F$ a sigma-algebra on $\Omega$,
then we call $(\Omega, \F)$ a \textbf{measurable space}.

\subsection{Probability measure}

\begin{bluebox}{Definition}
    A \textbf{probability measure} is a function $P: \F \to \R$ such that:
    \begin{enumerate}
        \item $P(A) \ge 0$ for all $A \in \F$
        \item $P(\Omega) = 1$
        \item If $A_1, A_2, \dots \in \F$ are pairwise disjoint, then:
              $$
                  P\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=1}^\infty P(A_i)
              $$
    \end{enumerate}
\end{bluebox}

\subsubsection{Properties of probability measures}

We can prove the following properties of probability measures:
\begin{enumerate}
    \item $P(\bigcup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)$
    \item $P(A^c) = 1 - P(A)$
    \item If $A \subseteq B$, then $P(A) \le P(B)$
    \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
\end{enumerate}

\begin{proof}
    We will prove the properties in order:
    \begin{enumerate}
        \item We will not prove this but it's easy to see that it follows from point 3 of the definition.
        \item Consider the disjoint events $A$ and $A^c$.
              We have that $P(A \cup A^c) = P(\Omega) = 1$.

              Since $A$ and $A^c$ are disjoint, we can apply property 1 to get that
              \begin{align*}
                           & P(A) + P(A^c) = P(\Omega) = 1 \\
                  \implies & P(A^c) = 1 - P(A)
              \end{align*}
        \item Write $B$ as $B = A \cup (B \setminus A)$.
              We have that $A$ and $B \setminus A$ are disjoint, so:
              $$
                  P(B) = P(A) + P(B \setminus A) \ge P(A)
              $$
              since $P(X) \ge 0 \quad \forall X \in \F$.
              Moreover $P(B \setminus A) = P(B) - P(A)$.

              We say that probability is monotone.
        \item We can write $A \cup B = A \cup (B \setminus A)$.
              We have that $A$ and $B \setminus A$ are disjoint, so:
              \begin{align*}
                  P(A \cup B) & = P(A) + P(B \setminus A)   \\
                              & = P(A) + P(B) - P(A \cap B)
              \end{align*}
              by property 3.
    \end{enumerate}
\end{proof}

We can extend property 4 to the case of $n$ events by induction:
\begin{itemize}
    \item For the case of $n = 3$ we have that
          \begin{align*}
              P(A \cup B \cup C) & = P(A \cup (B \cup C))                                                              \\
                                 & = P(A) + P(B \cup C) - P(A \cap (B \cup C))                                         \\
                                 & = P(A) + P(B) + P(C) - P(B \cap C) - P(A \cap B) - P(A \cap C) + P(A \cap B \cap C)
          \end{align*}
    \item For the general case we have that
          \begin{align*}
              P(\bigcup^n_{i=1} A_i) & = \sum^n_{i=1} P(A_i) - \sum_{1 \leq i < j \leq n} P(A_i \cap A_j) & \\
                                     & + \sum_{1 \leq i < j < k \leq n} P(A_i \cap A_j \cap A_k) - \dots    \\
                                     & + (-1)^{n-1} P(A_1 \cap \dots \cap A_n)
          \end{align*}
\end{itemize}
We will not prove the general case.

\begin{examplebox}{Example}
    $P(\text{winning the game}) = 1\%$, we need to find $P(\text{winning at least once in 3 games})$.

    Let $A_i$ be the event of winning the $i$-th game, then we have that:
    \begin{align*}
        P (A_1 \cup A_2 \cup A_3) & = P(A_1) + P(A_2) + P(A_3)                            \\
                                  & - P(A_1 \cap A_2) - P(A_1 \cap A_3) - P(A_2 \cap A_3) \\
                                  & + P(A_1 \cap A_2 \cap A_3)                            \\
                                  & < 3\%
    \end{align*}
\end{examplebox}

\subsubsection{Probability of monotone events}

We can prove the following properties:
\begin{enumerate}
    \item Let $A_1 \subseteq A_2 \subseteq \dots$ be a sequence of events, then:
          $$
              P\left(\bigcup_{i=1}^\infty A_i\right) = \lim_{n \to \infty} P(A_n)
          $$
    \item Let $A_1 \supseteq A_2 \supseteq \dots$ be a sequence of events, then:
          $$
              P\left(\bigcap_{i=1}^\infty A_i\right) = \lim_{n \to \infty} P(A_n)
          $$
    \item For every sequence $(A_n)$ of events, we have that:
          $$
              P(\bigcup_{i=1}^\infty A_i) \le \sum_{i=1}^\infty P(A_i)
          $$
\end{enumerate}

These properties are easy to see by looking at an Euler-Venn diagram but not as trivial to prove.

\begin{proof}
    We will prove the properties in order:
    \begin{enumerate}
        \item Let $(B_n)$ be a sequence of events defined by induction as $B_1 = A_1$ and $B_n = A_n \setminus A_{n-1}$.
              We note that $B_n$ are disjoint
              and that $\bigcup_{i=1}^\infty A_i = \bigcup_{i=1}^\infty B_i$
              since $A_i = \bigcup_{j=1}^i B_j$.

              We have that
              \begin{align*}
                  P\left(\bigcup_{i=1}^\infty A_i\right) & = P\left(\bigcup_{i=1}^\infty B_i\right)                             \\
                                                         & = \sum_{i=1}^\infty P(B_i) = \lim_{n \to \infty} \sum_{i=1}^n P(B_i) \\
                                                         & = \lim_{n \to \infty} P\left(\bigcup_{i=1}^n B_i\right)              \\
                                                         & = \lim_{n \to \infty} P(A_n)
              \end{align*}
        \item Let $(B_n)$ be the sequence defined as $B_n = A_n^c$.
              This sequence is increasing, hence we can apply property 1 and get that
              \begin{align*}
                  P\left(\bigcup_{i=1}^\infty B_i\right)                         & = \lim_{n \to \infty} P(B_n)              \\
                  \implies P\left(\left(\bigcap_{i=1}^\infty A_i\right)^c\right) & = \lim_{n \to \infty} (1-P(A_n))          \\
                  \implies \cancel{1 -} P\left(\bigcup_{i=1}^\infty A_i\right)   & = \cancel{1 -} \lim_{n \to \infty} P(A_n) \\
              \end{align*}
        \item Let $B_n = \bigcup_{i=1}^n A_i$.
              Indeed we have that
              \[
                  \bigcup_{i=1}^\infty B_n =
                  \bigcup_{i=1}^\infty \left( \bigcup_{i=1}^n A_i \right) =
                  \bigcup_{i=1}^\infty A_i
                  \tag{*}
              \]
              Moreover, we note that $B_n$ is increasing, hence we can apply property 1 to get that
              $$
                  P\left(\bigcup_{i=1}^\infty B_i\right) = \lim_{n \to \infty} P(B_n)
              $$

              We can apply (*) on the right hand side and the definition of $B_n$ on the left hand side to get that
              $$
                  P\left(\bigcup_{i=1}^\infty A_i\right) = \lim_{n \to \infty} P(B_n) \le \sum_{i=1}^\infty P(A_i)
              $$

              TODO: there's something wrong here, I'll fix it when i get the book.
    \end{enumerate}

    \begin{examplebox}{Example}
        We will toss a coin infinitely many times. What's the never getting a head?

        Let $A_n$ be the event of getting a head in the first $n$ tosses.
        We have that $P(A_n) = \left(\frac{1}{2}\right)^n$.

        We notice that $A_n \supseteq A_{n+1}$, hence it's a decreasing sequence.

        $$
            P\left(\bigcap_{i=1}^\infty A_i\right) = \lim_{n \to \infty} P(A_n) = 0
        $$

        by property 2.

    \end{examplebox}
\end{proof}

\subsection{Probability in fair games}

\subsubsection{Finite or countable sample space}

Let $\Omega = \{\omega_1, \omega_2, \dots\}$ and $p_i = P(\{\omega_i\})$.
Then
$$
    P(A) = \sum_{\omega_i \in A} p_i
$$

Let $N = |\Omega|$ be the cardinality of the sample space.
We have that
$$
    p_i = \frac{1}{N} \enspace \forall i \qquad \text{ and } \qquad P(A) = \frac{|A|}{N}
$$

Where $|A|$ is the cardinality of the set $A$.

\subsubsection{Uncountable sample space}

We will use $\Omega \subseteq \R^2$ as an example.

Then, the probability of an event $A$ is given by
$$
    P(A) = \frac{\text{Area of } A}{\text{Area of } \Omega}
$$

\begin{examplebox}{Example: Dart in a circular target or radius 1}
    We want to find the probability of $A = \text{distance from center} \leq 0.5$.

    We have that
    $$
        P(A) = \frac{\pi \cdot 0.5^2}{\pi \cdot 1^2} = \frac{1}{4}
    $$
\end{examplebox}

\section{Class of 09/02/2024 - Conditional probability}

\subsubsection{Notation}


Consider an experiment with sample space $\Omega$, sigma-algebra $\F$, and probability measure $P$.
We denote the \textbf{probability space} as $(\Omega, \F, P)$.

If $P(A) = 0$, then we say that $A$ is a \textbf{null event}.
$\varnothing$ is a null event.

If $P(B) = 1$, then we say that $B$ is a \textbf{almost sure event}.
$\Omega$ is a almost sure event.

\subsection{Definition of conditional probability}

\begin{warningbox}{Examples in this section}
    Since the examples made in class are the same as the ones in the book,
    I will write down only the ones that have some important comment to be made.
\end{warningbox}

\begin{notebox}{Definition}
    Let $A, B \in \F$ be events such that $P(B) > 0$.
    The \textbf{conditional probability} of $A$ given $B$ is defined as
    $$
        P(A|B) = \frac{P(A \cap B)}{P(B)}
    $$
\end{notebox}

The intuition behind this formula is that, since we know that $B$ occurred, we can restrict our attention to the set $B$.
Then, the probability of $A$ is the probability of $A \cap B$ normalized by the probability of $B$ which is kind of the new $\Omega$.

From the definition is immediate that
$$
    P(A \cap B) = P(A|B) \cdot P(B)
$$

\subsection{Bayes' Theorem}

Before stating Bayes' Theorem we need to define the concept of \textbf{partition} and prove the \textbf{total probability} lemma.

\subsubsection{Partition}

\begin{bluebox}{Definition}
    Let $(\Omega, \F, P)$ be a probability space and let $B_1, B_2, \dots \in \F$ be a sequence of events such that
    \begin{enumerate}
        \item $B_i \cap B_j = \varnothing \quad \forall i \ne j$
        \item $\bigcup_{i=1}^\infty B_i = \Omega$
    \end{enumerate}
\end{bluebox}

In words, this means that a partition is a sequence of events such that one and only one of them occurs.

\subsubsection{Total probability}

\begin{bluebox}{Lemma}
    Let $(\Omega, \F, P)$ be a probability space and let $B_1, B_2, \dots \in \F$ be a partition of $\Omega$
    such that $P(B_i) > 0 \enspace \forall i$.
    Then, for every event $A \in \F$ we have that
    $$
        P(A) = \sum_{i=1}^\infty P(A|B_i) \cdot P(B_i)
    $$
\end{bluebox}

\begin{proof}
    We divide $A$ in disjoint events $\{A_1, \dots, A_n\}$ where $A_i = A \cap B_i$.

    Then
    \begin{align*}
        P(A) & = P\left(\bigcup_{i=1}^\infty A_i\right)  \\
             & = \sum_{i=1}^\infty P(A_i)                \\
             & = \sum_{i=1}^\infty P(A \cap B_i)         \\
             & = \sum_{i=1}^\infty P(A|B_i) \cdot P(B_i)
    \end{align*}
\end{proof}

\subsubsection{Bayes' Theorem}

Now we are ready to prove Bayes' Theorem.

\begin{bluebox}{Theorem}
    Let $(\Omega, \F, P)$ be a probability space and let $B_1, B_2, \dots \in \F$ be a partition of $\Omega$
    such that $P(B_i) > 0 \enspace \forall i$.
    Let $A \in \F$ be an event such that $P(A) > 0$.
    Then, for every $i$ we have that
    $$
        P(B_i|A) = \frac{P(A|B_i) \cdot P(B_i)}{\sum_{j=1}^\infty P(A|B_j) \cdot P(B_j)}
    $$
\end{bluebox}

Note that the denominator is just the total probability of $A$.

This theorem is useful when we need to find $P(A|B)$ but we only know $P(B|A)$.

\begin{proof}
    We have that
    \begin{align*}
        P(B_i|A) & = \frac{P(A \cap B_i)}{P(A)}                                            \\
                 & = \frac{P(A|B_i) \cdot P(B_i)}{P(A)}                                    \\
                 & = \frac{P(A|B_i) \cdot P(B_i)}{\sum_{j=1}^\infty P(A|B_j) \cdot P(B_j)}
    \end{align*}
\end{proof}

\subsection{Independent events}

\begin{bluebox}{Definition}
    Let $(\Omega, \F, P)$ be a probability space and let $A, B \in \F$ be events.
    We say that $A$ and $B$ are (stochastically) \textbf{independent} if and only if
    $$
        P(A \cap B) = P(A) \cdot P(B)
    $$
\end{bluebox}

If we know that $P(A) > 0$ and $P(B) > 0$, then we can rewrite the definition as
$$
    P(A|B) = P(A) \quad \text{ and } \quad P(B|A) = P(B)
$$

This means that knowing that $B$ occurred doesn't change the probability of $A$ and vice versa.

Note that if $A$ is independent of $B$, then also $B$ is independent of $A$.

We can consider the following special cases:
\begin{itemize}
    \item If $P(A) = 0$, we have that $P(A \cap B) = 0$ and $P(A) \cdot P(B) = 0$.
          Hence, $A$ is independent of any $B$.
    \item If $P(A) = 1$, we have that $P(A \cap B) = P(B)$ and $P(A) \cdot P(B) = P(B)$.
          Hence, $A$ is independent of any $B$.
\end{itemize}

In general, null events and almost sure events are independent of any event.

\textbf{Notation}: We denote the independence of $A$ and $B$ as $A \indep B$.

\subsubsection{Mutually independent events}

\begin{bluebox}{Definition}
    Let $(\Omega, \F, P)$ be a probability space and let $\{A_i : A_i \in \F, i \in I\}$ be a sequence of events.
    We say that $A_1, A_2, \dots$ are mutually (stochastically) \textbf{independent} if
    $\forall J \in I$ finite we have that

    $$
        P\left(\bigcap_{i \in J} A_i\right) = \prod_{i \in J} P(A_i)
    $$

\end{bluebox}

This definition is a generalization of the definition of independence for two events.

If we have mutually independent events we know that if some of them occur, then the probability of the others won't change.

For example, if we want to check that $A, B, C$ are mutually independent, we need to check that
\begin{align*}
    P(A \cap B)        & = P(A) \cdot P(B)            \\
    P(A \cap C)        & = P(A) \cdot P(C)            \\
    P(B \cap C)        & = P(B) \cdot P(C)            \\
    P(A \cap B \cap C) & = P(A) \cdot P(B) \cdot P(C)
\end{align*}

It is not enough to check just the one with the other, we need to check all the possible intersections.


\subsection{Conditional independence}

Consider $P(A|B)$. If we fix $B$ we can define a new a function
$$
    P(\cdot|B) : A \in \F \mapsto P(A|B)
$$

We can prove that $P(\cdot|B)$ is a probability measure.

\begin{bluebox}{Definition}
    Let $(\Omega, \F, P)$ be a probability space and let $A, B, C \in \F$ be events.
    We say that $A$ and $C$ are (stochastically) \textbf{conditionally independent} given $B$ if and only if
    $$
        P(A \cap C|B) = P(A|B) \cdot P(C|B)
    $$
\end{bluebox}

\section{Class of 12/02/2024 - Random variables}

Random variables are functions $X: \Omega \to \R$. They are used to describe the outcome of an experiment in a numerical way.

A function to be a random variable needs to be measurable, this means that $(X \leq a) = \{\omega \in \Omega: X(\omega) \leq a\} \in \F \enspace \forall a \in \R$.

\subsubsection{Proprieties of random variables}

We can prove the following properties are equivalent:
\begin{enumerate}
    \item $(X \leq a) \in \F \enspace \forall a \in \R$
    \item $(X < a) \in \F \enspace \forall a \in \R$
    \item $(X > a) \in \F \enspace \forall a \in \R$
    \item $(X \geq a) \in \F \enspace \forall a \in \R$
\end{enumerate}

\begin{proof}
    We will prove just some of these proprieties, the proof for the others is very similar.
    \begin{description}
        \item[$1 \implies 2$] We need to write $(X < a)$ as a union of $(X \leq b)$.
            We have that
            $$
                (X < a) = \bigcup_{n=1}^\infty (X \leq a - \frac{1}{n})
            $$
        \item[$2 \implies 1$] We need to write $(X \leq a)$ as a intersection of $(X < b)$.
            We have that
            $$
                (X \leq a) = \bigcap_{n=1}^\infty (X < a + \frac{1}{n})
            $$
        \item[$1 \implies 3$] We need to write $(X > a)$ as a complement of $(X \leq b)$.
            We have that
            $$
                (X > a) = (X \leq a)^c
            $$
    \end{description}
\end{proof}

Moreover it is possible eto prove that $\forall B \in \mathcal{B}(\R) \enspace (x \in B) \in \F$

\subsection{Indicator function}

Let $A \in \F$, then the \textbf{indicator function} of $A$ is defined as

$$
    I_A(\omega) =
    \begin{cases}
        1 & \text{if } \omega \in A    \\
        0 & \text{if } \omega \notin A
    \end{cases}
$$

\subsubsection{Proprieties of the indicator function}

\begin{itemize}
    \item $I_{A \cap B} = I_A \cdot I_B$
    \item If $A \cap B = \varnothing$ then $I_{A \cup B} = I_A + I_B$
    \item Let ${B_1, \ldots, B_n}$ be a partition of $\Omega$, then $I_A = \sum_{i = 1}^n I_{A \cap B}$
\end{itemize}

\subsection{Cumulative probability function}

or cumulative distribution function (CDF).

\begin{bluebox}{Definition}
    Let $X$ be a random variable, then the \textbf{cumulative probability function} of $X$ is defined as the function $F_X: \R \to [0, 1]$ such that
    $$
        F_X(x) = P(X \leq x) \enspace \forall x \in \R
    $$
\end{bluebox}

\subsubsection{Proprieties of the cumulative probability function}

We can prove the following properties:
\begin{enumerate}
    \item $F_X$ is increasing
    \item $\lim_{x \to -\infty} F_X(x) = 0$ and $\lim_{x \to +\infty} F_X(x) = 1$
    \item $F_X$ is right-continuous, hence $\lim_{h \to 0^+} F_X(x + h) = F_X(x)$
\end{enumerate}

Any function that satisfies these properties is a cumulative probability function of some random variable $X$.

\section{Class of 14/02/2024}

\subsection{Proprieties of cumulative probability functions}

We can prove the following properties:
\begin{enumerate}
    \item $P(X > x) = 1 - F_X(x)$
    \item $P(x < X \leq y) = F_X(y) - F_X(x)$
    \item $P(X < x) = F_X(x^-)$ (left-limit of $F_X$)
    \item $P(X = x) = F_X(x) - F_X(x^-)$
\end{enumerate}

\begin{proof}
    We have
    \begin{enumerate}
        \item $(X > x) = (X \leq x)^c$, hence
              $$
                  P(X > x) = 1 - P(X \leq x) = 1 - F_X(x)
              $$
        \item $(x < X \leq y) = (X \leq y) \setminus (X \leq x)$, hence
              $$
                  P(x < X \leq y) = P(X \leq y) - P(X \leq x) = F_X(y) - F_X(x)
              $$
        \item We use the proprieties of monotone events to get that
              \begin{align*}
                  (X < x) & = \left(\bigcup^\infty_{n = 1} \left(X \leq x - \frac{1}{n}\right)\right) \\
                          & = \lim_{n \to \infty} P\left(X \leq x - \frac{1}{n}\right)                \\
                          & = F(x^-)
              \end{align*}
        \item
              \begin{align*}
                  P(X = x) & = P\left((X \leq x) \setminus (X < x)\right) \\
                           & = P(X \leq x) - P(X < x)                     \\
                           & = F_X(x) - F_X(x^-)
              \end{align*}
    \end{enumerate}
\end{proof}

\subsection{Discrete random variables}

\begin{bluebox}{Definition}
    Let $X$ be a random variable, then $X$ is \textbf{discrete} if there exists a countable set $S \subseteq \R$ such that
    $$
        P(X = x_i) > 0 \enspace \forall x_i \in S
    $$
\end{bluebox}

$S$ is called the \textbf{support} of $X$.

\subsubsection{Probability mass function (PMF)}

\begin{bluebox}{Definition}
    Let $X$ be a discrete random variable, then the \textbf{probability mass function (PMF)} of $X$ is defined as
    $$
        f_X(x) = P(X = x) \enspace \forall x \in \R
    $$
\end{bluebox}

PMFs have the following proprieties:
\begin{enumerate}
    \item $f(x) = 0 \enspace \forall x \notin S$
    \item $f(x) \ge 0 \enspace \forall x \in S$
    \item $\sum_{x \in S} f(x) = 1$
\end{enumerate}

Note that if we have a PMF, then we can find

$$
    F(X \in B) = \sum_{x \in B} f(x)
$$

From the proprieties of PMFs we can easily see that

\begin{enumerate}
    \item $F(x) = \sum_{x_i \leq x} f(x_i)$
    \item $f(x) = F(x) - F(x^-)$
\end{enumerate}

\subsection{Distributions}

\begin{bluebox}{Definition}
    Let $f$ be a PMF and $S$ the support of $f$, then the \textbf{distribution} of $f$ is the pair $(S, f)$.
\end{bluebox}

We will see some of the most common distributions.

\subsubsection{Bernoulli distribution}

Let $0 < p < 1$ and $q = 1 - p$.
The PMF of a Bernoulli distribution is

$$
    f(x) =
    \begin{cases}
        p & \text{if } x = 1 \\
        q & \text{if } x = 0 \\
        0 & \text{otherwise}
    \end{cases}
$$

with $x \begin{cases}
        0 & p \\
        1 & q
    \end{cases}$.

An example of a Bernoulli distribution is the indicator function of an event.
We say $I_A \sim \text{Bernoulli}(p)$ which reads as \say{is distributed as}.

\subsubsection{Binomial distribution}

Remember that the binomial coefficient is defined as

$$
    \binom{n}{k} = \frac{n!}{k! \cdot (n - k)!} = \frac{n \cdot (n - 1) \cdot \ldots \cdot (n - k + 1)}{k!} = \binom{n}{n - k}
$$

Now, consider an experiment and partition $\Omega$ in \say{success} and \say{failure}.
We repeat the experiment $n$ times and let $X$ be the number of successes.

Note that the experiment are independent, hence $P$ of the success is always the same.

\begin{align*}
    X        & = \text{ \# of successes}                      \\
    S        & = \{0, 1, \ldots, n\}                          \\
    P(X = x) & = \binom{n}{x} \cdot p^x \cdot (1 - p)^{n - x}
\end{align*}

We use the binomial coefficient to count the number of ways we can arrange the successes and the failures in $n$ attempts.

\end{document}