\documentclass[12pt]{extarticle}

\usepackage{preamble}

\title{Probability Notes, Partial 2}
\date{Semester 2, 2023/2024}

\setlength{\headheight}{15pt} % ??? we do what fancyhdr tells us to do

\begin{document}

\maketitle
\tableofcontents
\clearpage

% Class of 20/03/2024
\section{Continuous random variables}

In the last partial we studied discrete random variables, which are random variables that can take a finite or countably infinite number of values.
Now we will consider random variables that take values in an interval of the real line, which are called \emph{continuous random variables}.

\begin{definition}[continuous random variable]
    \label{def:continuous_random_variable}

    A random variable $X$ is said to be \emph{continuous} if $\exists f(x) : \forall a, b \in \R, a < b$,

    $$
        P(a \leq X \leq b) = \int_a^b f(x) \dd{x}
    $$

    where $f(x)$ is called the \emph{probability density function} (pdf) of $X$.
\end{definition}

Note that $f$ is not unique, and in fact, any function $f$ that satisfies the above equation is a valid pdf for $X$. Usually we will choose the simplest function that satisfies the equation, for example, a continuous one.

Moreover, we also note that if $f(x)$ is negative in an interval, then it is possible to have $P(a \leq X \leq b) < 0$, which is not possible.
Therefore we will consider only non-negative functions as pdfs.

\begin{proposition}[properties of the pdf]
    \label{prop:properties_of_pdf}
    Let $X$ be a continuous random variable with pdf $f(x)$, then

    \begin{enumerate}[label=\roman*)]
        \item $\forall x \in \R, P(X = x) = 0$
        \item $F(x) = \int_{-\infty}^x f(u) \dd{u}$ \item $\int_{-\infty}^{+\infty} f(x) \dd{x} = 1$
        \item if $f$ is continuous at $x$, then $F'(x) = f(x)$
    \end{enumerate}
\end{proposition}

\begin{proof}
    \skiplineafterproof
    \begin{enumerate}[label=\roman*)]
        \item
              \begin{align*}
                  P(X = x) & = P(\bigcap_{n=1}^\infty (x \leq X \leq x + \frac{1}{n}))      \\
                           & = \lim_{n \to \infty} P(x \leq X \leq x + \frac{1}{n})         \\
                           & = \lim_{n \to \infty} \int_x^{x + \frac{1}{n}} f(u) \dd{u} = 0
              \end{align*}

        \item
              \begin{align*}
                  F(x) & = P(X \leq x)                                  \\
                       & = P (\bigcup_{n=1}^\infty (x-n \leq X \leq x)) \\
                       & = \lim_{n \to \infty} P(x-n \leq X \leq x)     \\
                       & = \lim_{n \to \infty} \int_{x-n}^x f(u) \dd{u} \\
                       & = \int_{-\infty}^x f(u) \dd{u}
              \end{align*}

        \item
              \begin{align*}
                  \int_{-\infty}^{+\infty} f(x) \dd{x} & = \int_{-\infty}^x f(x) \dd{x}  \\
                                                       & = \lim_{x \to -\infty} F(x) = 1
              \end{align*}
        \item This is a direct consequence of ii) and the fundamental theorem of calculus.
    \end{enumerate}
\end{proof}

\begin{remark}
    The pdf $f(x)$ can be interpreted as the \emph{density} of the random variable $X$ at the point $x$, that is

    $$
        P(x \leq X \leq x + h) \sim f(x) h \quad \text{as } h \to 0
    $$
\end{remark}

\begin{lemma}[indipendent random variables]
    \label{lem:independent_random_variables}
    Let $X$ and $Y$ be continuous random variables such that $X \indep Y$, and let $B_1, B_2$ be any borel sets, then

    $$
        P\left((X \in B_1) \cap (Y \in B_2)\right) = P(X \in B_1) P(Y \in B_2)
    $$

    This is equivalent to saying

    $$
        P\left((X \in x) \cap (Y \in y)\right) = P(X \in x) P(Y \in y)
    $$
\end{lemma}

\begin{proof}
    We will not prove this as it is quite hard and not very useful.
\end{proof}

\begin{example}
    Our experiment is extracting random numbers from the interval $[0, 1]$.
    Let $X, Y$ such that $X \indep Y$.

    We want to calculate $P\left((0.5 \leq X \leq 0.75) \cap (0.5 \leq Y \leq 0.75)\right)$.

    Since they are independent we can use the lemma to get $P(0.5 \leq X \leq 0.75) P(0.5 \leq Y \leq 0.75)$.

    Note, by the nature of the experiment, that $P(a \leq X \leq b) = b - a$, then we get
    $P(0.5 \leq X \leq 0.75) P(0.5 \leq Y \leq 0.75) = 0.25 \cdot 0.25 = 0.0625$.

    Moreover we note that

    $$
        P(a \leq X \leq b) = \int_a^b I_{[0,1]}(x) \dd{x} = b - a
    $$

    where $I_{[0,1]}(x)$ is the indicator function of the interval $[0, 1]$.
\end{example}

\subsection{Transformation of random variables}

\begin{proposition}[transformation of independent random variables]
    \label{prop:transformation_indep_crv}

    If $X \indep Y$, then $g(X) \indep h(Y)$.
\end{proposition}

\begin{proof}
    \skiplineafterproof
    \begin{align*}
        P\left(
        (g(X) \in B_1) \cap (h(Y) \in B_2)
        \right) & = P\left(
        (X \in g^{-1}(B_1)) \cap (Y \in h^{-1}(B_2))
        \right)                                               \\
                & = P(X \in g^{-1}(B_1)) P(Y \in h^{-1}(B_2)) \\
                & = P(g(X) \in B_1) P(h(Y) \in B_2)
    \end{align*}

    where $B_1, B_2$ are borel sets and $g^{-1}(B_1), h^{-1}(B_2)$ are the preimages of $B_1, B_2$ under $g, h$ respectively.
\end{proof}

\begin{remark}
    Let $X$ be a continuous random variable with pdf $f_X(x)$ and let $Y = g(X)$,
    then $Y$ could be either continuous, discrete, or neither.
\end{remark}

\begin{theorem}[transformation of random variables]
    \label{thm:transformation_crv}

    Let $X$ be a continuous random variable, $f_X(x)$ its pdf, such that $X$ takes values in an open interval $U$.
    Let $g: U \to V$ be a one-to-one, strictly increasing or strictly decreasing function continuously differentiable in $U$.

    Then $Y = g(X)$ is a continuous random variable with pdf $f_Y(y)$ given by

    $$
        f_Y(y) = f_X(g^{-1}(y)) \abs{\dv{g^{-1}(y)}{y}}
    $$
\end{theorem}

\begin{proof}
    Let $a, b \in V$, such that $a < b$.
    Then
    \begin{align*}
        P(a \leq Y \leq b) & = P(a \leq g(X) \leq b)                                  \\
                           & = P(g^{-1}(a) \leq X \leq g^{-1}(b))                     \\
                           & = \int_{g^{-1}(a)}^{g^{-1}(b)} f_X(x) \dd{x}             \\
                           & = \int_a^b f_X(g^{-1}(y)) \abs{\dv{g^{-1}(y)}{y}} \dd{y}
    \end{align*}

    where we used the change of variables $x = g^{-1}(y)$.
    Note that the inverse of $g$ exists because $g$ is one-to-one.

    We need the absolute value because if $g$ is decreasing, then we get a minus sign from the change of variables that we don't want.
\end{proof}

\begin{example}
    Let $X$ a random variable representing the radius of a circle, and let $Y$ be the area of such circle.
    Let $f_X(x) = 6x(1-x)I_{(0,1)}(x)$, then we want to find $f_Y(y)$.

    We have that $Y = \pi X^2$, that is $g(x) = \pi x^2$ and its inverse is $g^{-1}(y) = \sqrt{\frac{y}{\pi}}$.

    Moreover $\dv{g^{-1}(y)}{y} = \frac{1}{2} \frac{1}{\sqrt{\pi y}}$.

    We use the theorem \ref{thm:transformation_crv} to get

    \begin{align*}
        f_Y & = 6\sqrt{\frac{y}{\pi}} \left(1- \sqrt{\frac{y}{\pi}}\right) \frac{1}{2} \frac{1}{\sqrt{\pi y}} I_{(0, 1)}\left(\frac{y}{\pi}\right) \\
            & = \frac{3}{\pi} \left(1- \frac{y}{\pi}\right) I_{(0, \pi)}(y)
    \end{align*}

\end{example}

% Class of 22/03/2024
\subsection{Expectation and variance}

\begin{definition}[expectation]
    Let $X$ be a continuous random variable with pdf $f(x)$, then the \emph{expectation} of $X$ exists if one at least of the following integrals is finite

    $$
        E(X) = \int_{-\infty}^{0} x f(x) \dd{x} + \int_{0}^{+\infty} x f(x) \dd{x}
    $$

    If $E(X)$ exists and is finite we say that $X$ is \emph{integrable}.
\end{definition}

\begin{lemma}[condition for integrability]
    $X$ is integrable iff

    $$
        \int_{-\infty}^{+\infty} \abs{x} f(x) \dd{x} < +\infty
    $$
\end{lemma}

\begin{example}
    Let $X$ have a pdf $f(x) = \frac{1}{\pi (1+x^2)}$.
    We want to find $E(X)$.

    We have that
    $$
        \int_0^{+\infty} x \frac{1}{\pi (1+x^2)} \dd{x} \quad \text{and} \quad \int_{-\infty}^0 x \frac{1}{\pi (1+x^2)} \dd{x}
    $$

    Note that as $x \to +\infty$, $x \frac{1}{\pi (1+x^2)} \sim \frac{1}{\pi x}$, which diverges to $+\infty$.
    For $x \to -\infty$ we use the same argument to get that the integral diverges to $-\infty$.

    Therefore $E(X)$ does not exist.
\end{example}

\begin{theorem}
    Let $X$ be a continuous random variable with pdf $f(x)$ continuous defined on a support $[a, b]$, with $a < 0, b > 0$, and $f(x) = 0$ outside $[a, b]$.

    Then

    $$
        E(X) = -\int_{a}^0 F(x) \dd{x} + \int_0^b (1 - F(x)) \dd{x}
    $$
    with $F'(x) = f(x)$.
\end{theorem}

\begin{proof}
    We have
    \begin{align*}
        E(X) & = \int_{-\infty}^0 x f(x) \dd{x} + \int_0^{+\infty} x f(x) \dd{x}           \\
             & = \int_{a}^0 x f(x) \dd{x} + \int_0^{b} x f(x) \dd{x}                       \\
             & = [x F(x)]_a^0 - \int_a^0 F(x) \dd{x} + [x F(x)]_0^b - \int_0^b F(x) \dd{x} \\
             & = -\int_a^0 F(x) \dd{x} + b \int_0^b F(x) \dd{x}                            \\
             & = -\int_a^0 F(x) \dd{x} + \int_0^b (1-F(x)) \dd{x}
    \end{align*}
    by applying integration by parts.
\end{proof}

\begin{remark}
    This is true even for $a = -\infty$ and $b = +\infty$.
\end{remark}

\begin{proposition}[transformation of expectation]
    Let $X$ be a continuous random variable with pdf $f(x)$ and $Y = g(X)$, then

    $$
        E(Y) = \int_{-\infty}^{+\infty} g(x) f(x) \dd{x}
    $$

    Moreover $Y$ is integrable iff

    $$
        \int_{-\infty}^{+\infty} \abs{g(x)} f(x) \dd{x} < +\infty
    $$
\end{proposition}

\begin{example}
    Let $X$ be a random variable with pdf $f(x) = 6x(1-x) I_{(0, 1)}(x)$, and $Y = X^2$.
    Then

    \begin{align*}
        E(Y) & = \int_{-\infty}^{+\infty} x^2 f(x) \dd{x} \\
             & = \int_0^1 x^2 6x(1-x) \dd{x}              \\
             & = 6 \int_0^1 (x^3 - x^4) \dd{x}            \\
             & = \frac{6}{20}
    \end{align*}
\end{example}

\begin{remark}
    In general we cannot say that $E(g(X)) = g(E(X))$.
\end{remark}

\begin{theorem}[linearity of expectation]
    Let $X$ be a random variable, $a, b \in \R$, then

    $$
        E(aX + b) = aE(X) + b
    $$
\end{theorem}

\begin{proof}
    \skiplineafterproof
    \begin{align*}
        E(aX + b) & = \int_{-\infty}^{+\infty} (ax + b) f(x) \dd{x}                                     \\
                  & = a \int_{-\infty}^{+\infty} x f(x) \dd{x} + b \int_{-\infty}^{+\infty} f(x) \dd{x} \\
                  & = aE(X) + b
    \end{align*}
\end{proof}

\begin{definition}[moment]
    Let $X$ be an integrable random variable, then we can define the following

    \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{moment}       & \textbf{symbol} & \textbf{definition} \\
            \hline
            first moment          & $\mu$           & $E(X)$              \\
            $k$-th moment         &                 & $E(X^k)$            \\
            $k$-th central moment &                 & $E((X - E(X))^k)$   \\
            variance              & $\sigma^2$      & $E((X - E(X))^2)$   \\
            \hline
        \end{tabular}

        \caption{Moments of a random variable}
        \label{tab:moments}
    \end{table}

    The even $k$-th moment have the propriety that $\int_{-\infty}^0 x^k f(x) \dd{x} = 0$.
\end{definition}

\begin{theorem}
    Let $X$ be a random variable with finite $k$-th moment.
    Then $\forall j < k$, the $j$-th moment exists and is finite.

    Moreover the $k$-th central moment exists and is finite.
\end{theorem}

\begin{proof}
    We know that $E(X^k)$ exists iff $E(\abs{X}^k)$ exists, we want to prove that $E(\abs{X}^j)$ exists $\forall j < k$.

    $$
        E(\abs{X}^j) = \int_{-\infty}^{+\infty} \abs{x}^j f(x) \dd{x}
    $$

    We would like to say that $\abs{x}^j \leq \abs{x}^k$, but this is not always true, for instance, this is true only if $\abs{x} \geq 1$.

    Then we can write $E(\abs{X}^j)$ as
    \begin{align*}
        E(\abs{X}^j) & \leq \int_{-\infty}^{+\infty} \max(\abs{x}^j, 1) f(x) \dd{x}                                      \\
                     & \leq \int_{-\infty}^{+\infty} \max(\abs{x}^k, 1) f(x) \dd{x}                                      \\
                     & \leq \int_{-\infty}^{+\infty} (\abs{x}^k + 1) f(x) \dd{x}                                         \\
                     & = \int_{-\infty}^{+\infty} \abs{x}^k f(x) \dd{x} + \int_{-\infty}^{+\infty} f(x) \dd{x} < +\infty
    \end{align*}

    For the second part we can write

    \begin{align*}
        E(\abs{X - \mu}^k) & = \int_{-\infty}^{+\infty} \abs{x - \mu} f(x) \dd{x}                                                 \\
                           & \leq \int_{-\infty}^{+\infty} (\abs{x} + \abs{\mu}) f(x) \dd{x}                                      \\
                           & = \int_{-\infty}^{+\infty} \sum_{j=0}^k \binom{k}{j} \abs{x}^j \abs{\mu}^{k-j} f(x) \dd{x}           \\
                           & = \sum_{j=0}^k \binom{k}{j} \abs{\mu}^{k-j} \int_{-\infty}^{+\infty} \abs{x}^j f(x) \dd{x} < +\infty
    \end{align*}
\end{proof}

\begin{definition}[variance]
    Let $X$ be a random variable, then the \emph{variance} of $X$ is defined as

    \begin{align*}
        V(X) & = \sigma^2 = E((X - \mu)^2)                        \\
             & = \int_{-\infty}^{+\infty} (x - \mu)^2 f(x) \dd{x}
    \end{align*}
\end{definition}

\begin{theorem}[properties of variance]
    \begin{enumerate}
        \item $V(X) = E(X^2) - E(X)^2$
        \item $V(X + b) = V(X)$
        \item $V(aX) = a^2 V(X)$
    \end{enumerate}
\end{theorem}

\begin{proof}
    \skiplineafterproof
    \begin{enumerate}
        \item
              \begin{align*}
                  V(X) = \int_{-\infty}^{+\infty} (x - \mu)^2 f(x) \dd{x} & = \int_{-\infty}^{+\infty} (x^2 - 2x\mu + \mu^2) f(x) \dd{x}                                                                          \\
                                                                          & = \int_{-\infty}^{+\infty} x^2 f(x) \dd{x} - 2\mu \int_{-\infty}^{+\infty} x f(x) \dd{x} + \mu^2 \int_{-\infty}^{+\infty} f(x) \dd{x} \\
                                                                          & = E(X^2) - 2\mu E(X) + \mu^2 = E(X^2) - E(X)^2
              \end{align*}

        \item \begin{align*}
                  V(X + b) & = E((X + b)^2) - E(X + b)^2                     \\
                           & = E(X^2 + 2bX + b^2) - (E(X) + b)^2             \\
                           & = E(X^2) + 2bE(X) + b^2 - E(X)^2 - 2bE(X) - b^2 \\
                           & = V(X)
              \end{align*}

        \item \begin{align*}
                  V(aX) & = E((aX)^2) - E(aX)^2     \\
                        & = E(a^2 X^2) - (aE(X))^2  \\
                        & = a^2 E(X^2) - a^2 E(X)^2 \\
                        & = a^2 (E(X^2) - E(X)^2)   \\
                        & = a^2 V(X)
              \end{align*}
    \end{enumerate}
\end{proof}

% Class of 25/03/2024

\subsection{Noteworthy continuous distributions}

\subsubsection{Uniform continuous distribution}

Let $a, b \in \R$ with $a < b$. $X$ is said to have a \emph{uniform continuous distribution} in the interval $[a, b]$ if its pdf is constant in the interval.

\begin{itemize}
    \item PDF:
          $$
              f(x) = \frac{1}{b-a} I_{[a, b]}(x)
          $$
    \item CDF:
          $$
              F(x) = \int_{-\infty}^x f(u) \dd{u} = \begin{cases}
                  0               & x < a        \\
                  \frac{x-a}{b-a} & a \leq x < b \\
                  1               & x \geq b
              \end{cases}
          $$

    \item Expectation:
          $$
              E(X) = \int_{-\infty}^{+\infty} x f(x) \dd{x} = \int_a^b \frac{x}{b-a} \dd{x} = \frac{1}{b-a} \left[\frac{x^2}{2}\right]_a^b = \frac{a+b}{2}
          $$
    \item Variance:
          $$
              V(X) = E(X^2) - E(X)^2 = \frac{b^2 + ab + a^2}{3} - \frac{(a+b)^2}{4} = \frac{(b-a)^2}{12}
          $$
\end{itemize}

\subsubsection{Exponential distribution}

We have one parameter $\lambda > 0$.

\begin{itemize}
    \item PDF:
          $$
              f(x) = \lambda e^{-\lambda x} I_{[0, +\infty)}(x)
          $$
    \item CDF:
          $$
              F(x) = \int_{-\infty}^x f(u) \dd{u} = (1 - e^{-\lambda x})I_{[0, +\infty)}(x)
          $$

          We know that $F(x) = P(X \leq x)$, then it's easy to remember that $P(X > x) = e^{-\lambda x}$.
    \item Expectation:
          $$
              E(X) = \int_{-\infty}^{+\infty} (1 - F(x)) \dd{x} = \int_0^{+\infty} \lambda e^{-\lambda x} \dd{x} = \frac{1}{\lambda}
          $$
    \item Variance:
          $$
              V(X) = E(X^2) - E(X)^2 = \frac{1}{\lambda^2}
          $$

          $E(X)^2$ can be computed by integrating by parts.
\end{itemize}

The exponential distribution is related to some discrete distributions:
\begin{itemize}
    \item Similarly to the geometric distribution, the exponential distribution is \emph{memoryless}, that is

          $$
              P(X > x + z | X > z) = P(X > x)
          $$

          We can also prove that the exponential distribution is the continuous counterpart of the geometric distribution.

    \item
          The exponential distribution is also the distribution of the time between two events in a Poisson distribution.

          \begin{proof}
              We can prove this by considering $X$ the time of the next arrival and $N_t$ the number of arrivals in an interval of length $t$.
              Then $X \sim \text{Exponential}(\lambda)$ and $N_t \sim \text{Poisson}(t \lambda)$.
              Moreover, $P(X > t) = P(N_t = 0)$, that is, the probability of the next visit being after $t$ is the same as the probability of having no visits in the interval $t$.
              Then $P(X > t) = e^{-\lambda t}$.
          \end{proof}
\end{itemize}

\begin{example}
    We have a rate of 20 people per hour.

    Let $N_t$ be the number of people entering in a certain time interval $t$, then $N_t \sim Poisson(t \lambda)$.
    We want to find the probability that the time between two arrivals is less than 3 minutes.

    Note that $N_{60} \sim \text{Poisson}(20)$, that is, $\lambda = \frac{1}{3}$.

    Then the time between two arrivals $X \sim \text{Exponential}\left(\frac{1}{3}\right)$, and $P(X < 3) = 1 - e^{-\lambda \cdot 3} = 1 - e^{-1}$.
\end{example}

\subsubsection{Normal or Gaussian distribution}

We have two parameters $\mu \in \R$ and $\sigma > 0$.

\begin{itemize}
    \item PDF:
          $$
              f(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
          $$
    \item CDF:
          $$
              F(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(u-\mu)^2}{2\sigma^2}} \dd{u}
          $$
          It is possible to prove that this is not a primitive function, hence it is not possible to express it in terms of elementary functions, but it is possible to compute that $\lim_{x \to +\infty} F(x) = 1$, but we will cover this in Analysis.
    \item Expectation:
          \begin{align*}
              E(X) & = \int_{-\infty}^{+\infty} x \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \dd{x}                                                                                         \\
                   & = \int_{-\infty}^{+\infty} (z + x) \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{z^2}{2\sigma^2}} \dd{z}                                                                                         \\
                   & = \int_{-\infty}^{+\infty} z \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{z^2}{2\sigma^2}} \dd{z} + \int_{-\infty}^{+\infty} \mu \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{z^2}{2\sigma^2}} \dd{z} \\
                   & = 0 + \mu = \mu
          \end{align*}
          where $z = x - \mu$.

          The last simplification is due to the fact that the first integral contains an odd function, and the second integral is the pdf of a normal distribution hence it is equal to 1.

    \item Variance:
          \begin{align*}
              V(X) & = \int_{-\infty}^{+\infty} (x - \mu)^2 \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \dd{x}                                                                 \\
                   & = \int_{-\infty}^{+\infty} z^2 \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{z^2}{2\sigma^2}} \dd{z}                                                                               \\
                   & = \frac{1}{\sqrt{2\pi} \sigma}(-\sigma^2) \int_{-\infty}^{+\infty} z \left(- \frac{z}{\sigma^2}\right) e^{-\frac{z^2}{2\sigma^2}} \dd{z}                                    \\
                   & = -\frac{\sigma}{\sqrt{2\pi}} \left\{ \left[ z e^{-\frac{z^2}{2\sigma^2}} \right]_{-\infty}^{+\infty} - \int_{-\infty}^{+\infty} e^{-\frac{z^2}{2\sigma^2}} \dd{z} \right\} \\
                   & = \frac{\sigma}{\sqrt{2\pi}} \int_{-\infty}^{+\infty} e^{-\frac{z^2}{2\sigma^2}} \dd{z}                                                                                     \\
                   & = \sigma^2 \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{z^2}{2\sigma^2}} \dd{z}                                                                          \\
                   & = \sigma^2
          \end{align*}
\end{itemize}

\end{document}