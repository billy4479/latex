\documentclass[10pt]{extarticle}
\title{Linear Algebra Notes}
\author{Giacomo Ellero}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{dirtytalk}
\usepackage{parskip}
\usepackage{mathrsfs}
\usepackage[many]{tcolorbox}
\usepackage{xparse}
\usepackage[a4paper,margin=1.5cm]{geometry}
\usepackage{bookmark}

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbf{F}}
\newcommand{\rk}{\operatorfont{rk} }
\newcommand{\citeladr}[1]{
    \begin{quotation}
        This section references \say{Linear Algebra Done Right}, #1.
    \end{quotation}
}
\newcommand{\citebarberian}[1]{
    \begin{quotation}
        This section references \say{Barberian}, #1.
    \end{quotation}
}
\newcommand{\innerprod}[1]{
    \langle #1 \rangle
}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\newenvironment{absolutelynopagebreak}
  {\par\nobreak\vfil\penalty0\vfilneg
   \vtop\bgroup}
  {\par\xdef\tpd{\the\prevdepth}\egroup
   \prevdepth=\tpd}

\newtcolorbox{statementbox}[1]{colback=green!5!white,colframe=green!40!black,title={#1},fonttitle=\bfseries,parbox=false}

\NewDocumentEnvironment{statement}{ O{} O{Theorem} m}{
\begin{absolutelynopagebreak}
\subsubsection{#3}
\label{#3}
{#1}
\begin{statementbox}{#2}
}
{
\end{statementbox}
\end{absolutelynopagebreak}
}


\begin{document}

\maketitle
\tableofcontents
\clearpage

\section{Structure of Vector Spaces}

\citebarberian{Chapter 3}

\subsection{Basis and Dimension}
\citebarberian{Chapter 3.5}

\begin{statement}[\citebarberian{3.5.5, (3.3.2 and 3.4.2 for the proof)}]{Bijectivity of mappings from basis of $V$ to $V$}

    Let $T: F^n \to V$ defined as $T(a_1, \dots, a_n) = a_1 x_1 + \dots + a_n x_n$.

    $$
        T \text{ is bijective } \iff  x_1, ..., x_n \text{ is a basis of } V
    $$


\end{statement}
\begin{proof}
    $T$ (defined above) takes the linear combination of $x_1, \dots, x_n$.

    If $x_1, \dots, x_n$ is a basis it's generating and linearly independent.

    By the definition of generating we know that if $x_1, \dots, x_n$ are generating
    every vector in $V$ can be written as a linear combination of $x_1, \dots, x_n$.
    This means that $T(V) = V$, hence $T$ is surjective.

    By the definition of linear independence we have that if $x_1, \dots, x_n$ are linearly independent
    $c_1 x_1 + \dots + c_n x_n = \underline 0 \iff c_1 = \dots = c_n = 0$.
    This means that $\ker T = \{ 0 \}$, hence $T$ is injective.
\end{proof}


\clearpage

\subsection{Dimension of $\mathscr L(V, W)$}

\citebarberian{Chapter 3.8}

\begin{statement}[\citebarberian{3.8.1}]{Converting basis of $V$ to elements of $W$}
    Let $x_1, ..., x_n$ be a basis of $V$, then $\exists ! T:V \to W$ such that $Tx_i = w_i$ for $i = 1, ..., n$.
\end{statement}
\begin{proof}
    We first prove existence then uniqueness.

    \textit{Existence}: Define $R: \F^n \to V$ and $S: \F^n \to W$ as follows

    \begin{align*}
        R(a_1, \dots, a_n) & = a_1 x_1 + \dots + a_n x_n \\
        S(a_1, \dots, a_n) & = a_1 w_1 + \dots + a_n w_n
    \end{align*}

    In this way if we use the canonical basis of $\F$ $e_1, \dots, e_n$
    we get that $Re_i = x_i$ and $Se_i = w_i$ for all $i = 1, \dots, n$.

    $R$ is bijective (\ref{Bijectivity of mappings from basis of $V$ to $V$}), hence invertible.
    We get that $e_i = R^{-1}x_i$ and $w_i = SR^{-1}x_i$,
    hence the linear mapping $T = SR^{-1}$ satisfies our requirements.

    \textit{Uniqueness}: Suppose $T_1: V \to W$ and $T_2: V \to W$ both satisfy our requirements.

    We would have that $T_1 x_i = w_i = T_2 x_i \implies (T_1 -T_2)(x_i) = \underline 0$.
    But this means that $\ker (T_1 - T_2)$ contains all the elements of $V$ since $x_1, \dots, x_n$ is a basis,
    hence $T_1 - T_2 = 0$ and $T_1 = T_2$.

\end{proof}

\begin{statement}[\citebarberian{3.8.2}]{Dimension of space of linear functions}
    If $V$ and $W$ are finite-dimensional then $\mathscr L(V, W)$ is also finite-dimensional.
    Moreover

    $$
        \dim \mathscr L(V, W) = (\dim V) (\dim W)
    $$

\end{statement}

\begin{proof}
    Let $n = \dim V$ and $m = \dim W$.
    We know that $W^n = W \times \dots \times W$ has dimension $n \cdot m$.
    $W^n$ is the space of matrixes of the operators from $V$ to $W$.

    Let $\Phi: \mathscr L (V, W) \to W^n$ be the mapping that takes each mapping
    to its matrix relative to some basis $x_1, \dots, x_n$ of $V$ defined as such

    $$
        \Phi(T) = (Tx_1, \dots, Tx_n)
    $$

    By \ref{Converting basis of $V$ to elements of $W$} we have that
    $\Phi$ is an bijective, hence an isomorphism, hence $\dim W^n = \dim \mathscr L(V, W)$.

\end{proof}

\clearpage

\subsection{Duality of Vector Spaces}

\citebarberian{Chapter 3.9}

\begin{statement}[\citebarberian{3.9.1}][Definition]{Definition of Dual Space}
    Let $\mathscr L(V, F)$ be the set of all \textbf{linear forms} on $V$ (that is the set of all linear mappings $T: V \to F$).

    This is called \textbf{dual space} of $V$ and is usually denoted as $V'$.
\end{statement}

\begin{statement}[\citebarberian{3.9.2}]{Dimension of the Dual Space}
    If $V$ is finite-dimensional, then
    $$
        \dim V' = \dim V
    $$
\end{statement}

\begin{statement}[\citebarberian{3.9.4}]{Definition of Dual Basis}

    Let $x_1, \dots, x_n$ be a basis of $V$.
    One calls $f_1, \dots, f_n$ the basis of $V'$ dual to $x_1, \dots, x_n$ if

    $$
        f_i(x_j) = \begin{cases} 1 \text{ if } i = j \\ 0 \text{ if } i \ne j  \end{cases}
    $$

\end{statement}

\begin{statement}[\citebarberian{3.9.6 and 3.9.7}][Definition]{Definition of Transpose of $T$}

    Let $T': W' \to V'$ defined as defined as $T'(g) = gT$ (with $T: V \to W$).
    This mapping is linear and it's called \textbf{transpose} of $T$.

\end{statement}

This means that $T'$, given a linear form $g: W \to F$ will yield back another linear form, say $h: V \to F$ defined as $h = g \circ T$.

Basically $V \xrightarrow{T} W \xrightarrow{g} F$ and $V \xrightarrow {gT} F$.

\begin{statement}[\citebarberian{3.9.10}][Definition]{Definition of Annihilator}

    Let $M$ be a linear subset of $V$.
    The annihilator of $M \subset V'$ is the set of all linear forms such that $\forall x \in M, f(x) = 0$.
    The annihilator of $M$ is denoted as $M^\circ$.

    $M^ \circ$ is a linear subspace of $V'$.
\end{statement}

\begin{statement}[\citebarberian{3.9.11}]{Dimension of the Annihilator}

    $$
        \dim M ^\circ =  \dim V - \dim M
    $$

\end{statement}
\begin{proof}
    Let $\{x_1, \dots, x_k\}$ be a basis of $M$.
    Extend it do be a basis of $V$ such as $\{x_1, \dots, x_k, x_{k+1}, \dots, x_n\}$.
    Now consider the dual basis of $V'$ (as seen in \ref{Definition of Dual Basis}) $\{ f_1, \dots, f_n \}$.

    By the definition of dual basis $\{f_1, \dots, f_k\} \notin M^\circ$ because
    $\exists x \in M$ such that $f(x) \ne 0$. On the contrary, $\{f_{k+1}, \dots, f_n\} \in M^\circ$.

    Therefore $\dim(V') = \dim (M) + \dim (M^\circ)$, applying \ref{Dimension of the Dual Space} we get the result.
\end{proof}


\begin{statement}[\citebarberian{3.9.9}]{Kernel of $T'$ is the Annihilator of $T(V)$}

    Let $T: V \to W$ and $T': W' \to V'$ its transpose.
    Then

    $$
        \ker T' = [T(V)] ^\circ
    $$

\end{statement}

\begin{proof}
    By the definition of kernel we have that
    $\ker T' = \{ g \in W' : T'g = \underline 0\}$ with $T'g \in V'$.

    That is
    \begin{align*}
        \forall g \in \ker T' & \iff T'g = \underline 0  & \text{by def. kernel}               \\
                              & \iff (T'g)(V) = \{ 0 \}  & \text{by def. } \underline 0 \in V' \\
                              & \iff gT(V) = \{ 0 \}     & \text{by def. } T'                  \\
                              & \iff g(T(V)) = \{ 0 \}   & \text{by def. composition}          \\
                              & \iff g \in [T(V)] ^\circ & \text{by def. annihilator}          \\
    \end{align*}

\end{proof}


\begin{statement}[\citebarberian{3.9.12}]{Rank of the Transpose}

    $$\rho(T') = \rho(T)$$

    Their nullity $\nu$ might be different though.

\end{statement}

\begin{proof}
    The linear mappings are $T: V \to W$ and $T': W' \to V'$.
    We have:

    \begin{align*}
        \rho(T') & = \dim(\Im T')                      & \text{by def. rank } \rho                                      \\
                 & = \dim (W') - \dim (\ker T')        & \text{by rank-nullity theorem}                                 \\ % TODO: prove it
                 & = \dim (W') - \dim ((\Im T)^\circ)  & \text{by (\ref{Kernel of $T'$ is the Annihilator of $T(V)$}) } \\
                 & = \dim(W') - \dim (W) + \dim(\Im T) & \text{by (\ref{Dimension of the Annihilator})}                 \\
                 & = \dim(\Im T)                       & \text{by (\ref{Dimension of the Dual Space})}                  \\
                 & = \rho(T)                           & \text{by def. rank } \rho
    \end{align*}

\end{proof}

\clearpage

\section{Matrixes}

\citebarberian{Chapter 4}

\subsection{Transpose of a Matrix}

\citebarberian{Chapter 4.6}

\begin{statement}[\citebarberian{4.6.6}]{The Matrix of the Transpose is the Transpose of the Matrix}

    Let $V$ and $W$ be finite-dimensional vector spaces,
    $T: V \to W$ a linear mapping and $T': W' \to V'$ its transpose.
    Choose some basis of $V$ and $W$ and construct the dual bases of $V'$ and $W'$.

    Relative to these bases we have that

    $$
        \operatorfont{mat}(T') = (\operatorfont{mat} \  T)'
    $$

\end{statement}

\begin{proof}
    Let $(x_1, \dots, x_n)$ and $(y_1, \dots, y_m)$ be the bases of $V$ and $W$
    and $(f_1, \dots, f_n)$ and $(g_1, \dots, g_m)$ be their respective dual bases.

    We have that

    \begin{align}
         & \forall j = 1, \dots, n & Tx_j = \sum^m_{i=1} a_{ij}y_i   \label{eq:transpose:1} \\
         & \forall i = 1, \dots, m & T'g_i = \sum^m_{j=1} b_{ji}f_i \label{eq:transpose:2}
    \end{align}

    We will prove that $a_{ij} = b_{ji}$.

    Now, for any $i = 1, \dots, m$ and $k = 1, \dots, n$ we evaluate the $i$-th form in
    (\ref{eq:transpose:1}) at each basis $x_k$:

    \begin{align*}
        (T'g_i)(x_k) & = \sum^m_{j=1} b_{ji}f_i(x_k)     &                                         \\
        g_i(Tx_k)    & = \sum^m_{j=1} b_{ji} \delta_{jk} & \text{by def. transpose and dual basis} \\
                     & = b_{ki}                          & \text{by def. } \delta_{jk}
    \end{align*}

    But we can also use (\ref{eq:transpose:2}) to compute $g_i(Tx_k)$:

    \begin{align*}
        g_i(Tx_k) & = g_i \left(\sum^m_{h=1} a_{hk} y_h \right) &                             \\
                  & = \sum^m_{h=1} a_{hk} g_i(y_h)              &                             \\
                  & = \sum^m_{h=1} a_{hj} \delta_{ih}           & \text{by def. dual basis}   \\
                  & = a_{ik}                                    & \text{by def. } \delta_{ih}
    \end{align*}

    Hence $a_{ik} = g_i(Tx_k) = b_{ki}$.
\end{proof}

\begin{statement}[\citebarberian{4.6.7}]{Row Rank and Column Rank}
    The row rank of a matrix is equal to its column rank.
\end{statement}
\begin{proof}
    Let $A \in \mathscr M_{m,n}(\F)$ and $T: \F^n \to \F^m$
    such that $A$ is the matrix of $T$ relative to the canonical basis of $\F^n$.

    We saw how the column rank of $A$ is equal to the rank of $T$, % TODO: write proof of this.
    But the rank of $T$ is equal to the rank of $T'$ (\ref{Rank of the Transpose})
    which is equal to the column rank of $A'$ (\ref{The Matrix of the Transpose is the Transpose of the Matrix})
    which is equal to the row rank of $A$.

    Formally:

    \begin{align*}
        \rk_{\text{col}}(A) & = \rho(T)              \\
                            & = \rho(T')             \\
                            & = \rk_{\text{col}}(A') \\
                            & = \rk_{\text{row}}(A)
    \end{align*}
\end{proof}

\clearpage

\section{Similarity, Determinant and Trace}

\subsection{Determinant}

\begin{statement}[][Definition]{Definition of $(i,j)$-th Minor}
    Let $A \in \mathscr M_n(F)$.
    For each $(i,j)$, let $A^{i,j} \in \mathscr M_{n-1}$
    be the matrix obtained by removing $i$-th row and $j$-th column.

    We call $A^{i,j}$ the $(i,j)$-th minor of $A$.
\end{statement}

\begin{statement}[\citebarberian{7.4.1}][Definition]{Definition of Cofactor}
    Let $A \in \mathscr M_n(F)$.
    Then the cofactor of $a_{ij}$ is

    $$
        A_{ij} = (-1)^{i+j} \det A^{i,j}
    $$

\end{statement}

\begin{statement}[][Definition]{Definition of Determinant}
    Let $A \in \mathscr M_n(F)$.

    If $n = 1$, $\det A = a_{11}$.

    If $n \ge 2$ we have that

    $$
        \det A = \sum^n_{j=1} a_{1j}A_{1j} = \sum^n_{j=1} a_{1j}(-1)^{1+j} \det A^{1,j}
    $$

    with the notation of cofactor and $(i,j)$-th minor from before.
\end{statement}



\clearpage

\section{Inner product spaces}

\citeladr{Chapter 6}

\subsection{Inner product and norms}

\citeladr{Chapter 6.A}

\begin{statement}[][Definition]{Definition of complex conjugate}

    Let $z \in \C$, then

    $$
        \overline z = \overline{a + bi} = a - bi
    $$

\end{statement}

\begin{statement}[][Definition]{Definition of complex absolute value}

    Let $z \in \C$, then

    $$
        |z| = \sqrt{a^2 + b^2}
    $$

    and

    $$
        |z|^2 = z \overline z
    $$
\end{statement}

\begin{statement}[\citeladr{6.3, 6.4}][Definition]{Definition of Inner Product}

    Let $V$ be a finite-dimensional vector space.
    An inner product on $V$ is a function that takes an ordered pair
    $(u, v) \in V^2$ to a number in $\F$.

    It is usually denoted as

    $$
        \innerprod{u, v}
    $$

    Inner products have the following proprieties:

    \begin{enumerate}
        \item $\forall v \in V, \innerprod{v, v} \ge 0$
        \item $\forall v \in V, \innerprod{v, v} \ge 0 \iff v = \underline 0$
        \item Quasi-linearity: fixed the value on the right, the value on the left behaves like a linear function
        \item $\innerprod{v, u} = \overline {\innerprod{u,v}}$
    \end{enumerate}

\end{statement}

On $\F^n$ we can define the \textbf{Euclidean inner product} as

$$
    \innerprod{(x_1, \dots, x_n), (y_1, \dots, y_n)} = x_1 \overline{y_1} + \dots + x_n \overline{y_n}
$$

In the space of continuos real functions in an interval (say $[-1, 1]$)
we can define the inner product as follows

$$
    \innerprod{f, g} = \int^1_{-1} f(x)\overline{g(x)}dx
$$

In the space of polynomials we can define the inner product as follows

$$
    \innerprod{P, Q} = \int^ \infty _0 P(x)\overline{Q(x)} e^{-x} dx
$$


\begin{statement}[\citeladr{6.8}][Definition]{Definition of Norm}

    Let $v \in V$, then

    $$
        \norm v = \sqrt{\langle v, v \rangle}
    $$

\end{statement}

\begin{statement}[\citeladr{6.11}][Definition]{Definition of Orthogonal}

    Two vectors $u, v$ are orthogonal if

    $$
        \langle v, u \rangle = 0
    $$

    This is just a fancy way to say they are perpendicular since, for example, in $\R^2$, $\langle v, u \rangle = \norm v \norm u \cos \theta$.

\end{statement}

\begin{statement}[\citeladr{6.13}]{Pythagorean theorem}

    Let $v, u \in V$ be orthogonal, then

    $$
        \norm{u+v}^2 = \norm u^2 + \norm v^2
    $$

\end{statement}

\begin{proof}
    Let $u, v \in V$ such that $\innerprod{u, v} = 0$.
    Then

    \begin{align*}
        \norm{u+v}^2 & = \innerprod{u+v, u+v}                                                   \\
                     & = \innerprod{u, u+v} + \innerprod{v, u+v}                                \\
                     & = \innerprod{u, u} + \innerprod{u,v} + \innerprod{v,u} + \innerprod{v,v} \\
                     & = \norm u ^2 + \norm v ^2
    \end{align*}

    Note that this works only if $\innerprod{u, v} + \innerprod{v, u} = 0$.
    This is always true if $\F = \R$, but if $\F = \C$ we are not sure about this.
\end{proof}

\begin{statement}[\citeladr{6.14}]{Orthogonal Decomposition}

    Let $v, u \in V$.
    We need to find $w \in V$ such that $w$ is orthogonal to $v$.

    We want to find $c \in \F$ and such that

    $$
        u = cv + w, \quad \innerprod{v, w} = 0
    $$

    To solve this problem we can set

    $$
        c = \frac{\innerprod{u, v}}{\norm v^2},
        \quad
        w = u - \frac{\innerprod{u, v}}{\norm v^2} v
    $$

    In this way we obtain the wanted result.

\end{statement}

\begin{proof}
    \begin{align*}
        \innerprod{u, v} & = \innerprod{cv + w, v}                \\
                         & = c \innerprod{v,v} + \innerprod{w, v} \\
                         & = c \norm v ^2                         \\
        c                & = \frac{\innerprod{u,v}}{\norm v^2}
    \end{align*}
\end{proof}

\begin{statement}[\citeladr{6.15}]{Cauchy-Schwarz Inequality}

    Let $u, v \in V$. Then

    $$
        |\innerprod{u, v} \le \norm u \norm v
    $$
\end{statement}

\begin{proof}
    Consider the orthogonal decomposition $u = cv + w$.
    Since $cv$ is orthogonal to $w$, by the Pythagorean Theorem we have that

    \begin{align*}
        \norm u ^2 & = \norm {c v}^2 + \norm w^2                                             \\
                   & = \norm {\frac{\innerprod{u, v}}{\norm v^2} v}^2 + \norm w^2            \\
                   & = \left(\frac{\innerprod{u, v}}{\norm v^2} \norm v\right)^2 + \norm w^2 \\
                   & = \frac{|\innerprod{u, v} |^2}{\norm v^2} + \norm w^2                   \\
                   & \ge \frac{|\innerprod{u, v} |^2}{\norm v^2}                             \\
    \end{align*}

    Now we take square roots on both sides and rearrange the terms to get our result.

\end{proof}

\begin{statement}[\citeladr{6.18}]{Triangle Inequality}

    Let $u, v \in V$. Then

    $$
        \norm{u+v} \le \norm u + \norm v
    $$
\end{statement}

\begin{proof}
    \begin{align*}
        \norm{u+v}^2 & = \innerprod{u+v, u+v}                                                                 \\
                     & = \innerprod{u, u} + \innerprod{v,v} + \innerprod{u, v} + \innerprod{v, u}             \\
                     & = \innerprod{u, u} + \innerprod{v,v} + \innerprod{u, v} + \overline {\innerprod{u, v}} \\
                     & = \norm u^2 + \norm v^2 + 2 \Re \innerprod{u, v}                                       \\
                     & \le \norm u^2 + \norm v^2 + 2|\innerprod{u, v}|                                        \\
                     & \le \norm u^2 + \norm v^2 + 2 \norm u \norm v                                          \\
                     & = (\norm u + \norm v)^2
    \end{align*}
\end{proof}

\begin{statement}[\citeladr{6.22}]{Parallelogram Equality}

    Let $u, v \in V$. Then

    $$
        \norm {u+v}^2 + \norm{u-v}^2 = 2 (\norm u^2 + \norm v^2)
    $$
\end{statement}
This is quite easy to prove just with the definition of norm and inner product.

\clearpage

\subsection{Orthonormal Bases}

\citeladr{Chapter 6.B}


\begin{statement}[\citeladr{6.23}]{Definition of Orthonormal List}

    A list of vectors is orthonormal if each vector has norm 1 and
    is orthogonal to the other vectors in the list.

    This means that in an orthonormal list $e_1, \dots, e_n$ of vectors in $V$

    $$
        \innerprod{e_i, e_j} = \begin{cases*}
            1  \text{ if } j = k, \\
            0  \text{ if } j \ne k
        \end{cases*}
    $$
\end{statement}
From now on when we usually denote an orthonormal lists of vectors with $e_i$.

\begin{statement}[\citeladr{6.25}]{Norm of an orthonormal linear combination}

    Let $e_1, \dots, e_n$ orthonormal. Then

    $$
        \norm{a_1 e_1 + \dots + a_n e_n} = |a_1|^2 + \dots + |a_n|^2
    $$

    for all $a_1, \dots, a_n \in \F$.
\end{statement}
\begin{proof}
    This is just the repeated application of the Pythagorean Theorem.
\end{proof}


\begin{statement}[\citeladr{6.26, 6.27 and 6.28}]{Definition of Orthonormal Basis}
    As a corollary of the previous theorem we have that \textbf{every orthonormal list of vectors is linearly independent}.
    This is quite obvious from the previous theorem.

    It follows that if we have an orthonormal list of the right length we can form a basis.
    Such basis will be referred as an \textbf{orthonormal basis}.
\end{statement}

\begin{statement}[\citeladr{6.30}]{Writing a Vector as a L.C. of Orthonormal Basis}

    Let $e_1, \dots, e_n$ be an orthonormal basis of $V$ and $v \in V$.
    Then

    $$
        v = \innerprod{v, e_1}e_1 + \dots + \innerprod{v, e_n}e_n
    $$

    and

    $$
        \norm v^2 = |\innerprod{v, e_1}| + \dots + |\innerprod{v, e_n}|
    $$

\end{statement}

\begin{statement}[\citeladr{6.31}]{Gram-Schmidt Procedure}

    This is a procedure for finding orthonormal basis of a given vector space.

    Let $v_1, \dots, v_n$ be a basis of $V$.
    Define by induction the list $e_1, \dots, e_n$ such that for $i = 1$ we have that

    \begin{align*}
        v_1' & = v_1                   \\
        e_1  & = \frac{v_1}{\norm v_1} \\
    \end{align*}

    $\forall i \ge 2$ we define

    \begin{align*}
        v_i' & = v_i - \innerprod{v_i, e_1}e_1 - \dots - \innerprod{v_i, e_{j-1}}e_{j-1} \\
        e_i  & = \frac{v_i'}{\norm {v_i'}}                                               \\
    \end{align*}

\end{statement}

\begin{statement}[\citeladr{6.42}]{Riesz Representation Theorem}
    Let $V$ be finite-dimensional and $\varphi: V \to \F$.
    Then there exists an unique $u \in V$ such that

    $$
        \forall v \in V, \quad \varphi(v) = \innerprod{v, u}
    $$

    This gives an isomorphism between $V' = \mathscr L(V, \F)$ and $V$.
\end{statement}

\clearpage

\subsection{Orthogonal Complements}

\citeladr{Chapter 6.B}

\begin{statement}[\citeladr{6.45}]{Definition of Orthogonal Complement}
    Let $U \subset V$ (not necessarily a subspace).
    Then we denote as $U^\perp$ the \textbf{orthogonal complement} of $U$
    the set of all vectors in $V$ orthogonal to every vector in $U$.

    $$
        U^\perp = \{ v \in V \enspace | \enspace \forall u \in U, \innerprod{v, u} = 0 \}
    $$

\end{statement}

\begin{statement}[\citeladr{6.46, 6.47, and 6.48}]{Proprieties of the Orthogonal Complement}

    Applying the Riesz theorem we get an isomorphism between $U^\perp$ and $U^\circ$.

    \tcbline

    The following proprieties hold:

    \begin{enumerate}
        \item $U^\perp$ is a linear subspace of $V$
        \item $\{\underline 0\}^\perp = V$
        \item $V^\perp = \{\underline 0\}$
        \item $U \cap U^\perp$ is either $\{\underline 0\}$ or $\varnothing$
        \item If $U \subseteq W$, then $W^\perp \subseteq U^\perp$
    \end{enumerate}

    \tcbline

    Also it holds that

    $$
        V = U \oplus U^\perp
    $$

    \tcbline

    Moreover, if $V$ is finite-dimensional we get that

    $$
        \dim U^\perp = \dim V + \dim U
    $$



\end{statement}

\clearpage

\section{Operators on Inner Product Spaces}

\citeladr{Chapter 7}

\subsection{Self-Adjoint and Normal Operators}

\citeladr{Chapter 7.A}

\begin{statement}[\citeladr{7.11}]{Definition of Self-Adjoint Mapping}
    A linear mapping $T \in \mathscr L(V)$ is self-adjoint if and only if

    $$
        \forall v, w \in V, \quad \innerprod{Tv, w} = \innerprod{v, Tw}
    $$
\end{statement}

\begin{statement}[\citeladr{7.10 (chopped)}]{Transpose and Conjugate of Self-Adjoint Mapping}

    Let $V$ be finite-dimensional, $T$ be a self-adjoint mapping on $V$,
    and $\mathscr B = (e_1, \dots, e_n)$ be an orthogonal basis of $V$.

    Then the matrix $A$ of $T$ relative to $\mathscr B$ has the following propriety

    $$
        A' = \overline A
    $$

    (The transpose of $A$ is its conjugate.)

\end{statement}

\begin{statement}[\begin{quote}
            This section references the slides of week 11 and is not contained in this chapter of "Linear Algebra Done Right".
        \end{quote}]{Transpose of the Product of Matrices}

    Let $A \in \mathscr M_{m,n}(F)$ and $B \in \mathscr M_{n, p}(F)$ such that $AB$ is defined. Then

    $$
        (AB)' = B'A'
    $$

\end{statement}

This is a quite general result that will be useful for other proofs even though it's not strictly part of this topic.

\clearpage

\subsection{The Spectral Theorem}

\citeladr{Chapter 7.B}

\begin{statement}{Eigenvalues of Self-Adjoint Mapping are Real}
    Let $T$ be a self-adjoint mapping on $V$.
    Then all the eigenvalues of $T$ are real.

    In other words if a matrix $A$ is such that $A' = \overline A$,
    then $P_A$ (characteristic polynomial) has all roots in $\R$.
\end{statement}

\begin{statement}[\citeladr{7.24 and 7.29}]{The Spectral Theorem}

    Let $V$ be finite-dimensional and $T$ be a self-adjoint mapping on $V$.
    Then there exists an orthonormal basis of $V$ consisting of eigenvectors of $T$.

    Moreover if $\F = \R$ this is an equivalence (if and only if).

\end{statement}
\begin{proof}
    We proceed by induction on the dimension of $V$.
    Let $n = \dim V$.

    First, if $n = 1$ there is nothing more to prove
    since we saw already that $T$ has at least one eigenvalue.

    If $n \ge 2$ let $\lambda$ be an eigenvalue of $T$ and $u$ its eigenvalue.
    We can find another eigenvector of $T$ $u_n = \frac{u}{\norm u}$.
    Let $U = [u_n]$ (the span of $u_n$) and $U^\perp$ which will have $\dim U^\perp = n-1$.

    We now want to prove that any $Tv \in U^\perp$.
    To do so we have to show that any $Tv$ is orthogonal to $u_n$.
    We proceed as follows:

    $$
        \innerprod{Tv, u_n} \stackrel{\text{self-adjoint}}{=}
        \innerprod{v, Tu_n} \stackrel{\text{eigenvector}}{=}
        \innerprod{v, \lambda u_n} \stackrel{\text{semi-linearity}}{=}
        \overline \lambda \innerprod{v, u_n} \stackrel{\text{orthognality of } U^\perp}{=}
        0
    $$

    (Note that $\lambda \in \R$, so $\overline \lambda = \lambda$.)

    Then we can restrict $T$ such that $\tilde T: U^\perp \to U^\perp$.
    $\tilde T$ is a self-adjoint mapping of $U^\perp$,
    hence we can repeat the procedure and get an orthonormal basis of $U^\perp$
    made of eigenvectors of $\tilde T$ that looks like $u_1, \dots, u_{n-1}$.
    To this list we append $u_n$ so that the resulting list $u_1, \dots, u_{n-1}, u_n$
    is an orthonormal basis of $V$ consisting of eigenvectors of $T$.

\end{proof}

\clearpage

\end{document}
