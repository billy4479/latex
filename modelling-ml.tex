\documentclass[12pt]{extarticle}

\setlength{\headheight}{16pt} % ??? we do what fancyhdr tells us to do  

\title{Mathematical Modelling for ML}
\author{Giacomo Ellero}
\date{a.y. 2024/2025}

\usepackage{preamble}

%\renewcommand{\vec}[1]{\uvec{#1}}
\renewcommand{\vec}[1]{\bm{#1}}

\begin{document}

\oldfirstpage

\section{Introduction}

Assume our data is of the form $(x_n, y_n)$, where $x \in \R ^d$ and $y \in \R$
and we want to find a predictor $f_\theta(\cdot): \R^d \to \R$.

To figure out how good our model is we want to find a \emph{loss function} which tells us
how far is our predicted value from the real one.
Ideally we'd like to \say{count} the errors we make and use that as our loss function,
however these functions are usually not differentiable.
To fix this problem we look for another function which is differentiable and bounds the \say{true}
loss function, an example is the \emph{square error} (as we saw in statistics).
Eventually we want to find a model which minimizes the average loss:
\begin{equation}
	\argmin_\theta \bm R_\text{emp}(\theta) = \frac{1}{N} \sum_{i = 1}^N \ell(y_i, \hat y_i)
\end{equation}
Note that we will use $\theta$ and $w$ interchangeably,
they both represents the parameters/coefficients/weights of our model.

As we saw in linear regressions, the best estimator for $w$ is
\begin{equation}
	\hat w= (W^T W)^{-1} X^T y
\end{equation}
However, the Hessian matrix (given by $\laplacian \ell (w) = X^T X$)
is \emph{positive semi-definite}: this means that the function indeed has a global minimum,
but it might not be unique.
If we find more than one minimum we take the one which is closer to the origin:
in this way we avoid noise as much as possible.

\subsection{Reducing overfitting}
When we train our model we try to minimize the error against the training set.
Then we run the model against the test dataset:
of course we will get a higher loss than on the training set, however we want to check how the model
behaves when we modify the number of parameters to check for overfitting or underfitting.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{./assets/modelling-ml/overfitting-vs-underfitting.jpg}
	\caption{Graph of the loss function as the number of parameters changes.}
\end{figure}

To reduce the number of parameters and ensure positive definiteness (i.e. uniqueness of the minimum)
we can use a \emph{Ridge regression} (see statistics), where we penalize the model based on the
magnitude of the parameters, this is more numerically stable.

\subsection{Bayes theorem}

\begin{equation}
	p(\theta \mid x) = \frac{p(x\mid \theta) p(\theta)}{p(x)}
\end{equation}

Our final goal is to compute the posterior $p(\theta \mid x)$.
This is hard by itself, however by using the theorem we can separate it in \say{computable}
chunks: $p(x \mid \theta)$ is the easiest; while the prior $p(\theta)$
is definitely more \say{guessable}.

Instead of minimizing the negative maximum likelihood estimator
we minimize the negative log-posterior
using a technique called \emph{maximum a posteriori} estimation.
\begin{equation}
	\argmin_\theta - \log [p(x \mid \theta) p(\theta)]
\end{equation}

\subsection{Kullback-Leibler divergence}

The definition of the KL divergence is
\begin{equation}
	D_{KL}(p,q) = \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}
\end{equation}
This is a way to measure the distance between two functions.
We can use this to minimize the distance between the given data and some theoretical distribution.
We can model the empirical data into an actual distribution by considering its histogram:
this distribution will just be a sum of $\delta$ functions of the observations we had.

\begin{proposition}{Non-negativity of DL-divergence}{dl-non-neg}
	For all probability measures $p, q$ we have that $D_{KL}(p, q) \geq 0$.
\end{proposition}

\begin{proof}
	First invert the sign of the definition:
	\begin{align}
		-D_{KL}(p,q)          & = -\sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}   \\
		                      & = \sum_{x \in X} p(x) \log \frac{q(x)}{p(x)}    \\
		                      & \leq \log \sum_{x \in X} p(x) \frac{q(x)}{p(x)} \\
		                      & = \log \sum_{x \in X} q(x)                      \\
		                      & = \log (1) = 0                                  \\
		\implies D_{KL}(p, q) & \geq 0
	\end{align}
	where we have used Jensen inequality.
\end{proof}

\subsubsection{KL divergence and MLE}

We can use this distance in order to \say{fit} a model to some data:
we set $P$ to be the distribution of the empirical data, defined as
\begin{equation}
	P(x) = \frac{1}{M} \sum^M_{\mu = 1} \delta(x, x^\mu)
\end{equation}
where $\delta(x, x^\mu)$ is $1$ when $x$ is equal to the empirical sample data and $0$ otherwise.
$Q_\theta(x)$ is the approximating probability distribution.

Turns out that minimizing the KL divergence is equivalent to maximizing the MLE.

\begin{proposition}{KL divergence and MLE}{kl-mle}
	Let $P$ be the empirical distribution of the sample and $Q_\theta$ the model distribution
	parametrized by $\theta$.
	Then minimizing the KL divergence is equivalent to maximizing the MLE for $\theta$:
	\begin{equation}
		\argmin_\theta D_{KL}(P, Q_\theta) = \argmax_\theta \frac{1}{M} \sum^M_{\mu = 1} \log
		Q_\theta(x^\mu)
	\end{equation}
\end{proposition}
\begin{proof}
	We start by rewriting the KL distance:
	\begin{align}
		D_{KL}(P, Q_\theta) & = \sum_x P(x) \log \frac{P(x)}{Q_\theta(x)}                                                                                   \\
		                    & = \sum_x \frac{1}{M} \sum^M_{\mu = 1} \delta(x, x^\mu) \log \frac{\frac{1}{M} \sum^M_{\mu = 1} \delta(x, x^\mu)}{Q_\theta(x)} \\
		                    & = \frac{1}{M} \sum^M_{\mu = 1} \log \frac{\frac{1}{M}}{Q_\theta(x^\mu)}                                                       \\
		                    & = \frac{1}{M} \sum^M_{\mu = 1}\left( \log \frac{1}{M} - \log Q_\theta(x^\mu) \right)                                          \\
		                    & = -\underbrace{\log M}_\text{const} - \frac{1}{M} \sum^M_{\mu = 1} \log Q_\theta(x^\mu)
	\end{align}
	where the first term is a constant and we recognize the second term to be the MLE.
\end{proof}

\section{Dimensionality reduction}

Many times we need to analyze very high dimensional data: this is usually very hard to analyze,
almost impossible to visualize and storing it can be expensive.
We want to project our high dimensional data onto a lower dimensional space, however we want to do
this in a smart way, such that we lose the least information possible.

\subsection{The setting}

We will use a linear setting for simplicity.

Our setup is defined as a dataset $\vec X = \{ \vec x_1, \dots, \vec x_N\}$
with each $\vec x_i \in \R^D$, also assume that they are i.i.d. and with mean $0$.
Then define the \emph{data covariance matrix} as
\begin{equation}
	S = \frac{1}{M} \sum_{i = 1}^N \vec x_i \vec x_i^T
\end{equation}

We will want to project to a space of dimension $M$. If we assume $M = D$, then we can have
an invertible matrix $B$ that projects $\bm X$ onto $\R^M$ and $B^{-1}$ which does the opposite
without any error.

However, $M < B$, therefore $B$ is not a square matrix hence not invertible.
Our goal is to find a matrix $B$ such that $\vec z_i = B^T \vec x_i$ and
$\tilde{\vec x_i} = B \vec z_i$ and the error, defined as
\begin{equation}
	\norm{\vec x_i - \tilde{\vec x_i}}^2
\end{equation}
is as small as possible.
We call $B^T$ the \emph{encoder} and $B$ the \emph{decoder}.
Moreover, we want $B^T$ to be \textbf{orthonormal},
that is its columns $\vec b_i$ have $\norm{\vec b_i} = 1$ and $\vec b_i \cdot \vec b_j = 0$.

\subsection{Eigendecomposition}

In order to preserve as much data as possible we want to project in the direction that preserves
the maximum variance in the output.

We want to find vectors $b_1, \dots, b_M \in \R^D$ which will be the columns of $B^T$
Consider some vector $\vec b_i$, we want that projecting the data in its direction
$\vec z_{in} = \vec b_i^T \vec x_n \in \R$ has maximum variance:
\begin{align}
	V_i & = \mathrm{var} (\vec z_i) = \frac{1}{N} \sum^N_{j = 1} \vec z_{ij}^2               \\
	    & = \frac{1}{N} \sum^N_{j = 1} (\vec b_i^T \cdot \vec x_n)^2                         \\
	    & = \frac{1}{N} \sum^N_{j = 1} \vec b_i^T \vec x_n \vec x_n^T \vec b_i               \\
	    & = \vec b_i^T \left(\frac{1}{N} \sum^N_{j = 1} \vec x_n \vec x_n^T \right) \vec b_i \\
	    & = \vec b_i^T S \vec b_i
\end{align}
where we have used the symmetry of the product $\vec a^T \vec b = \vec a \vec b^T$.
We have obtained the so called \emph{data covariance matrix}: this is an $n \cross n$
\emph{symmetric} matrix.

\begin{theorem}{Eigendecomposition for PCA}{eigendecomposition}
	The orthonormal matrix $B^T$ which maximizes variance of the projected data
	for each input $\vec x_n$	have as its columns the eigenvectors of $S$
	with the largest eigenvalues.
\end{theorem}

\begin{proof}
	First, note that the existance of an orthonormal basis of eigenvectors of $S$ is guaranteed by
	the spectral theorem.

	We will proceed by induction, adding one by one each vector $\vec b_i$ in our basis.
		{
			\setlist{font=\normalfont\itshape}
			\begin{description}
				\item[Base case]
				      We want to solve
				      \begin{align}
					      \max_{\vec b_1} \vec b_1^T S \vec b_1 &                       \\
					      \text{subject to }                    & \norm{\vec b_1}^2 = 1
				      \end{align}

				      The lagrangian is
				      \begin{equation}
					      \mathcal L(\vec b_1, \lambda) = \vec b_1^T S \vec b_1 + \lambda(1-\vec b_1 \vec b_1^T)
				      \end{equation}
				      and setting its derivatives equal to zero we get that $S \vec b_1 = \lambda \vec b_1$
				      and $\vec b_1^T \vec b_1 = 1$.

				      By substituting we obtain
				      \begin{equation}
					      V_1 = \vec b_1^T S \vec b_1 = \lambda \vec b_1^T \vec b_1 = \lambda_1
				      \end{equation}
				      which confirms that $\vec b_1$ is an eigenvector of $S$.
				      Moreover, $V_1 = \lambda_1$ (the eigenvalue of $\vec b_1$), therefore we choose
				      $\vec b_1$ to be the eigenvectors with largest eigenvalue within the orthonormal basis
				      of $S$.

				\item[Inductive step] Assume we already have $b_1, \dots, b_k$ orthonormal eigenvectors
				      vectors of $S$ and we want to show that the next best orthonormal vector
				      $\vec b_{k+1}$ is also an eigenvector of $S$.

				      We want to solve
				      \begin{align}
					      \max_{\vec b_{k+1}}  \vec b_{k+1}^T S \vec b_{k+1} &                                                             \\
					      \text{subject to }                                 & \norm{\vec b_{k+1}}^2 = 1                                   \\
					                                                         & \vec b_{k+1}^T \vec b_i = 0 \quad \forall i \in 1, \dots, k
				      \end{align}
				      which gives us the lagrangian
				      \begin{equation}
					      \mathcal L(\vec b_1,\dots, \vec b_{k+1}, \lambda, \eta_1, \dots, \eta_k)
					      = \vec b_1^T S \vec b_1 - \lambda(\vec b_1 \vec b_1^T -1)
					      - \sum_{i = 1}^k \eta_i \vec b_{k+1}^T \vec b_i
				      \end{equation}
				      (note that we can change the signs pretty much the way we like,
				      we can incorporate the change of sign inside $\lambda$ or $\eta_i$.)

				      We take the derivative with respect to $\vec b_{k+1}$ and set it equal to zero
				      \begin{equation}
					      \pdv{\mathcal L}{\vec b_{k+1}} = 2 S \vec b_{k+1} - 2 \lambda \vec b_{k+1}
					      - \sum_{i = 1}^k \eta_i \vec b_i = 0
					      \label{eq:pca-lagrangian}
				      \end{equation}

				      Now, multiply both sides by $\vec b_j^T$ with $j \in 1, \dots, k$ so that
				      \begin{align}
					      0 & = 2 \vec b_j^T S \vec b_{k+1} - 2 \lambda \vec b_j^T \vec b_{k+1} - \sum_{i = 1}^k \eta_i \vec b_j^T \vec b_i \\
					        & = 2 \lambda_j \vec b_j^T \vec b_{k+1} - 0 - \eta_j                                                            \\
					        & = \eta_j                                                                                                      \\
				      \end{align}
				      where we have used the fact that $\vec b_j$ is an eigenvectors of $S$ and $\vec b_j$
				      is orthonormal to the other $\vec b_i$ and to $\vec b_{k+1}$.

				      We have obtained that $\eta_j = 0$ for any $j$, therefore, substituting this result
				      in \cref{eq:pca-lagrangian}, we obtain
				      \begin{equation}
					      S \vec b_{k+1} = \lambda \vec b_{k+1}
				      \end{equation}
				      which tells us that $\vec b_{k+1}$ is also an eigenvector of $S$ and gives us that
				      $V_{k+1} = \lambda$.
				      \qedhere
			\end{description}
		}
\end{proof}

\subsubsection{Data compression prospective}
It can be proven that the same conclusion would be reached if we tried to minimize the mean square
error instead.

In the end, performing some (a lot) of algebra on the mean square error we reach the same variance
formula we had before.

\subsection{Principal component analysis}

Our algorithm is then as follows:
\begin{enumerate}[label=\roman*.]
	\item Shift by the mean
	\item Scale by the variance of each dimension of the dataset
	\item Apply eigendecomposition
	\item Project along the chosen direction
\end{enumerate}

Moreover, we can prove (see slides, lecture 3), that minimizing the reconstruction error
is equivalent to finding the projected maximum variance.

\subsection{Non linear encoders}
To achieve better results we can have, on top of the linear transformation, some non-linear function,
this of course adds complexity but also gives better results.

For some $f, g$ non-linear write
\begin{align}
	\vec z          & = f(W \vec x) \\
	\vec {\tilde x} & = g(V \vec z)
\end{align}

This could lead to way better results at the price of a much larger complexity.

\section{Entropy}

\subsection{Definition}

Given a probability measure $P(x)$ we define the (Shannon) entropy as
\begin{equation}
	h(x) = \log_2 \frac{1}{P(x)}
\end{equation}
Entropy is the measure of the \say{surprise}, the more the event $x$ is unlikely, the larger the
entropy.
We can also define the average entropy of some sample $X$
\begin{equation}
	H(X) = \sum_{x \in X} P(x) \log_2 \frac{1}{P(x)} = \mathds{E}_x[h(x)]
\end{equation}

\subsubsection{Derivation}

The definition of entropy comes from four fundamental axioms about the amount of information $I(x)$
carried by each event. Consider two events $x, y$ with probabilities $p, q$:
\begin{enumerate}
	\item \emph{Normalization}: If $P(x) = 1$ then $I(p) = 0$.
	\item \emph{Monotonicity}: If $P(x) > P(y)$ then $I(p) < I(q)$.
	\item \emph{Continuity}: $I(p)$ is continuous in $p$.
	\item \emph{Additivity} for independent events: If $x \indep y$ then $I(pq) = I(p) + I(q)$.
\end{enumerate}

We can use these axioms to identify that any entropy function must have the form of
\begin{equation}
	I(p) = -C \ln p \quad \text {with } C > 0
\end{equation}
Indeed, Shannon entropy respects this result, since all logarithms are the same up to multiplication
by a constant.

\subsubsection{Information gain}

We define the conditional entropy as
\begin{equation}
	H(Y \mid X = x) = - \sum_{y \in Y} P(y \mid x) \log P(y \mid x)
\end{equation}
This allows us to define the \emph{information gain} from knowing that a certain event $x$ happened:
\begin{equation}
	\mathrm{IG}(x) = H(Y) - H(Y \mid X = x)
\end{equation}

\subsection{Decision trees}

We can use information gain to ask questions about some sample we want to classify:
assume our sample is $k$ dimensional, we split the data based on some feature $x_j > \nu$ which
maximizes information gain.

To choose the right feature $x_j$ and the right value of $\nu$ note that, even though $\nu \in \R$,
our sample is finite, therefore we only have a finite number of values $x_j$ can take which means
we only have a finite number of splits: for each $j \in 1, \dots, k$ we can just test each value
that $x_j$ takes and find the split that maximizes information gain.

Then we repeat the procedure for each split, asking a different question for each one of them, in
order to maximize the information gain each time.
We can define various criteria to stop, some of the most commons are bounding the depth of the
tree or stopping when the information gain for every choice is less than some threshold.
Note that decision trees have the tendency to overfit.

Finding the smallest consistent tree is a very complicated problem (NP-hard), however our greedy
algorithm still gives ok results.

\subsubsection{Random forests}

An even more effective way of using decision trees through the technique of
\emph{ensamble learning}, which means we take many models and the prediction is the result of a
majority vote or the average of each model.

This is useful as we can prove that averaging many i.i.d. random variables we obtain one with the
same mean but smaller variance.

Since the trees are deterministic (given the same input and hyperparameters they will produce the
same output) we differentiate them by sampling different portions of the dataset or by selecting
only certain features to split on.

Random forests are still used as a benchmark in comparison to more sophisticated approaches.

\section{Clustering}

Clustering is a class of unsupervised learning algorithms which classify the input data
$\vec x_1, \dots, \vec x_N \in X$ into $K \in \N$ partitions of $X$.
The input to the algorithm are the data $\vec x_1, \dots, \vec x_N \in X$, the number of clusters
$K$ and a distance function $d: X \cross X \to \R$.


\begin{definition}{Distance function}{distance-function}
	A function $d: X \cross X \to \R$ is a distance function on $X$ if
	\begin{enumerate}[label=\roman*.]
		\item \emph{Non-negativity}: $d(x, y) \geq 0$ for all $x, y \in X$.
		\item \emph{Identity}: $d(x, y) = 0$ if and only if $x = y$.
		\item \emph{Symmetry}: $d(x, y) = d(y, x)$ for all $x, y \in X$.
		\item \emph{Triangle inequality}: $d(x, z) \leq d(x, y) + d(y, z)$ for all $x, y, z \in X$.
	\end{enumerate}
\end{definition}

In our applications usually $X = \R^d$ and $d$ is one of the Minkowski metrics.

\begin{definition}{Minkowski metrics}{minkowski-metrics}
	Given a parameter $p \in \N$ we define the Minkowski distance on $\R^d$ as
	\begin{equation}
		d_p(\vec x, \vec y) = \left( \sum_{i = 1}^D \abs{x_i - y_i}^p\right)^{\frac{1}{p}}
	\end{equation}

	In particular:
	\begin{itemize}
		\item When $p = 1$ we obtain the \emph{Manhattan distance};
		\item When $p = 2$ we obtain the \emph{Euclidean distance};
		\item When $p \to \infty$ we obtain the \emph{Chebyshev distance}, that is
		      \begin{equation}
			      d_\infty(\vec x, \vec y) = \max_i (\abs{x_i - y_i})
		      \end{equation}
	\end{itemize}
\end{definition}

\subsection{Hierarchical algorithms}

This algorithm starts with $N$ clusters, one for each sample, and starts merging them based on the
distance between each cluster.
We could also do the opposite: start with one big cluster and split it up, but we will not focus on
this.

Starting from $N$ clusters we look for the two clusters with the smallest distance between them and
we merge them. Then we update the distance matrix between this new cluster and all the others and
repeat until we only have $K$ clusters left.

There are many ways to define the distance between two clusters $d(c_i, c_j)$:
\begin{itemize}
	\item \emph{Single linkage}:
	      \begin{equation}
		      d(c_i, c_j) = \min_{\substack{a \in c_i \\ b \in c_j}} d(a, b)
	      \end{equation}

	\item \emph{Complete linkage}:
	      \begin{equation}
		      d(c_i, c_j) = \max_{\substack{a \in c_i \\ b \in c_j}} d(a, b)
	      \end{equation}

	\item \emph{Average linkage}:
	      \begin{equation}
		      d(c_i, c_j) = \frac{1}{\abs{c_i} \abs{c_j}}
		      \sum_{\substack{a \in c_i \\ b \in c_j}} d(a, b)
	      \end{equation}
\end{itemize}

This procedure is $O(N^2)$ and it is a totally greedy approach.

The output of this algorithm is either the partition of $X$ or a \emph{dentrogram} (see
\cref{fig:dendrogram}).
These are also useful to find outliers (see lecture notes for example).

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.5\textwidth]{./assets/modelling-ml/dendrogram.png}
	\caption{A dendrogram.}
	\label{fig:dendrogram}
\end{figure}

\subsection{\texorpdfstring{$k$}{k}-means clustering}

In this algorithm we start by randomly picking $\vec \mu_1, \dots, \vec \mu_K \in \R^d$ which will be the
\say{centers} of our clusters.
Then we repeat the following steps until convergence:
\begin{itemize}
	\item \emph{Update samples}: for each $i \in \{1, \dots, N\}$ we put $\vec x_i$ into the
	      \say{closer} cluster.
	      \begin{equation}
		      c_i = \argmax_{j \in \{1, \dots, K\}} d(\vec x_i, \vec \mu_j)
	      \end{equation}
	      where $c_i$ is the index of the cluster of $x\vec x_i$.

	\item \emph{Update centers}: for each $j \in \{1, \dots, K\}$ we update $\vec \mu_k$ to be the
	      center of the clusters.
	      \begin{equation}
		      \vec \mu_j = \frac{\sum_{i = 1}^N \delta(c_i, j) \vec x_i}{\sum_{i = 1}^N \delta(c_i, j)}
	      \end{equation}
\end{itemize}

It is possible to prove that the algorithm will converge and that each iteration will take $O(Nk)$
but it is not possible to know how many iterations it will take to converge.

\section{Perceptron}

This is a model that tries to work similarly to the actual biological neurons.
The simplest model of a neuron looks like
\begin{equation}
	y = \Theta \left(\sum_i w_i x_i - T \right)
\end{equation}
where $y \in \{0, 1\}$ is the neuron output (either it fires or it doesn't),
$\Theta$ is the Heavisde function defined as
\begin{equation}
	\Theta(x) = \begin{cases}
		1 & \text{if } x \geq 0 \\
		0 & \text{otherwise}
	\end{cases}
\end{equation}
while $w_i$ are called weights and $x_i$ are the outputs of other connected neurons.
$T$ is the fire threshold.

By changing the weight we can modify the way that neurons fire: this is a phenomenon which happens
also in biological brains.

Depending on how the neurons are connected to each other we can define \emph{feedforward}
networks, where each neuron is connected only to some \say{next} ones, and \emph{recurrent} ones,
where we can have neurons which link back to other neurons, this for example allows for short-term.

\subsection{Learning}

There are multiple way an actor can learn: we can have an unsupervised learning where the actor
is just trying to make statistical sense of the input;
we can have reinforced learning where the actor gets a reward for a correct output, this is what
happens with dopamine in biological brain;
we can have supervised learning where the actor knows what it wants to achieve and it gets feedback
on how good or bad it did, for example trying to throw a ball at a target.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{./assets/modelling-ml/brain-learning-types.jpg}
	\caption{Different parts of the brain learn in different modes.}
\end{figure}

We want our model to be able to classify inputs $\vec x^\mu$ as $y^\mu \in \{0, 1\}$,
that is, we want to find some weights $\vec w$ such that
\begin{equation}
	y^\mu = \Theta(\vec w \cdot \vec x^\mu)
\end{equation}
Geometrically, this is equivalent to finding an hyperplane which divides the two sets of inputs.

A simple perceptron is able to solve problems where the inputs are already linearly separable,
for more complex inputs we will need to add hidden layers, but we will cover this further
ahead in the course.

\subsubsection{Learning algorithm}

Initialize $t = 0$ and $\vec w_t$.
Now iterate on $t$: pick a pattern $\mu$, if $\vec w_t$ gives the right output we leave it the same,
if it doesn't we modify $\vec w_{t+1}$ as follows
\begin{equation}
	\vec w_{t + 1} = \begin{cases}
		\vec w_t + \eta \vec x^\mu & \text{if } y^\mu = 1 \\
		\vec w_t - \eta \vec x^\mu & \text{if } y^\mu = 0
	\end{cases}
\end{equation}
where $\eta \in \R$ is the learning rate.

\begin{proposition}{Convergence of the perceptron algorithm}{perceptron-convergence}
	If there exists an hyperplane which separates the inputs as desired
	the perceptron algorithm will converge to it.
\end{proposition}

\begin{proof}
	We claim that if there exists $\vec w^*$ with $\norm{\vec w^*} = 1$ and
	$\vec w^* \cdot \vec z^\mu > \varepsilon$ for all $\mu$ then the algorithm will converge to it.

	For simplicity consider a learning rate $\eta = 1$
	and we let $\vec z^\mu = (2 y^\mu - 1) \vec x^\mu$ so that
	\begin{equation}
		\vec z^\mu = \begin{cases}
			\vec x^\mu  & \text{if }y^\mu = 1 \\
			-\vec x^\mu & \text{if }y^\mu = 0
		\end{cases}
	\end{equation}
	Moreover we assume that $\vec z^\mu$ is bounded, i.e. $\exists K : \norm{\vec z^\mu} < K$,
	such that $\vec w \cdot \vec z^\mu > 0$.

	The idea of this claim is that we want to find a lower bound for $A_t = \vec w_t \cdot \vec w^*$
	and an upper bound for $B_t = \norm{\vec w_t}^2$.

	Assume there exists a $\mu$ such that such $\vec w^*$ exists.
	Intialize the algorithm with $\vec w_0 = 0$ start the algorithm.
	At some time $t$ assume that $\vec w_t \cdot \vec z^\mu <0$, which means we have to perform the
	the $t+1$ modification to $\vec w$.

	For $A_{t+1}$ we have
	\begin{align}
		A_{t+1} & = (\vec w_t + \vec z^\mu) \cdot \vec w^* \\
		        & = A_t + \vec z^\mu \cdot \vec w^*        \\
		        & > A_t + \varepsilon                      \\
		        & > (t+1) \varepsilon                      \\
	\end{align}
	where in the last step we have used the fact that $A_0 = $ since $\vec w_0 = 0$ and then proceeded
	by induction.

	For $B_{t+1}$ we have
	\begin{align}
		B_{t+1} & = \norm{\vec w_t + \vec z^\mu}^2                             \\
		        & = \norm{\vec w_t}^2 + 2 \vec w_t \vec z^\mu + \norm{z^\mu}^2 \\
		        & < B_t + K^2                                                  \\
		        & < (t+1)K^2
	\end{align}
	where we have used the same induction as before and the fact that $\vec w_t z^\mu < 0$.

	Now, since $A_t < \sqrt{B_t}$ for all $t$ we have
	\begin{align}
		t\varepsilon < A_t & < \sqrt{B_t} < \sqrt{t K^2} \\
		t^2 \varepsilon^2  & < t K^2                     \\
		t                  & < \frac{K^2}{\varepsilon^2}
	\end{align}
	which gives us a bound for $t$.
\end{proof}

\subsubsection{Capacity of a perceptron}
Given a set of $p$ associations $\vec x^\mu \to y^\mu$ what is the probability to find
a weight vector that correctly classifies all input patterns?

We know perceptrons can only work for patterns which are separable by an hyperplane.
Therefore our task is to find the probability that our inputs can be separated by a hyperplane.

We solve this problem geometrically, by finding the number of dichotomies of the input (that is,
what are the possible ways we can color the inputs with two colors).
In particular we look for \emph{linear dichotomies}, which are the patterns in which we can separate
the two colors with an hyperplane.

\begin{proposition}{Probability of finding a linear dichotomy}{prob-lin-dichotomy}
	The probability of finding a linear dichotomy in a set of $p$ points in $N$ dimensions is
	\begin{equation}
		P = \frac{1}{2^{p-1}} \sum_{k = 0}^{N -1} \binom{p-1}{k}
		\label{eq:prob-lin-dichotomy}
	\end{equation}
\end{proposition}

\begin{proof}
	Let $C(p, N)$ be our goal function which counts the number of linear dichotomies of $p$ points
	in $N$ dimensions.

	We know that the total number of dichotomies is $2^p$, which means that the probability of finding
	a linear dichotomy is
	\begin{equation}
		P = \frac{C(p, N)}{2^p}
	\end{equation}

	Suppose we know $C(p, N)$ and we add another point: we want to compute $C(p + 1, N)$.
	To do so we count the ways we can move the separating hyperplane without changing the output
	of the other $p$ points, but this is equivalent to counting the linear dichotomies where
	the separating hyperplane goes through a particular point.
	Since now we are constraint by passing through a point the number of dimensions
	is reduced to $N-1$, therefore we conclude that
	\begin{equation}
		C(p+1, N) = C(p, N) + C(p, N-1)
	\end{equation}

	Now we check that \cref{eq:prob-lin-dichotomy} is correct.
	Recall the way we construct Pascal's triangle: there holds that
	\begin{equation}
		\binom{p}{k} = \binom{p-1}{k} + \binom{p-1}{k-1}
	\end{equation}
	We know that $C(1, N) = 2$ and since $\binom{0}{0} = 1$ we have $C(1, N) = 2 \binom{0}{0}$.
	Then
	\begin{align}
		C(p+1, N) & = 2 \sum_{k = 1}^{N-1} \binom{p}{k}                                           \\
		          & = 2 \sum_{k = 1}^{N-1} \binom{p-1}{k} + 2 \sum_{k = 1}^{N-1} \binom{p-1}{k-1} \\
		          & = C(p, N) + 2 \sum_{k = 1}^{N-2} \binom{p-1}{k}                               \\
		          & = C(p, N) + C(p, N-1)
	\end{align}
	where we have shifted the sum by one in the third step.
\end{proof}

We can study what happens in the limit:
\begin{itemize}
	\item $p \leq N \implies P = 1$;
	\item $N \leq p \leq 2N \implies P \to 1$ as $N \to \infty$;
	\item $p = 2N \implies P = 0.5$
	\item $p > 2N \implies P \to 0$ as $N\to \infty$.
\end{itemize}
We notice that we have a very sharp bound at $p = 2N$ which separates the cases where $P = 0$ and
the ones where $P = 1$. We call $p = 2N$ the \emph{capacity} of the perceptron.

\subsubsection{Continuous perceptron}
If $y^\mu$ is a continuous variable, instead of being in $\{0, 1\}$, we need to define a cost
function $\phi$ such that

NEXT CLASS!

\end{document}
