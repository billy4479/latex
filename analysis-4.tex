\documentclass[12pt]{extarticle}

\setlength{\headheight}{16pt} % ??? we do what fancyhdr tells us to do  

\title{Analysis 4}
\author{Giacomo Ellero}
\date{a.y. 2024/2025}

\usepackage{preamble}

\renewcommand{\vec}[1]{\uvec{#1}}

\begin{document}
\oldfirstpage

The first part of the course will be about complex analysis and the second one will be about
differential geometry.

\section{Complex analysis}

Naively we can start by replacing $\R$ with $\C \approxeq \R^2$.
A lot things carry over from real analysis but \emph{complex differentiable} functions have
many more interesting properties.

\subsection{Constructing complex numers}

\subsubsection{The boring way}
The \say{boring} definition of complex number is a set $\C \supseteq \R$ where $\sqrt{-1}$ exists
and is defined as $i$ and $-i$.
Formally, this is the smallest \emph{field} where we include $\R$ and $i$.

A complex number $z$ can be written as
\begin{equation}
	z = a + bi \in \C \quad \text{with } a, b \in \R
\end{equation}

This representation is useful since it has a bunch of nice properties:
\begin{itemize}
	\item It is closed under multiplication;
	\item We define the complex conjugate $\overline{a + bi} = a - bi$ and
	      if we multiply $z$ with $\overline z$ we get a real number;
	\item We define a norm over $\C$ as $\norm{z} = z \overline z$;
	\item There exists an inverse, that is for each $z = a + bi \neq 0$
	      (i.e. either $a$ or $b$ $\neq 0$) we can define
	      \begin{equation}
		      z^{-1} = \frac{\overline z}{\norm{z}^2}
	      \end{equation}
\end{itemize}

Now we can formally say that $a + bi$ is the formal expression of a number in $\C$,
define the sum as the usual sum in $\R^2$ and the product as
\begin{equation}
	(a, b) \cdot (c, d) = (ac - bd, ad + bc)
\end{equation}
and we are able to prove that this is commutative and associative,
the proof is long and boring therefore we skip it.

\subsubsection{Geometric construction}

First we start with a simple lemma about isometries.

\begin{lemma}{Isometries that fix the origin are linear maps}{}
	Consider an isometry $f: \R^n \to \R^n$,
	that is $\abs{f(p) - f(q)} = \abs{p-q}$ for all $p, q \in \R^2$.
	If $f(0) = 0$, then $f$ is linear and we can write $f(v) = Av$
	where $A \in O(n)$ is an orthogonal $n\cross n$ matrix, that is $A^{-1} = A^T$.
\end{lemma}

In dimension $n = 2$ we can explicitly write a matrix $A$ which rotates the plane counterclockwise
by an angle $\theta$ as
\begin{equation}
	A = \begin{pmatrix}
		\cos \theta & -\sin \theta \\
		\sin \theta & \cos \theta
	\end{pmatrix}
\end{equation}
Rotation matrices form a subset of $O(n)$, namely those with $\det = 1$.

On the other hand, we define a \emph{homothety} or \emph{dilation} as the linear maps which
can be written as $f = \rho I$, with $\rho \geq 1$.
Now we can combine rotations and dilations to obtain a set of matrices of the form
\begin{align}
	\mathcal C & = \left\{ A = \rho I \cdot R_\theta \mid \rho \geq 0, \theta \in \R\right\} \\
	           & =  \left\{ A = \begin{pmatrix}
		                            \rho \cos \theta & - \rho \sin \theta \\
		                            \rho \sin \theta & \rho \cos \theta
	                            \end{pmatrix}
	\mid \rho \geq 0, \theta \in \R\right\}                                                  \\
	           & = \left\{ A = \begin{pmatrix}
		                           a & -b \\
		                           b & a
	                           \end{pmatrix}
	\mid a, b \in \R\right\}
\end{align}
We notice that $\mathcal C$ is a vector space of matrices of dimension 2 with bases
$I$ (the identity matrix) and $J = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$.

We can now draw a parallel between $\mathcal C$ and $\C$:
$I$ corresponds to $1$ and $J$ corresponds to $i$.

We can now write a formal definition using the matrices:
the sum and the product is the same as in matrices,
note that this set is closed under multiplication as $J ^2 = -I$.
Moreover we get for free that the product is associative (since it is that way with matrices)
and since $IJ = JI$ it is also commutative.

\subsection{Complex differentiability}
We will identify $\C \approxeq \R^2$ so that we get for free the definition of open and closed sets.

\begin{remark}{Product of complex numbers as a matrix}{}
	We can prove that identifying two complex numbers as pairs in $\R^2$ we get
	\begin{equation}
		(a, b) \cdot (c, d) = \begin{pmatrix} a & -b \\ b & a \end{pmatrix}
		\begin{pmatrix} c \\d \end{pmatrix}
	\end{equation}
\end{remark}

Now we can define complex differentiability:
\begin{definition}{Complex differentiability}{}
	Let $U \in \C$ open, $f: U \to \C$ be a function and $z_0 \in U$.
	We say that $f$ is complex differentiable at $z_0$ if
	\begin{equation}
		f(z) = f(z_0) + \alpha (z-z_0) + \o(\abs{z -z_0})
	\end{equation}
	for some $\alpha \in \C$ or equivalently
	\begin{equation}
		f'(z_0) \coloneq \lim_{z \to z_0} \frac{f(z) - f(z_0)}{z - z_0} = \alpha \in \C
	\end{equation}
	exists for some $\alpha$.
\end{definition}
Basically this definition tells us that $f$ is complex differentiable
if the first order taylor expansion exists and is linear.

\begin{definition}{Holomorfic function}{}
	A function $f: U \to \C$ is holomorfic if it is complex differentiable at every $z_0 \in U$.
\end{definition}

\begin{proposition}{Cauchy-Riemann equations}{}
	$f$ is complex differentiable at $z_0 \in U$ $\iff$
	$Df(z_0) = \begin{pmatrix} a & -b \\ b & a \end{pmatrix}$ for some $a, b \in \R$, in which case
	$f'(z_0) = a + bi$.

	Equivalently we can say that there exists $g, h: \R \to \R$ that satisfy
	\begin{equation}
		\begin{cases}
			\pdv{g}{x}()(z_0) = \pdv{h}{y}()(z_0) \\
			\pdv{h}{z}()(z_0) = -\pdv{g}{y}()(z_0)
		\end{cases}
	\end{equation}
	and
	\begin{equation}
		f(z) = f(x + yi) = g(x+yi) + h(x+yi)i
	\end{equation}
\end{proposition}

\begin{proof}
	This is actually quite easy, just see $f$ as a function from and to $\R^2$.
	Now we can compute the differential the usual way and if we want it to look like
	$\begin{pmatrix} a & -b \\ b & a \end{pmatrix}$
	we have to impose the two equations above.
\end{proof}

\subsection{Cauchy integral theorem}

In this section we will discuss and prove Cauchy integral theorem.
We will assume $U$ to be an open subset of $\C$.

\subsubsection{Integration over \texorpdfstring{$\C$}{C}}

\begin{definition}{Piecewise $C^1$ curve}{piecewise-c1}
	A function of the form $\gamma: [s, t]: U$ with $U$ and $s \leq t \in \R$
	is piecewise $C^1$ if for some suitable $s = t_0 < t_1 < \dots < t_N = t$
	the restriction $\eval{\gamma}_{[t_{j-1}, t_j]}$ is of class $C^1$ for all $j = 1, \dots, N$.

	Moreover, we say $\gamma$ is a \emph{loop} if $\gamma(s) = \gamma(t) = z_0$ and we call $z_0$ the
	\emph{basepoint} of the loop.
\end{definition}

\begin{definition}{Line integral}{line-integral}
	Given a continuous function $f: U \to \C$ and a piecewise $C^1$ curve $\gamma$ in $U$ we define the line
	integral of $f$ along $\gamma$ as
	\begin{equation}
		\int_\gamma f(z) \dd z \coloneq
		\sum_{j = 1}^N \int_{t_{j-1}}^{t_j} f(\gamma(\tau)) \cdot \gamma'(\tau) \dd{\tau}
	\end{equation}
\end{definition}

\begin{proposition}{Reparametrization independence}{reparametrization}
	Let $\gamma: U \to [s, t]$ and $\varphi: [s, t] \to [s', t']$ strictly increasing and both
	piecewise $C^1$.
	Let $\delta = \gamma \circ \varphi$, then, for all $f$ continuous,
	\begin{equation}
		\int_\gamma f(z) \dd z = \int_\delta f(z) \dd z
	\end{equation}
\end{proposition}
\begin{proof}
	This is the result of applying the change of variable formula with $z'= \varphi(z)$
	together with the chain rule.
\end{proof}

From now on we will often describe a loop by just its shape and it's orientation, without specifying
the parametrization or the basepoint, since, as we just saw, the integral does not depend on those.

\subsubsection{Cauchy integral theorem special form}

TODO: lec 3

\begin{theorem}{Cauchy integral theorem for rectangles}{cauchy-integral-rect}
\end{theorem}

\subsubsection{Local primitives}

\begin{definition}{Connected set}{connected-set}
	A non-empty set $V \subseteq \C$ is connected if
	$\nexists S_1, S_2 \in V$ open with $S_1, S_2 \neq \varnothing$,
	$V = S_1 \cup S_2$ and $S_1 \cap S_2 = \varnothing$.
\end{definition}

We note that a set is connected if and only if given $p, q \in V$ we can find $\gamma: [s, t] \in V$
continuous such that $\gamma(s) = p$ and $\gamma(t) = q$.
Moreover, if we have a family of connected sets $V_j$ such that they have a point in common, then
their union is still connected.

\begin{definition}{Local primitive}{local-primitive}
	Let $f: U \to \C$ holomorfic and $V \subseteq U$ open and connected.
	Then $F: V \to \C$ is a local primitive of $f$ on $V$ if
	\begin{equation}
		F' = f
	\end{equation}
\end{definition}

\begin{proposition}{Uniqueness of local primitives}{uniq-local-prim}
	Two local primitives $F_1, F_2: V \to \C$ of the same $f$, defined on the same $V \subseteq U$
	are unique up to a constant.
\end{proposition}

This is the same behavior we observe for primitives over $\R$.

\begin{proof}
	Let $H = F_2 - F_1$, we want to prove that $H$ is constant.
	Let $\lambda \in H(V)$ be one of the values assumed by $H$, then
	\begin{equation}
		V = \{ z \in V : H(z) = \lambda \} \cup \{ z \in V : H(z) \neq \lambda \}
	\end{equation}
	This is a disjoint union and we claim that the sets are both open, hence, since $V$ is connected,
	one of the two must be empty.
	We know that $\{ z \in V : H(z) = \lambda \} \neq \varnothing$ therefore necessarily
	$\{ z \in V : H(z) \neq \lambda \} = \varnothing$ and $\lambda$ is the only value assumed by $H$.

	Now, $\{ z \in V : H(z) \neq \lambda \}$ is open because it is the preimage of an open set
	($\C \setminus \lambda$) through a continuous function
	($H$ is holomorfic and therefore continuous).

	To show that $\{ z \in V : H(z) = \lambda \}$ is open we fix a point $z_0 \in V$ such that
	$H(z_0) = \lambda$. Since $V$ is open we can find $B_r(z_0) \in V$ and we will show that
	$H(B_r(z_0)) = \lambda$ which is enough to conclude that $\{ z \in V : H(z) = \lambda \}$ is open.
	To prove this we use the fact that $H' = f - f = 0$ and viewing it as a function from $\R^2$ to
	$\R^2$ we get that $DH(z) = 0$ everywhere.
	Now we consider $z_1 \in B_r(z_0)$ and we let $\gamma: [0,1] \to B_r(z_0)$ be the parametrization
	of the segment from $z_0$ to $z_1$:
	\begin{equation}
		\gamma(t) = H(z_0 + t(z_1 - z_0))
	\end{equation}
	and by the chain rule we have that
	\begin{equation}
		(H\circ \gamma)'(t) = DH(\gamma(t)) [\gamma'(t)] = 0
	\end{equation}
	Thus $H \circ \gamma: [0, 1] \to \R^2$ is a function with zero derivative at each point,
	which means that
	\begin{equation}
		H(z_1) = H(\gamma(1)) = H(\gamma(0)) = H(z_0) = \lambda
	\end{equation}
	as desired.
\end{proof}

\subsubsection{Integration over piecewise \texorpdfstring{$C^1$}{C1} curves}

\begin{proposition}{Fundamental theorem of calculus for curves}{fundamental-curves}
	Given a local primitive $F: V \to \C$ of $f$ and a piecewise $C^1$ curve $\gamma: [s, t] \to V$
	we have
	\begin{equation}
		\int_\gamma f(z) \dd z = F(\gamma(t)) - F(\gamma(s))
	\end{equation}
\end{proposition}

\begin{proof}
	Let $s = t_0 < t_1 < \dots < t_N = t$ such that the restriction $\eval{\gamma}_{[t_{j-1}, t_j]}$
	is $C^1$ for all $j = 1, \dots, N$.
	The usual proof of the chain rule gives us that
	\begin{equation}
		\dv{\tau} (F\circ \gamma)(\tau) = F'(\gamma(\tau)) \cdot \gamma'(\tau)
	\end{equation}
	where the lat $\cdot$ is the product of complex numbers.

	Since $F' = f$ we get
	\begin{align}
		\int_\gamma f(z) \dd z
		 & = \sum^N_{j = 1} \int^{t_j}_{t_{j-1}} f(\gamma(\tau)) \cdot \gamma'(\tau) \dd \tau  \\
		 & = \sum^N_{j = 1} \int^{t_j}_{t_{j-1}} F'(\gamma(\tau)) \cdot \gamma'(\tau) \dd \tau \\
		 & = \sum^N_{j = 1} \int^{t_j}_{t_{j-1}} (F\circ \gamma)'(\tau) \dd \tau               \\
		 & = \sum^N_{j = 1} [F \circ \gamma(t_j) - F \circ \gamma(t_{j-1})]                    \\
		 & = F \circ \gamma(t) - F \circ \gamma(s)
	\end{align}
	where we have used the fundamental theorem of calculus and
	the fact that the last sum is telescopic.
\end{proof}

\begin{corollary}{}{}
	If a local primitive $F: V \to \C$ exists and $\gamma: [s, t] \to V$ is a piecewise $C^1$ loop
	we have
	\begin{equation}
		\int_\gamma f(z) \dd z = 0
	\end{equation}
\end{corollary}
\begin{proof}
	Follows from the proposition since $F(\gamma(t)) = F(\gamma(s))$.
\end{proof}

\subsubsection{Local primitives in a ball}

We will now show that a local primitive always exists within open balls
(often called disks when over $\C$).

We will also introduce the following notation:
\begin{itemize}
	\item Given two curves $\eta$ and $\beta$ we indicate with $\eta * \beta$ the concatenation
	      of two curves such that
	      \begin{equation}
		      \int_{\eta * \beta} f(z) \dd z = \int_\eta f(z) \dd z + \int_\beta f(z) \dd z
	      \end{equation}
	\item We denote $[p, q]$ any parametrization of the segment that connects the two points $p, q$.
	      A standard parametrization is $\tau \mapsto p + \tau(q-p)$ with $\tau \in [0, 1]$.
\end{itemize}

\begin{proposition}{Existence of local primitives in a ball}{existence-local-primitive}
	Let $f: U \to \C$ holomorfic and $V = B_r(z_0) \subseteq U$ be an open ball.
	Then there exists a local primitive $F(z)$ for all $z \in V$.
\end{proposition}

\begin{proof}
	Let $z \in V$ and $a + bi = z - z_0$, consider two piecewise $C^1$ curves $\gamma, \delta$
	defined as
	\begin{align}
		\gamma & = [z_0, z_0 + bi] * [z_0 + bi, z] \\
		\delta & = [z_0, z_0 + a] * [z_0 + a, z]
	\end{align}
	that is, both curves start at $z_0$ and reach $z$ but
	$\gamma$ travels first vertically then horizontally while
	$\delta$ travels first horizontally and then vertically.

	Let $\alpha = \delta * \gamma^{-1}$ be the concatenation of $\delta$ and the \emph{reverse} of
	$\gamma$, that is, $\alpha$ parametrizes the boundary of the rectangle $R$ with vertices
	$\{z_0, z_0 + a, z_0 + bi, z\}$.
	This gives
	\begin{equation}
		\int_\delta f(w) \dd w - \int_\eta f(w) \dd w = \int_{\partial R} f(z) \dd z = 0
	\end{equation}
	by \cref{thm:cauchy-integral-rect} which we have already proven. Also note that all $w \in R$ are
	also included in $V$.
	Now, since their difference is $0$, we can let $F(z)$ be the common value of these two integrals:
	\begin{equation}
		F(z) = \int_\delta f(w) \dd w = \int_\eta f(w) \dd w
	\end{equation}

	Interpreting $F$ as a map of the form $F: \R^2 \to \C$, we have
	\begin{equation}
		\pdv{F}{x}()(z) = \lim_{\varepsilon \to 0} \frac{F(z + \varepsilon) - F(z)}{\varepsilon}
	\end{equation}
	where $\varepsilon \in \R$.
	Now if we consider the definition of $F$ using $\gamma$, we have
	\begin{align}
		F(z + \varepsilon) & = \int_{[z_0, z_0 + ib] * [z_0 + ib, z + \varepsilon]} f(w) \dd w          \\
		                   & = \int_{[z_0, z_0 + ib] * [z_0 + ib, z] * [z, z + \varepsilon]} f(w) \dd w \\
		                   & = \int_{[z_0, z_0 + ib] * [z_0 + ib, z]} f(w) \dd w
		+ \int_{[z, z + \varepsilon]} f(w) \dd w                                                        \\
		                   & = F(z) + \int_{[z, z + \varepsilon]} f(w) \dd w
	\end{align}
	then we choose $\tau \mapsto z + \tau \varepsilon$ as our parametrization of $[z, z + \varepsilon]$
	so that $\dv{\tau} (z + \tau \varepsilon) = \varepsilon$.
	We get
	\begin{equation}
		F(z + \varepsilon) - F(z) = \int_0^1 f(z + \tau \varepsilon) \cdot \varepsilon \dd \tau
		= \int_0^\varepsilon f(z + \sigma) \dd{\sigma}
	\end{equation}
	where we have used the change of variable $\sigma = \tau \varepsilon$.
	Then, since $f$ is continuous, (and???)
	\begin{equation}
		\pdv{F}{x}()(z) = \lim_{\varepsilon \to 0} \frac{1}{\varepsilon}
		\int_0^\varepsilon f(z + \sigma) \dd \sigma = f(z)
	\end{equation}

	Similarly for the imaginary part we have
	\begin{equation}
		\pdv{F}{y}()(z) = \lim_{\varepsilon \to 0} \frac{1}{\varepsilon}
		\int_0^\varepsilon f(z + i \sigma) \cdot i \dd \sigma = f(z) \cdot i
	\end{equation}
	where the additional $i$ comes from the fact that now the extra piece we are integrating over is
	$[z, z + i \varepsilon]$.

	We have just showed that $F$ admits a derivative at any point $z \in V$ and it is continuous,
	since $f$ is continuous, therefore, we can invoke the powers of multivariate calculus to say that
	$F$ is indeed differentiable and of class $C^1$.
	Moreover if we write $f(z) = \alpha + i \beta$ we have
	\begin{align}
		\pdv{F}{x}()(z) & = \alpha + i \beta  \\
		\pdv{F}{y}()(z) & = - \beta + i\alpha
	\end{align}
	which means that $F$ satisfies Cauchy-Riemann equations therefore $F$ is holomorfic
	and $F'(z) = f(z)$ for all $z \in V$.
\end{proof}

\subsubsection{Integration over continuous curves}

We now state a more general result for integrating over all continuous curves
(not just those which are piecewise $C^1$).

\begin{proposition}{Line integral over a continuous curve}{integral-continuous-curve}
	Let $f: U \to C$ holomorfic and $\gamma: [s, t] \to U$ continuous,
	then there exists a subdivision $s = t_0 < t_1 \dots < t_N = t$ such that
	for each $j = 1, \dots, N$ the image of $\gamma([t_{j-1}, t_j])$ is included in the domain
	$V_j \subseteq U$ of some local primitive $F_j: V \to \C$ of $f$.

	Moreover,
	\begin{equation}
		\int_\gamma f(z) \dd z = \sum_{j = 1}^N [F_j(\gamma(t_j)) - F_j(\gamma(t_{j-1}))]
	\end{equation}
	and this value does not depend on the choice of the subdivision, of $V_j$ or of $F_j$.
\end{proposition}

\begin{proof}
	If $U = \C$ we can just take $N = 1$ and $V_1$ be any ball included in the image of $\gamma$.

	Otherwise we let $\rho: \C \to \R$ be the function that assigns to each $z \in \C$ the distance to
	$\C \setminus U$, that is
	\begin{equation}
		\rho(z) = \inf_{w \in \C \setminus U} \abs{z - w}
	\end{equation}
	Moreover, by the triangle inequality we have that $\rho(z') \leq \abs{z-z'} + \rho(z)$
	and interchanging the roles of $z$ and $z'$ we get $\abs{\rho(z') - \rho(z)} \leq \abs{z' - z}$
	which means that $\rho$ is Lipschitz continuous.
	Furthermore, since $U$ is open, $\rho$ is positive on $U$, thus we can let
	\begin{equation}
		r = \min_{\tau \in [s, t]} \rho(\gamma(\tau))) > 0
	\end{equation}
	Now, since $\gamma$ is continuous and its domain is compact it is also uniformly continuous,
	thus, for some $N$ large enough, we have
	\begin{equation}
		\abs{\gamma(\tau) - \gamma(t_j)} < r \quad \text{for } \tau \in [t_{j-1}, t_j],
		\quad \text{with } t_j = s + j \frac{t-s}{N}
	\end{equation}
	which means we can take $\gamma(t_j)$ as the center of the ball $V_j$ with radius $r$, which is
	guaranteed to be included in $U$ since $r \leq \rho(\gamma(t_j))$,
	and invoking \cref{prop:existence-local-primitive} we obtain a local primitive $F_j$.

	TODO: idk about the second part of the proof, too hard for now.
\end{proof}

Moreover, this definition is coherent with the one over $C^1$ curves.

\subsubsection{Invariance under homotopies}

\begin{definition}{Homotopy}{homotopy}
	Given two curves $\gamma_0: [s, t] \to U$ and $\gamma_1: [s, t] \to U$ with the same endpoints
	$p = \gamma_0(s) = \gamma_1(s)$ and $q = \gamma_0(t) = \gamma_1(t)$, a homotopy is a
	\emph{continuous} function
	\begin{equation}
		\Gamma: [0,1]\cross[s, t] \to U
	\end{equation}
	such that $\Gamma(0, \cdot) = \gamma_0$, $\Gamma(1, \cdot) = \gamma_1$, and
	for all $\lambda \in [0,1]$ we have that $\Gamma(\lambda, \cdot)$
	is a curve with endpoints $p, q$.
	That is $\Gamma$	interpolates between the two curves.

	If such function exists we say that $\gamma_0, \gamma_1$ are \emph{homotopic}
	and we write $\gamma_0 \simeq \gamma_1$.
\end{definition}

\begin{theorem}{Invariance under homotopies}{invariance-homotopies}
	Let $f: U \to \C$ holomorfic and two curves such that $\gamma_0 \simeq \gamma_1$.
	Then
	\begin{equation}
		\int_{\gamma_0} f(z) \dd z = \int_{\gamma_1} f(z) \dd z
	\end{equation}
\end{theorem}

\begin{proof}
	Assume that $s = 0$ and $t = 1$ (which we can obtain by reparametrizing the curves which remain
	homotopic) and let $\Gamma: [0,1]\cross[0,1]$ be an homotopy between $\gamma_0$ and $\gamma_1$.

	With a similar argument as the beginning of the proof of \cref{prop:integral-continuous-curve},
	for some $N$ large enough, we can subdivide the domain $Q = [0, 1] \cross [0,1]$ into $N^2$
	smaller squares of sidelength $\frac{1}{N}$ such that, for each small square $Q'$, the image
	$\Gamma(Q')$ is included in some ball in $U$.

	Let $\eta_0$ be the curves that travels from $(0,0)$ to $(1,1)$ along the bottom and
	the right side of $Q$ and $\eta_1$ a curve with the same endpoints which travels along the
	left and top side instead.
	Now consider the integral $\int_{\Gamma \circ \eta_0} f(z) \dd z$: the right side (and the
	left side as well) contribute nothing to the integral since they are just moving from a curve to
	another.

	Let $Q'$ be the bottom-right square of the given subdivision and $\eta_1$ be the slightly
	modified curve which rather travels on the other two sides of $Q'$ (left and top instead of bottom
	right) and agrees with $\eta_0$ everywhere else.
	Now, since $Q'$ is included in a ball where a local primitive exists, we have
	\begin{equation}
		\int_{\Gamma \circ \eta_0} f(z) \dd z = \int_{\Gamma \circ \eta_1} f(z) \dd z
	\end{equation}

	Now consider the small square $Q''$ next to $Q'$ and perform the same modification to $\eta_1$
	obtaining the curve $\eta_2$. After repeating the steps $N^2$ times we obtain that
	\begin{equation}
		\int_{\gamma_0} f(z) \dd z = \int_{\Gamma \circ \eta_0} f(z) \dd z = \dots =
		\int_{\Gamma \circ \eta_{N^2}} f(z) \dd z = \int_{\gamma_1} f(z) \dd z
	\end{equation}
	as desired.
\end{proof}

\begin{theorem}{Cauchy integral theorem}{cauchy-integral}
	If $f : U \to \C$ is homotopic and $\gamma$ is a continuous loop which can be continuously
	deformed into a point (among loops in $U$) then
	\begin{equation}
		\int_\gamma f(z) \dd z = 0
	\end{equation}
\end{theorem}
\begin{proof}
	If $\gamma$ is homotopic to a constant curve $\gamma_1$ then, by \cref{thm:invariance-homotopies},
	\begin{equation}
		\int_\gamma f(z) \dd z = \int_{\gamma_1} f(z) \dd z =
		\int_s^t f(\gamma_1(t)) \cdot \gamma_1'(t) \dd z = 0
	\end{equation}
	since $\gamma_1' = 0$.
\end{proof}

\begin{theorem}{Cauchy integral formula}{cauchy-integral-formula}
	If $\overline B_r(z_0) \subseteq U$ for $f$ homotopic we have
	\begin{equation}
		f(z_0) = \frac{1}{2 \pi i} \int_{\partial B_r(z_0)} \frac{f(z)}{z - z_0} \dd z
	\end{equation}
	where $\partial B_r(z_0)$ is travelled counterclockwise.
\end{theorem}

Note that we can replace $\partial B_r(z_0)$ with any loop in $\C \setminus \{z_0\}$
homotopic to it.

\begin{proof}
	Not in the exam; see lecture 4.
\end{proof}

\section{Differential geometry}

\subsection{Inverse function theorem}

\begin{definition}{Diffeomorphism}{diffeomorphism}
	Let $U, V \subseteq \R^n$ both open. A function $\Phi : U \to V$ is a diffeomorphism if it is
	bijective, with $\Phi$ and $\Phi^{-1}$ both of class $C^1$.

	Moreover, we say that $\Phi$ is a diffeomorphism \emph{with its image} if $V = \Phi(U)$.
\end{definition}

\begin{theorem}{Inverse function theorem}{inverse-func}
	Let $f: U \to \R^n$ of class $C^1$ with $U \subseteq \R^n$ open.
	If the differential $Df(x_0)$ is invertible at a given point $x_0 \in U$, then there exists a
	smaller open set $U' \subseteq U$ such that $x_0 \in U'$, $f(U')$ is open and $f \eval_{U'}$ is a
	bijection from $U'$ to $f(U')$ with a $C^1$ inverse.

	In other words, $f\eval_{U'}$ is a diffeomorphism with its image.
\end{theorem}

This theorem essentially says that the Taylor expansion $T_1(x) = f(x_0) + (Df(x_0))(x-x_0)$ is
invertible if $f$ is also invertible near $x_0$.

\begin{proof}
	TODO: thm 9.1
\end{proof}

\begin{theorem}{Implicit function theorem}{implicit-func}
	Let $P \subset \R^k$ and $S, V \subseteq \R^n$, all open, and a function $f: P \cross S \to V$ of
	class $C^1$.
	If $D_s f(p_0, s_0)$ is invertible for some $(p_0, s_0) \in P \cross S$ then, letting
	$v_0 = f(p_0, s_0)$, there exists some smaller sets $P' \subseteq P$, $S'\subseteq S$, and
	$V' \subseteq V$ containing $p_0, s_0, v_0$ respectively such that, for $p \in P'$ and $v \in V'$
	the equation $f(p, s) = v$ has an unique solution $s \in S'$.
	Furthermore, the assignment $(p, v) \to s$ giving the solution is a $C^1$ function
	$P \cross V \to S$.
\end{theorem}

Basically this theorem gives us a way of finding a solution to a non-linear equation $f(p, s) = v$
for $s$ given a value $v$ and a parameter $p$.
Note that if we fix the parameter $p = p_0$ we are back to the same situation as the inverse
function theorem.

This is why we impose the condition that $D_s f$ (the differential of $f$ limited to the partial
derivatives in $s_1, \dots s_n$) is invertible at $(p_0, s_0)$.
Recall that the differential is a matrix, in this case $n \cross (k + n)$, and can be written as
\begin{equation}
	Df(p_0, s_0) = (D_p f(p_0, s_0) \mid D_s f(p_0, s_0))
\end{equation}

\begin{proof}
	TODO: thm 9.3
\end{proof}

\subsection{Definition of manifold}

Intuitively a $k$-dimensional manifold is a subset of $\R^n$, with $k \in \{1, \dots, n\}$, which
resembles an affine space of dimension $k$ close to any point in it. In other words, if we zoom in
to a point in the manifold we have something that looks like a straight line.

\begin{definition}{First definition of manifold}{manifold-1}
	A set $M \subseteq \R^n$ is a $k$-dimensional manifold if, for any $p \in M$, there exists an open
	set $U \subseteq \R^n$, such that $p \in U$, and a function $\Phi : U \to \R^n$ which is a diffeomorphism with
	its image, such that
	\begin{equation}
		\Phi(M \cap U) = [\R^k \cross \{0\}] \cap \Phi(U)
	\end{equation}
\end{definition}

This definition tells us that, within a set $U$, we can deform the portion of $M$ contained in $U$
top obtain a $k$-dimensional plane (or, more precisely, the portion of this plane included in
$\Phi(U)$).
The notation $\R^k \cross \{0\}$ indicates a $k$-dimensional subspace of $\R^n$: for example, if
$n = 3$ and $k = 2$ we would obtain the $xy$ plane \emph{in} $\R^3$, i.e. the set of points
$(x, y, 0) \in \R^3$.

\begin{definition}{Second definition of manifold}{manifold-2}
	A set $M \subseteq \R^n$ is a $k$-dimensional manifold if, for any $p \in M$, there exists an open
	set $U \subseteq \R^n$, such that $p \in U$, and a function $h: U \to \R^{n-k}$ such that $Dh(p)$
	is surjective (or, equivalently, has full rank) and
	\begin{equation}
		M \cap U = \{ x \in U : h(x) = 0 \}
	\end{equation}
\end{definition}

This definition is kind of complementary to the previous one: it says that $M$ is locally the zero
of a function with values in $\R^{n-k}$, which imposes $n-k$ constraints.

\begin{definition}{Third definition of manifold}{manifold-3}
	A set $M \subseteq \R^n$ is a $k$-dimensional manifold if, for any $p \in M$, there exists an open
	set $U \subseteq \R^n$, such that $p \in U$, an open set $V \subseteq \R^k$, and a
	\emph{parametrization} $\psi: V \to U$, that is $\psi$ of class $C^1$ with the following
	properties:
	\begin{itemize}
		\item $D \psi (x) $ is injective (or, equivalently, has full rank) for all $x \in V$;
		\item $\psi$ is a homeorphism with its image, that is $\psi$ as a map from $V \to \psi(V)$ is
		      bijective and has continuous inverse;
		\item $M \cap U = \psi(V)$.
	\end{itemize}
\end{definition}

Here we are told that around any point $p \in M$ we can write $M$ as the image of a parametrization
$\psi$. Note that it is enough to check the first condition at $\psi^{-1}(p)$ because $D\psi$ is
continuous.

\begin{definition}{Fourth definition of manifold}{manifold-4}
	A set $M \subseteq \R^n$ is a $k$-dimensional manifold if, for any $p \in M$, up to a permutation
	of the coordinates, there exists an open set $U \subseteq \R^n$ of the form $U = V \cross W$, with
	$V \subseteq \R^k$ and $W \subseteq \R^{n - k}$ both open, and a function $\tilde \psi: V \to W$
	of class $C^1$ such that $M \cap U$ is the graph of $\tilde \psi$, i.e.
	\begin{equation}
		M \cap U = \{ (x, \tilde \psi(x)) \mid x \in V\}
	\end{equation}
\end{definition}


\begin{theorem}{Equivalence of the definitions of manifold}{equivalence-def-manifold}
	The four definitions of manifold (\cref{def:manifold-1}, \cref{def:manifold-2},
	\cref{def:manifold-3}, and \cref{def:manifold-4}) are equivalent.
\end{theorem}
\begin{proof}
	TODO: thm 9.9
\end{proof}

\subsection{Spaces on manifolds}
\subsubsection{Tangent space}

\begin{definition}{Tangent space}{tangent-space}
	Let $M$ be a manifold and $p \in M$. The tangent space to $M$ at point $p$ is defined as
	\begin{equation}
		T_p M = \{ \gamma'(0) \mid \gamma: [0, \varepsilon] \to M \text{ of class } C^1 \text{ with }
		\gamma(0) = p \text{ and } \varepsilon > 0 \}
	\end{equation}
\end{definition}
In other words we consider all the possible initial speeds of the curves starting at $p$ and
travelling along $M$.

\begin{proposition}{Tangent space is linear}{tangent-space-linear}
	The tangent space $T_p M$ of a $k$-dimensional manifold $M$ at $p$ is a $k$-differential linear
	subspace of $\R^n$ for any $p \in M$.
\end{proposition}

\begin{proof}
	TODO: prop 10.2
\end{proof}

\subsubsection{Tangent cone}

\begin{definition}{Tangent cone}{tangent-cone}
	Given a set $S \subseteq \R^n$ and $p \in S$, we define the tangent cone of $S$ at $p$ as
	\begin{equation}
		T_p S = \left\{ \lambda v \mid \lambda \geq 0,
		v = \lim_{i \to \infty} \frac{p^{(i)} - p}{\abs{p^{(i)} - p}}\right\}
	\end{equation}
	where we consider any sequence $p^{(i)} \in S \setminus \{p\}$ such that $p^{(i)} \to p$ and
	$\frac{p^{(i)} - p}{\abs{p^{(i)} - p}}$ also converges.

	If $p$ is isolated in $S$ we let $T_p S = \varnothing$.
\end{definition}

In other words, we consider all possible limit directions of approaching $p$ within $S$ and we
consider all non-negative multiples of them.

\begin{proposition}{Tangent cone are necessary for manifold}{cone-necessary-manifold}
	If $S$ is a $k$-dimensional manifold, then $T_p S$ is a $k$-dimensional vector space.
\end{proposition}

Note that this condition is not sufficient, counterexample in remark 10.6.
Moreover, if $S$ is convex, the tangent cone ends up being equivalent to the closure of the one we
defined in advanced programming.

\begin{proof}
	TODO: prop 10.5
\end{proof}

\subsection{Smooth maps between manifolds}

For us smooth means $C^1$ although some time it may be more strict.
We want to define what it means for a function $f: M \to N$ to be smooth, where $M \subseteq \R^m$
and $N \subseteq \R^n$ are both manifolds.

\begin{definition}{Smooth maps to a manifold}{}
	Given a set $U \subseteq \R^m$ and a function $f: U \to N$ we say that $f$ is smooth if
	$f: U \to \R^n$ is of class $C^1$ in the usual way, that is, differentiable with continuous
	differential.
\end{definition}

\begin{definition}{Smooth maps from a manifold}{smooth-map-from-manifold}
	Let $f: M \to \R^n$. $f$ is smooth if for all parametrizations $\psi: V \to M$
	(as in \cref{def:manifold-3}) defined on an open set $V \subseteq \R^k$ so that the composition
	\begin{equation}
		f \circ \psi: V \to \R^k
	\end{equation}
	is of class $C^1$.
\end{definition}

We would need to check this for infinitely many parametrizations, however, luckily, it is usually
enough to check finitely many of them.

\begin{proposition}{}{}
	Let $\psi^{(i)}: V^{(i)} \to M$ be a family of parametrizations for $i \in I$ so that
	$M = \bigcup_{i \in I} \psi^{(i)}(V^{(i)})$.
	Then a map $f: M \to \R^n$ is smooth if and only if each composition $f \circ \psi^{(i)}$ is
	$C^1$.
\end{proposition}

\begin{proof}
	skip.
\end{proof}

\begin{proposition}{}{}
	A function $f: M \to \R^n$ is of class $C^1$ if and only if, for any $p \in M$, there exists an
	open set $U$ such that $p \in U$ and $\tilde f: U \to \R^n$ of class $C^1$ such that
	\begin{equation}
		f |_{M \cap U} = \tilde f |_{M \cap U}
	\end{equation}
\end{proposition}

\begin{proof}
	TODO: prop 12.6
\end{proof}

\begin{definition}{Smooth map between manifolds}{}
	A function $f: M \to N$ is smooth if it is smooth according to \cref{def:smooth-map-from-manifold}
	when viewed as a map $f: M \to \R^n$.
\end{definition}

\begin{definition}{Diffeomorphism between manifolds}{}
	A function $f: M \to N$ is a diffeomorphism it is bijective and $f$ and $f^{-1}$ are both smooth.
\end{definition}

\subsubsection{Differential of smooth functions between manifolds}

\begin{definition}{Differential of a smooth function between manifolds}{differential-smooth}
	Given a $C^1$ function $f: M \to N$ between two manifolds $M, N$, for any point $p \in M$ we
	define the differential of $f$ at $p$ as the \emph{linear} map
	\begin{equation}
		Df(p): T_p M \to T_{f(p)} N
	\end{equation}
	defined as
	\begin{equation}
		Df(p)[\gamma'(0)] = (f \circ \gamma)' (0)
	\end{equation}
	for any $\gamma: [0, \varepsilon] \to M$ of class $C^1$ with $\gamma(0) = p$.
\end{definition}

\begin{proposition}{Differential is well defined}{}
	The differential is well-defined (i.e. for two different $\gamma_1, \gamma_2$ we would obtain the
	same result) and linear.
	Moreover, given any $p \in M$ and a local $C^1$ extension $\tilde f: U \to \R^n$ near $p$, we have
	\begin{equation}
		Df(p)[v] = D\tilde f (p)[v]
	\end{equation}
	for any $v \in T_p M$.
\end{proposition}
\begin{proof}
	TODO: prop 13.3
\end{proof}

\subsection{Area}

Let $k \in \{1, \dots, k\}$ be an integer.
Then there exists a \say{good} way to we define the $k$-dimensional area of any $S \subseteq \R^n$.

\begin{definition}{Area ($k$-dimensional Hausdorff measure)}{area}
	Given $\delta > 0$, we cover $S$ with an at-most countable collection of sets $E_i \subseteq \R^n$
	with $\mathrm{diam}(E_i) \leq \delta$, so that $S \subseteq \bigcup_{i \in I} E_i$.
	Then the area is defined as
	\begin{equation}
		\mathcal H_\delta^k (S) = \inf \sum_{i \in I} \frac{\omega_k}{2^k} \mathrm{diam}(E_i)^k
	\end{equation}
	where we take the infimum over all the possible choices of sequences $(E_i)_{i \in I}$ and
	$\omega_k$ is the area of the $k$-dimensional unit ball.

	We can also write
	\begin{equation}
		\mathcal H^k (S) = \lim_{\delta \to 0} \mathcal H_\delta^k(S)
	\end{equation}
\end{definition}

We notice the following properties of the area:
\begin{itemize}
	\item If $k = 0$ then $\mathcal H^0(S) = \# S$ (the cardinality of $S$ if $S$ is finite and
	      infinity otherwise) $\forall S$.

	\item $\mathcal H^k (\varnothing) = 0$.

	\item $\mathcal H^k(rS + a) = r^k \mathcal H^k (S)$.

	\item For a finite or countable sequence $(S_j)_{j \in J}$ of Borel sets we have
	      \begin{equation}
		      \mathcal H^k\left(\bigcup_{j \in J} S_j\right) \leq \sum_{j \in J} \mathcal H^k (S_j)
	      \end{equation}

	\item If $S$ includes in a non-empty $k$-dimensional manifold, then $\mathcal H^k(S) > 0$.
	\item If $S$ is included in a compact $k$-dimensional manifold, then $\mathcal H^k(S) < +\infty$.

	\item If $\mathcal H^k(S) >0$ and $\ell < k$, then $\mathcal H^\ell(S) = + \infty$.
	\item If $\mathcal H^k (S) < +\infty$ (or $S$ is included in a countable union of $k$-dimensional
	      manifolds) and $\ell > k$, then $\mathcal H^\ell (S) = 0$.
\end{itemize}

\begin{definition}{Jacobian}{jacobian}
	Let $M \subseteq \R^r, N \subseteq \R^\ell$ be two $k$-dimensional manifolds.
	Given $f: M \to N$ of class $C^1$ and $x \in M$, the Jacobian of $f$ at $x$ is defined as
	\begin{equation}
		J_f(x) = \abs{\det(A)}
	\end{equation}
	where $A$ is the $k\cross k$ matrix representing the differential $Df(x): T_x M \to T_{f(x)} N$.
\end{definition}

\begin{proposition}{}{jacobian-sqrt-det}
	Consider the differential as a map to $R^s$: $Df(x): T_x M \to \R^s$.
	Let $\{v_1, \dots, v_k\}$ be an orthonormal basis of $T_x M$ and consider
	\begin{equation}
		B = \begin{pmatrix}
			Df(x) [v_1] & \dots & Df(x) [v_k]
		\end{pmatrix}
	\end{equation}

	Then $J_f(x) = \sqrt{\det(B^T B)}$.
\end{proposition}

\begin{proof}
	Take an orthonormal basis $\{w_1, \dots, w_k \}$ of $T_{f(x)} N$ so that we can define the $A$ in
	the original definition.
	Consider $Df(x)$ as the composition of two maps so that $T_x M \to T_{f(x)} N \to \R^s$.
	Note that the last map is just an inclusion, therefore we just need a change of basis from
	$\{w_1, \dots, w_k\}$ to the canonical basis, that is
	\begin{equation}
		B =
		\begin{pmatrix}
			w_1 & \dots & w_k
		\end{pmatrix} A
	\end{equation}

	Then
	\begin{equation}
		B^T B = A^T \begin{pmatrix} w_1^T \\ \dots \\ w_k^T \end{pmatrix}
		\begin{pmatrix} w_1 & \dots & w_k \end{pmatrix} A
		= A^T \mathds 1_{k \cross k} = A^T A
	\end{equation}
	which means $\det(B^T B) = \det(A^T A) = \det(A)^2$.
\end{proof}

\begin{theorem}{Area formula}{area-formula}
	Let $f: M \to N$ be a $C^1$ map between two $k$-dimensional manifolds and
	$h : N \to [0, + \infty]$.

	If $f$ is injective then
	\begin{equation}
		\int_{f(S)} h(y) \dd \mathcal H^k(y) = \int_S (h \circ f)(x) J_f(x) \dd \mathcal H^k(x)
	\end{equation}

	If $f$ is not injective then
	\begin{equation}
		\int_{f(S)} h(y) \cdot \nu(y) \dd \mathcal H^k(y) = \int_S (h \circ f)(x) J_f(x) \dd \mathcal H^k(x)
	\end{equation}
	where $\nu(y) = \# \{ x \in S : f(x) = y\}$
\end{theorem}

\begin{proof}
	TODO: lec 14
\end{proof}

\begin{corollary}{}{}
	If $h = 1$ and we have
	\begin{equation}
		\mathcal H^k(f(S)) = \int_S J_f(x) \dd \mathcal H^k(x)
	\end{equation}
	for $f$ injective
\end{corollary}

\begin{remark}{}{jacobian-on-differentials}
	If $\{v_1, \dots, v_k\}$ is an orthonormal basis of $T_xM$ and $Df(x)[v_1], \dots, Df(x)[v_k]$ are
	orthogonal to each other, then
	\begin{equation}
		J_f(x) = \abs{Df(x)[v_1]} \dots \abs{Df(x)[v_k]}
	\end{equation}
\end{remark}

\begin{proof}
	Let $w_i = \frac{Df(x)[v_i]}{\abs{Df(x)[v_i]}}$.
	Then
	\begin{equation}
		A = \begin{pmatrix}
			\abs{Df(x)[v_1]} &        & 0                \\
			                 & \ddots &                  \\
			0                &        & \abs{Df(x)[v_k]}
		\end{pmatrix}
	\end{equation}
	TODO: what
\end{proof}

\begin{example}{Surface of revolution}{}
	Let $g: I \to (0, +\infty)$ of class $C^1$ where $I$ is an open interval on $\R$.
	In $\R^3$, consider the manifold
	\begin{equation}
		N = \{ (x,y,z) \in \R^3 \mid x \in I, \abs{(y, z)} = g(x) \}
	\end{equation}

	Compute the area.
\end{example}

\begin{proof}[Solution]
	First we want to find a parametrization of $N$.
	We choose the parametrization $f: I \cross (0, 2 \pi) \to N$ defined as
	\begin{equation}
		f(s, \theta) = (s, g(s) \cos \theta, g(s) \sin \theta)
	\end{equation}

	We notice that $f$ is indeed injective and almost surjective, as it misses the line where
	$\theta = 0$. The image of $f$ is
	\begin{equation}
		f(I \cross (0, 2\pi) =
		N \setminus \underbrace{\{ (s, g(s), 0) \mid s \in I \}}_{\mathcal H^2(\cdot) = 0}
	\end{equation}
	but since a line has area zero when $k = 2$ we have that
	\begin{equation}
		\mathcal H^2(N) = \mathcal H^2(f(I \cross (0, 2\pi))
	\end{equation}

	We can now use the area formula:
	\begin{align}
		\mathcal H^2(f(I \cross (0, 2\pi))) & = \int_{I \cross (0, 2\pi)} J_f(s, \theta) \dd \mathcal
		H^2(s, \theta)                                                                                    \\
		                                    & = \int_{I \cross (0, 2\pi)} J_f(s, \theta) \dd s \dd \theta
	\end{align}
	where we can change the differential since ??? like the thing is open and they have the same
	lebesgue measure?

	Now we need to find the jacobian: we have that the tangent plane to $I \cross (0, 2\pi)$ is
	$T_{(s, \theta)}(I\cross (0,2\pi)) = \R^2$, therefore we can take the canonical basis
	$\{ e_1, e_2 \}$ as our orthonormal basis.

	Now either we find an orthonormal basis of $T_{f(s, \theta)} N$ or we use
	\cref{prop:jacobian-sqrt-det}. However in this case we are lucky since we will be able to apply
	\cref{rk:jacobian-on-differentials}:
	\begin{align}
		Df(s, \theta)[e_1] = \pdv{f}{s}{}(s, \theta)      & = (1, g'(s) \cos \theta, g'(s) \sin \theta) \\
		Df(s, \theta)[e_2] = \pdv{f}{\theta}{}(s, \theta) & = (0, -g(s) \sin \theta, g(s) \cos \theta)
	\end{align}
	which are orthogonal.

	Then the jacobian is
	\begin{align}
		J_s(s, \theta)           & = \abs{Df(s, \theta)[e_1]} \abs{Df(s, \theta)[e_2]}           \\
		                         & = \sqrt{1+g'(s)^2} g(s)                                       \\
		\implies \mathcal H^2(N) & = \int_0^{2\pi} \int_I \sqrt{1+g'(s)^2} g(s) \dd s \dd \theta \\
		                         & = 2 \pi \int_I \sqrt{1+g'(s)^2} g(s) \dd s
	\end{align}
	since $\theta$ is not in the integrand.
\end{proof}

\begin{remark}{Cauchy-Binet formula}{cauchy-binet-formula}
	\begin{equation}
		\det(B^T B) = \sum_{C \ k\cross k \text{ minor of } B} \abs{\det (C)}^2
	\end{equation}
\end{remark}
This is particularly useful if we choose to apply \cref{prop:jacobian-sqrt-det}.

\begin{example}{Area of $n$ dimensional sphere}{}
	Compute the area of any unit sphere $S^n$ inductively on $n$.
\end{example}
\begin{proof}[Solution]
	Recall that $S^n \subseteq \R^{n+1}$.
	We can parametrize most spheres $S^n$ by $(0, \pi)\cross S^{n-1}$
	(assume $n \geq 2$), where we call our extra parameter $\theta \in (0, \pi)$.

	Notice that the set of points in $S^n$ forming an angle $\theta$ with $e_{n+1}$ is a copy of the
	$S^{n-1}$ sphere with radius $\sin \theta$.
	Then we choose as our parametrization to be $f: (0, \pi) \cross S^{n-1} \to S^n$ defined as
	\begin{equation}
		f(\theta, x) = ((\sin \theta) x, \cos \theta)
	\end{equation}
	Note that $f$ excludes $\{\pm e_{n+1}\}$ but these points have no dimension so we are fine.

	We have
	\begin{equation}
		\mathcal H^n(S^n) = \int_{(0, \pi) \cross S^{n-1}} J_f(\theta, x) \dd \mathcal H^n (\theta, x)
	\end{equation}

	Given $(\theta, x) \in (0, \pi) \cross S^{n-1}$ we select an orthonormal basis
	$u_1, \dots, u_{n-1}$ of $T_x S^{n-1}$. Then, the tangent space of the product of two spaces is
	the product of the tangent spaces and, since the tangent space of $(0, \pi)$ is $\R$, we get
	\begin{equation}
		T_{(\theta, x)}((0, \pi) \cross S^{n-1}) = \R \cross T_x S^{n-1}
	\end{equation}
	so that the vectors $(1, 0, \dots, 0), (0, u_1), \dots (0, u_{n-1})$ form an orthonormal basis
	of $T_{(\theta, x)}$.

	Let us compute the differentials:
	\begin{align}
		Df(\theta, x)[(1, 0, \dots, 0)] & = ((\cos \theta) x, -\sin \theta) \\
		Df(\theta, x)[(0, u_j)]         & = ((\sin\theta) u_j, 0)
	\end{align}
	where we get the $u_j$ since we have to take into account the speed of the parametrization.

	(Rigorous reason on why we have $u_j$). If we consider
	$\gamma: (-\varepsilon, \varepsilon) \to (0, \pi) \cross S^{n-1}$ such that
	$\gamma(0) = (\theta, x)$ and $\gamma = (\theta, \delta)$ where $\theta$ is constant and $\delta$
	is another curve in $S^{n-1}$, in this way $\gamma' = (0, \delta')$, where $\delta'(0) = u_j$.
	Then, the derivative of the composition is
	\begin{align}
		(f \circ \gamma)' (0) & = \eval{\dv{f}{t}{} (\theta, \delta(t))}_{t = 0}                    \\
		                      & = \eval{\dv{f}{t}{} ((\sin \theta) \delta(t), \cos \theta)}_{t = 0} \\
		                      & = ((\sin \theta) \delta'(0), 0)
	\end{align}

	We obtain that the differentials are orthogonal, hence we can write the jacobian using
	\cref{rk:jacobian-on-differentials}:
	\begin{align}
		J_f(\theta, x) & = \abs{((\cos \theta) x, -\sin)} \cdot \abs{(\sin \theta)u_1, 0} \cdot \dots
		\cdot \abs{(\sin \theta) u_{n-1}, 0}                                                          \\
		               & = 1 \cdot \sin \theta \cdot \dots \cdot \sin \theta                          \\
		               & = (\sin \theta)^{n-1}
	\end{align}
	and the area is computed as
	\begin{align}
		\mathcal H^n(S^n) & = \int_0^\pi (\sin \theta)^{n-1} \int_{S^{n-1}} \dd \mathcal H^{n-1}(x) \dd \theta \\
		                  & = \mathcal H^{n-1}(S^{n-1}) \int_0^\pi (\sin \theta)^{n-1} \dd \theta
	\end{align}

	There is a smarter way to do this, which is in the lecture notes.
\end{proof}

\subsection{Orientation}

\begin{definition}{Orientation}{orientation}
	Let $V$ be a vector space of finite dimension.
	Let us define an equivalence relation $\sim$ between bases of $V$: two bases $\mathcal A$ and
	$\mathcal B$ are equivalent if the change of basis matrix $P$ between the two has $\det(P) > 0$.

	Each equivalence class of $\sim$ is called orientation.
\end{definition}

\begin{proposition}{}{}
	$\sim$ is an equivalence relation. Moreover, there are exactly 2 equivalence classes.
\end{proposition}

\begin{proof}
	$\sim$ is reflective since if $\mathcal A = \mathcal B$ we have that $P = \mathds 1$ which has
	positive determinant.

	Inverting the roles of the two bases yields $P^{-1}$ instead, which still has the same sign of the
	determinant, therefore $\sim$ is also symmetric.

	Consider three bases $\mathcal A, \mathcal B, \mathcal C$ so that $P$ goes from $\mathcal A$ to
	$\mathcal B$ and $Q$ goes from $\mathcal B$ to $\mathcal C$. The change of basis from $\mathcal A$
	to $\mathcal C$ is given by $PQ$.
	If $\det (P), \det (Q) > 0$ then also $\det(PQ) = \det(P) \det(Q) > 0$ and we have transitivity.

	Let $\mathcal B = \{ v_1, \dots, v_n\}$ and consider $\mathcal B' = \{-v_1, v_2, \dots, v_k\}$,
	then $\det(P) < 0$ therefore we have at least two equivalent classes.

	TODO: prove that there are at most 2
\end{proof}

TODO: parametrization map?? lec 15

\begin{definition}{Orientation of manifold}{orientation-manifold}
	An orientation of an $n$-dimensional manifold $M$ is a choice of an orientation $\tau(p)$ of
	$T_pM$ which is continuous over $p$.

	If a manifold admits an orientation we say that the manifold is orientable and, once we choose
	one, we call the pair of manifold and orientation the oriented manifold.
\end{definition}

In this context continuous means that whenever we take a parametrization
$\psi: V \subseteq \R^n \to M$, with $V$ connected, so that $\forall x \in V$ the differential
$D\psi (x): \R^n \to T_{\psi(x)} M$ is an invertible linear map, then
$(D\psi (x)^{-1})_*(\tau(\psi(x)))$ is constant.

\begin{proposition}{}{}
	If $M = \{ x \in \R^N \mid h(x) = 0 \}$ (which is $n$ dimensional), where $h: \R^N \to \R^{N-n}$
	is $C^1$ and $Dh(x)$ has full rank for all $x \in M$, then $M$ is oriented:
	given $x \in M$ we have that
	$T_x M = \mathrm{span} \{\grad h_1(x), \dots, \grad h_{N-n}(x)\}^\perp$ and we complete it to a
	basis of $\R^N$ using $v_1, \dots, v_n \in T_xM$ so that the result is equivalent $\sim$ to the
	canonical basis of $\R^N$.
	Then we can take $\tau(x)$ to be the equivalence class of $(v_1, \dots, v_n)$ and we obtain a
	well-defined and continuous orientation.
\end{proposition}

\begin{definition}{Parallelizable manifold}{parallelizable-manifold}
	We say that a manifold $M$ is parallelizable if we can choose a basis of $T_x M$ in a continuous
	way.
	This means that $(v_1(x), \dots, v_n(x))$ is a basis of $T_x M$ and for all
	$i \in \{1, \dots, n\}$ we have that $v_i: M \to \R^N$ is continuous.
\end{definition}

Being parallelizable is a very strong result: even $S^n$ is not always parallelizable. Notably only
$S^n$ is parallelizable only for $n = 0, 1, 3, 7$.

\subsection{Multilinear \texorpdfstring{$k$}{k}-forms}

\begin{definition}{Multilinear alternating $k$-form}{}
	A function $\alpha: \underbrace{V \cross \dots \cross V}_k \to \R$ is a multilinear alternating
	$k$-form if we freeze all entries but one it is linear in the remaining one and if we flip two
	entries we get the opposite result.
\end{definition}

Note that the set of these forms is a vector space denoted by $\Lambda^k V$. Note that when $k = 1$
we obtain the dual of $V$.

\begin{definition}{Wedge product}{wedge-product}
	Let $\alpha \in \Lambda^k V$ and $\beta \in \Lambda^\ell V$.
	The wedge (or exterior) product of $\alpha$ and $\beta$ is
	\begin{equation}
		\alpha \land \beta (v_1, \dots, v_{k+\ell}) = \sum_{\sigma \in S_{k+\ell}}
		\frac{\mathrm{sgn}(\sigma)}{k! \ell!} \alpha(v_{\sigma(1)}, \dots, v_{\sigma(k)})
		\beta(v_{\sigma(k+1)}, \dots, v_{\sigma(k+\ell)})
	\end{equation}
	where $\sigma$ is a permutation of the indices and $\mathrm{sgn}$ is the sign of the permutation,
	i.e. $(-1)^q$ where $q$ is the number of flips which we need to create the permutation.
\end{definition}

The wedge product maintains multilinearity and the alternating property.

TODO: is there more of lec 15?

\begin{proposition}{}{}
	Given a basis $e_1, \dots, e_n$ of $V$,
	\begin{equation}
		e_{i_1}^* \land \dots \land e_{i_n}^*
	\end{equation}
	where $(i_j)_{j \in \{ 1, \dots, k \}}$ is a combination of the indices from $1$ to $n$ and
	$e_i^*$ are the elements of the dual basis, as $i_k$ vary, form a basis of $\Lambda^k V$ for
	$0<k\leq n$.
\end{proposition}

\begin{proof}
	TODO: lec 16 ??
\end{proof}

\begin{lemma}{}{}
	In the setting of the previous proposition if $k > n$, then $\Lambda^k V = \{ 0 \}$.
\end{lemma}
\begin{proof}
	Let $\alpha \in \Lambda^k V$ and we claim that for every $k$ vectors $v_1, \dots v_k$
	$\alpha(v_1, \dots v_k) = 0$.

	Write each vector as $v_i = \sum_j a_{ij} e_j$, therefore we have
	\begin{equation}
		\alpha(v_1, \dots v_k) = \alpha\left(\sum_j a_{1j} e_j, \dots, \sum_j a_{kj} e_j \right)
	\end{equation}
	and once at the time take them out of $\alpha$. In this way we obtain a huge linear combination of
	terms which look like
	\begin{equation}
		\alpha(e_{j_1}, \dots, e_{j_k})
	\end{equation}
	however, necessarily we have a repetition, since there are only $n$ elements in the basis and
	$k > n$, therefore the result of each one of these combination is zero since if we have a
	repetition in an alternating $k$-form the result is always zero.
\end{proof}

\begin{definition}{Differential $k$-form}{}
	Let $U \subseteq \R^n$ open. A differential $k$-form of class $C^1$ is a function
	$\alpha: U \to \Lambda^k \R^n$ of class $C^1$.

	Equivalently, $\alpha(x) \in \Lambda^k \R^n$ and
	\begin{equation}
		\alpha(x) = \sum_{1 \leq i_i \leq i_k \leq n} a_{i_1, \dots, i_k}(x)
		\dd x_{i_1} \land \dots \dd x_{i_k}
	\end{equation}
	(where $\dd x_i = e_i^*$ w.r.t. the canonical basis) we have that each coefficient
	$a_{i_1, \dots, i_k}(x)$ is $C^1$.
\end{definition}

\begin{definition}{Pullback}{pullback}
	Let $F: U \subseteq \R^m \to V \subseteq \R^n$ with $U, V$ open of class $C^2$,
	and $\alpha$ a differentiable $k$-form on $V$.
	The pullback of $\alpha$ (by $F$) is $F^* \alpha$, a $k$-form on $U$ given by
	\begin{equation}
		F^* \alpha (x) [v_1, \dots, v_k] = \alpha(F(x))[DF(x)[v_1], \dots, DF(x)[v_k]]
	\end{equation}
\end{definition}

\begin{definition}{Exterior differential}{}
	If $\alpha$ is a differential $k$-form on $U \subseteq \R^n$, writing it as
	\begin{equation}
		\alpha(x) = \sum a_{i_1, \dots, i_k} \dd x_{i_1} \land \dots \land \dd x_{i_k}
	\end{equation}
	its exterior differential is the differential $(k+1)$-form defined as
	\begin{equation}
		\dd \alpha(x) = \sum_{1 \leq i_i \leq i_k \leq n} \left( \sum_j^n \pdv{a_{i_1, \dots, i_k}(x)}{x_j} \dd x_j \right)
		\land \dd x_{i_1} \land \dots \land \dd x_{i_k}
	\end{equation}
\end{definition}

Note that if $k = 1$ we have that $\Lambda^1 V = V^*$ and, for a differential $0$-form $f: U \to \R$
of class $C^1$, its exterior differential is $\dd f(x) = Df(x)$.

\begin{proposition}{}{}
	If $\alpha$ is a differential $k$-form and $\beta$ a differential $\ell$-form, then
	\begin{equation}
		\dd(\alpha \land \beta) \dd \alpha \land \beta + (-1)^k \alpha \land \dd \beta
	\end{equation}
\end{proposition}

\begin{proof}
	No proof.
\end{proof}

\begin{proposition}{}{}
	If $\alpha$ is a $C^2$ differential $k$-form, then $\dd(\dd \alpha) = 0$.
\end{proposition}
\begin{proof}
	TODO: lec 16

	proof by example on $k=1, n=3$.
\end{proof}

\begin{proposition}{}{}
	Let $F: U \to V$ of class $C^2$ and $\alpha$ a differential $k$-form on $V$.
	Then $\dd(F^* \alpha) = F^*(\dd \alpha)$.
\end{proposition}

\end{document}
