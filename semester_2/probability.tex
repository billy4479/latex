\documentclass[10pt]{extarticle}
\title{Probability Notes}
\author{Giacomo Ellero}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{dirtytalk}
\usepackage{parskip}
\usepackage{mathrsfs}
\usepackage[many]{tcolorbox}
\usepackage{xparse}
\usepackage[a4paper,margin=1.5cm]{geometry}
\usepackage{bookmark}
\usepackage{cancel}

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\newenvironment{absolutelynopagebreak}
  {\par\nobreak\vfil\penalty0\vfilneg
   \vtop\bgroup}
  {\par\xdef\tpd{\the\prevdepth}\egroup
   \prevdepth=\tpd}

\newtcolorbox{examplebox}[1]{colback=green!5!white,colframe=green!40!black,title={#1},fonttitle=\bfseries,parbox=false}
\newtcolorbox{notebox}[1]{colback=blue!5!white,colframe=blue!40!black,title={Note: #1},fonttitle=\bfseries,parbox=false}
\newtcolorbox{bluebox}[1]{colback=blue!5!white,colframe=blue!40!black,title={#1},fonttitle=\bfseries,parbox=false}
\newtcolorbox{warningbox}[1]{colback=orange!5!white,colframe=orange!90!black,title={Warning: #1},fonttitle=\bfseries,parbox=false}

\begin{document}

\maketitle
\tableofcontents
\clearpage

\section{Class of 05/02/2024}

\subsection{Interpretations of probability}

Probability has had many interpretations over the years, here's some of them:
\begin{enumerate}
    \item \textbf{Classical interpretation}: Games of chances, all the outcomes are equally likely.
          We can calculate probability as the number of favorable outcomes over the total.
          This only works on finite spaces with perfect symmetries.
    \item \textbf{Frequency interpretation}: This is the probability calculated in the long run by repeating the experiment infinitely many times.
          This only works if the experiment is repeatable. What if we cannot take the limit?
    \item \textbf{Subjective interpretation}: How much are people willing to bet on something to happen?
\end{enumerate}

\subsection{Sample spaces and outcomes}

In an \textbf{experiment} we call \textbf{sample space} $\Omega$ the set of all possible outcomes,
while a singular outcome is denoted as $\omega$.
$\Omega$ can be finite, countable or uncountable.

\begin{examplebox}{Example}
    \begin{itemize}
        \item Rolling a die: $\Omega = \{1, 2, 3, 4, 5, 6\}$
              \begin{itemize}
                  \item This is sample space is finite.
              \end{itemize}
        \item Tossing a coin until heads comes up: $\Omega = \{H, TH, TTH, \dots, TTT\dots\}$
              \begin{itemize}
                  \item This is sample space is not finite, but it is countable.
              \end{itemize}
        \item Dart in a circular target of radius $r$: $\Omega = \{(x, y) \in \R^2 : x^2 + y^2 \le r\}$
              \begin{itemize}
                  \item This is sample space is uncountable.
              \end{itemize}
    \end{itemize}
\end{examplebox}

\begin{notebox}{impossible $\ne$ probability 0}
    Despite some elements having probability 0, it doesn't mean they are impossible.

    \begin{itemize}
        \item In the dart example, the probability of hitting a single point is 0 since $\Omega$ is uncountable, but it's not impossible.
        \item In the coin example, the probability of getting an infinite number of tails is 0, but it's not impossible.
    \end{itemize}

    In particular note that if $\Omega$ is uncountable every singular outcome has probability 0.
\end{notebox}

\subsection{Events}

An \textbf{event} is a subset of the sample space of all the outcomes such that the event occurs.
We denote it as $A \subseteq \Omega$.

In particular:
\begin{itemize}
    \item The \textbf{empty set} $\varnothing$ is the impossible event.
    \item The \textbf{whole sample space} $\Omega$ is the certain event.
\end{itemize}

We can define the following operations on events:
\begin{itemize}
    \item \textbf{Complement}: $A^c$, the event occurs if $A$ doesn't.
    \item \textbf{Union}: $A \cup B$, the event occurs if $A$ \textit{or} $B$ occurs.
    \item \textbf{Intersection}: $A \cap B$, the event occurs if $A$ \textit{and} $B$ occur.
    \item \textbf{Difference}: $A \setminus B$, the event occurs if $A$ occurs but $B$ doesn't.
    \item \textbf{Symmetric difference}: $A \triangle B$, the event occurs if $A$ or $B$ occur but not both.
\end{itemize}

Moreover we say that $A$ \textbf{depends} of $B$ if $A \subseteq B$. This means that if $B$ occurs, then $A$ also occurs.

\begin{examplebox}{Example}
    Say we toss a coin twice, then the sample space is $\Omega = \{HH, HT, TH, TT\}$.

    We define the following events:
    \begin{itemize}
        \item $A = \{\text{first toss is a head}\} = \{HH, HT\}$
        \item $B = \{\text{2 times heads}\} = \{HH\}$
    \end{itemize}

    We have that $A \subseteq B$, so $A$ depends on $B$.
\end{examplebox}

\begin{notebox}{singleton events}
    Events that contain only one outcome are called \textbf{singleton events}.
\end{notebox}

\subsection{Sigma-algebras}

We usually denote a sigma-algebra as $\F$ and it's a subset of the power set of $\Omega$.
Every sigma-algebra, by definition, has the following properties:
\begin{enumerate}
    \item $\Omega \in \F$
    \item If $A \in \F$, then $A^c \in \F$
    \item If $A_1, A_2, \dots \in \F$, then $\bigcup_{i=1}^\infty A_i \in \F$
\end{enumerate}

\begin{notebox}{notation}
    We introduce the following notation:
    $$
        \bigcap_{i=1}^\infty A_i \qquad \text{ and } \qquad \bigcup_{i=1}^\infty A_i
    $$

    These symbols just denote the intersection and the union respectively for a infinite, countable set of events.
\end{notebox}

\subsubsection{Proprieties of sigma-algebras}

Moreover, we can prove the following properties:
\begin{enumerate}
    \item $\varnothing \in \F$
    \item $\F$ is closed under finite unions
    \item $\F$ is closed under finite and countable intersections
\end{enumerate}

\begin{proof}
    According to the properties of sigma-algebras we have that:

    \begin{enumerate}
        \item $\Omega \in \F$ by $(1)$, so $\Omega^c = \varnothing \in \F$, by $(2)$.
        \item Let $A_1 \cup A_2 \cup \dots \cup A_n$, we extend it by adding $\varnothing$ to the end of the sequence.
              Then this is in $\F$ by $(3)$.
        \item According to De Morgan's laws we have:
              $$
                  \bigcap_{i=1}^n A_i = \left(\left(\bigcap_{i=1}^n A_i\right)^c\right)^c = \left(\bigcup_{i=1}^n A_i^c\right)^c
              $$
              Since the intersection is in $\F$ by $(3)$ and the complement is in $\F$ by $(2)$, then the union is also in $\F$.
    \end{enumerate}
\end{proof}

\begin{notebox}{Reminder of De Morgan's Laws}
    $$
        \left(\bigcup_{i=1}^n A_i\right)^c = \bigcap_{i=1}^n A_i^c
        \qquad \text{ and } \qquad
        \left(\bigcap_{i=1}^n A_i\right)^c = \bigcup_{i=1}^n A_i^c
    $$
\end{notebox}

\subsubsection{Borel sigma-algebra}

Suppose $\Omega$ is uncountable and we want to define a sigma-algebra on it.

We start by choosing a class $\mathcal C$ of subsets of $\Omega$ that we want to include in the sigma-algebra.
Then we define the sigma-algebra generated by $\mathcal C$ as the smallest sigma-algebra that contains $\mathcal C$.

In particular Borel sigma-algebra is the sigma-algebra generated by the open sets of $\R$:
$$
    \mathcal B (\R) = \sigma(\{(a, b) : -\infty < a < b < \infty\})
$$

This definition can be extended to $\R^n$ by taking the gaussian product sigma-algebra:
$$
    \mathcal B (\R^n) = \sigma(\{(a_1, b_1) \times \dots \times (a_n, b_n) : -\infty < a_i < b_i < \infty\})
$$

\section{Class of 07/02/2024}

Let $\Omega$ be a sample space and $\F$ a sigma-algebra on $\Omega$,
then we call $(\Omega, \F)$ a \textbf{measurable space}.

\subsection{Probability measure}

\begin{bluebox}{Definition}
    A \textbf{probability measure} is a function $P: \F \to \R$ such that:
    \begin{enumerate}
        \item $P(A) \ge 0$ for all $A \in \F$
        \item $P(\Omega) = 1$
        \item If $A_1, A_2, \dots \in \F$ are pairwise disjoint, then:
              $$
                  P\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=1}^\infty P(A_i)
              $$
    \end{enumerate}
\end{bluebox}

\subsubsection{Properties of probability measures}

We can prove the following properties of probability measures:
\begin{enumerate}
    \item $P(\bigcup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)$
    \item $P(A^c) = 1 - P(A)$
    \item If $A \subseteq B$, then $P(A) \le P(B)$
    \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
\end{enumerate}

\begin{proof}
    We will prove the properties in order:
    \begin{enumerate}
        \item We will not prove this but it's easy to see that it follows from point 3 of the definition.
        \item Consider the disjoint events $A$ and $A^c$.
              We have that $P(A \cup A^c) = P(\Omega) = 1$.

              Since $A$ and $A^c$ are disjoint, we can apply property 1 to get that
              \begin{align*}
                           & P(A) + P(A^c) = P(\Omega) = 1 \\
                  \implies & P(A^c) = 1 - P(A)
              \end{align*}
        \item Write $B$ as $B = A \cup (B \setminus A)$.
              We have that $A$ and $B \setminus A$ are disjoint, so:
              $$
                  P(B) = P(A) + P(B \setminus A) \ge P(A)
              $$
              since $P(X) \ge 0 \quad \forall X \in \F$.
              Moreover $P(B \setminus A) = P(B) - P(A)$.

              We say that probability is monotone.
        \item We can write $A \cup B = A \cup (B \setminus A)$.
              We have that $A$ and $B \setminus A$ are disjoint, so:
              \begin{align*}
                  P(A \cup B) & = P(A) + P(B \setminus A)   \\
                              & = P(A) + P(B) - P(A \cap B)
              \end{align*}
              by property 3.
    \end{enumerate}
\end{proof}

We can extend property 4 to the case of $n$ events by induction:
\begin{itemize}
    \item For the case of $n = 3$ we have that
          \begin{align*}
              P(A \cup B \cup C) & = P(A \cup (B \cup C))                                                              \\
                                 & = P(A) + P(B \cup C) - P(A \cap (B \cup C))                                         \\
                                 & = P(A) + P(B) + P(C) - P(B \cap C) - P(A \cap B) - P(A \cap C) + P(A \cap B \cap C)
          \end{align*}
    \item For the general case we have that
          \begin{align*}
              P(\bigcup^n_{i=1} A_i) & = \sum^n_{i=1} P(A_i) - \sum_{1 \leq i < j \leq n} P(A_i \cap A_j) & \\
                                     & + \sum_{1 \leq i < j < k \leq n} P(A_i \cap A_j \cap A_k) - \dots    \\
                                     & + (-1)^{n-1} P(A_1 \cap \dots \cap A_n)
          \end{align*}
\end{itemize}
We will not prove the general case.

\begin{examplebox}{Example}
    $P(\text{winning the game}) = 1\%$, we need to find $P(\text{winning at least once in 3 games})$.

    Let $A_i$ be the event of winning the $i$-th game, then we have that:
    \begin{align*}
        P (A_1 \cup A_2 \cup A_3) & = P(A_1) + P(A_2) + P(A_3)                            \\
                                  & - P(A_1 \cap A_2) - P(A_1 \cap A_3) - P(A_2 \cap A_3) \\
                                  & + P(A_1 \cap A_2 \cap A_3)                            \\
                                  & < 3\%
    \end{align*}
\end{examplebox}

\subsubsection{Probability of monotone events}

We can prove the following properties:
\begin{enumerate}
    \item Let $A_1 \subseteq A_2 \subseteq \dots$ be a sequence of events, then:
          $$
              P\left(\bigcup_{i=1}^\infty A_i\right) = \lim_{n \to \infty} P(A_n)
          $$
    \item Let $A_1 \supseteq A_2 \supseteq \dots$ be a sequence of events, then:
          $$
              P\left(\bigcap_{i=1}^\infty A_i\right) = \lim_{n \to \infty} P(A_n)
          $$
    \item For every sequence $(A_n)$ of events, we have that:
          $$
              P(\bigcup_{i=1}^\infty A_i) \le \sum_{i=1}^\infty P(A_i)
          $$
\end{enumerate}

These properties are easy to see by looking at an Euler-Venn diagram but not as trivial to prove.

\begin{proof}
    We will prove the properties in order:
    \begin{enumerate}
        \item Let $(B_n)$ be a sequence of events defined by induction as $B_1 = A_1$ and $B_n = A_n \setminus A_{n-1}$.
              We note that $B_n$ are disjoint
              and that $\bigcup_{i=1}^\infty A_i = \bigcup_{i=1}^\infty B_i$
              since $A_i = \bigcup_{j=1}^i B_j$.

              We have that
              \begin{align*}
                  P\left(\bigcup_{i=1}^\infty A_i\right) & = P\left(\bigcup_{i=1}^\infty B_i\right)                             \\
                                                         & = \sum_{i=1}^\infty P(B_i) = \lim_{n \to \infty} \sum_{i=1}^n P(B_i) \\
                                                         & = \lim_{n \to \infty} P\left(\bigcup_{i=1}^n B_i\right)              \\
                                                         & = \lim_{n \to \infty} P(A_n)
              \end{align*}
        \item Let $(B_n)$ be the sequence defined as $B_n = A_n^c$.
              This sequence is increasing, hence we can apply property 1 and get that
              \begin{align*}
                  P\left(\bigcup_{i=1}^\infty B_i\right)                         & = \lim_{n \to \infty} P(B_n)              \\
                  \implies P\left(\left(\bigcap_{i=1}^\infty A_i\right)^c\right) & = \lim_{n \to \infty} (1-P(A_n))          \\
                  \implies \cancel{1 -} P\left(\bigcup_{i=1}^\infty A_i\right)   & = \cancel{1 -} \lim_{n \to \infty} P(A_n) \\
              \end{align*}
        \item Let $B_n = \bigcup_{i=1}^n A_i$.
              Indeed we have that
              \[
                  \bigcup_{i=1}^\infty B_n =
                  \bigcup_{i=1}^\infty \left( \bigcup_{i=1}^n A_i \right) =
                  \bigcup_{i=1}^\infty A_i
                  \tag{*}
              \]
              Moreover, we note that $B_n$ is increasing, hence we can apply property 1 to get that
              $$
                  P\left(\bigcup_{i=1}^\infty B_i\right) = \lim_{n \to \infty} P(B_n)
              $$

              We can apply (*) on the right hand side and the definition of $B_n$ on the left hand side to get that
              $$
                  P\left(\bigcup_{i=1}^\infty A_i\right) = \lim_{n \to \infty} P(B_n) \le \sum_{i=1}^\infty P(A_i)
              $$

              TODO: there's something wrong here, I'll fix it when i get the book.
    \end{enumerate}

    \begin{examplebox}{Example}
        We will toss a coin infinitely many times. What's the never getting a head?

        Let $A_n$ be the event of getting a head in the first $n$ tosses.
        We have that $P(A_n) = \left(\frac{1}{2}\right)^n$.

        We notice that $A_n \supseteq A_{n+1}$, hence it's a decreasing sequence.

        $$
            P\left(\bigcap_{i=1}^\infty A_i\right) = \lim_{n \to \infty} P(A_n) = 0
        $$

        by property 2.

    \end{examplebox}
\end{proof}

\subsection{Probability in fair games}

\subsubsection{Finite or countable sample space}

Let $\Omega = \{\omega_1, \omega_2, \dots\}$ and $p_i = P(\{\omega_i\})$.
Then
$$
    P(A) = \sum_{\omega_i \in A} p_i
$$

Let $N = |\Omega|$ be the cardinality of the sample space.
We have that
$$
    p_i = \frac{1}{N} \enspace \forall i \qquad \text{ and } \qquad P(A) = \frac{|A|}{N}
$$

Where $|A|$ is the cardinality of the set $A$.

\subsubsection{Uncountable sample space}

We will use $\Omega \subseteq \R^2$ as an example.

Then, the probability of an event $A$ is given by
$$
    P(A) = \frac{\text{Area of } A}{\text{Area of } \Omega}
$$

\begin{examplebox}{Example: Dart in a circular target or radius 1}
    We want to find the probability of $A = \text{distance from center} \leq 0.5$.

    We have that
    $$
        P(A) = \frac{\pi \cdot 0.5^2}{\pi \cdot 1^2} = \frac{1}{4}
    $$
\end{examplebox}

\end{document}