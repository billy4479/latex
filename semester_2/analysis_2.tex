\documentclass[10pt]{extarticle}
\title{Analysis 2 Notes}
\author{Giacomo Ellero}
\date{Semester 2, 2023/2024}

\usepackage{preamble}

\newcommand{\F}{\mathcal{F}}
\renewcommand{\vec}[1]{\underbar{\ensuremath{#1}}}
   
\begin{document}

\maketitle
\tableofcontents
\clearpage

\section{Class of 12/02/2024}

In this class we will discuss mainly multi-variable calculus:
\begin{itemize}
    \item Parametric curves: $\R \to \R^d$.
    \item Graphs: $\R^2 \to \R$.
    \item Vector fields: $\R^d \to \R^d$.
\end{itemize}

And in general we will discuss functions $f: \R^d \to \R^p$.

$\R^d$ is a vector space, like the ones we have seen in linear algebra, so many notions carry over.

\subsection{Dot product and distance}

\textbf{\underline{Definition} (dot product)}: Let $\vec{x}, \vec{y} \in \R^d$. The dot product of $x$ and $y$ is defined as

$$
    \vec{x} \cdot \vec{y} = \sum_{i=1}^d x_i y_i
$$

We have seen dot products (inner products) in linear algebra but we will see some of their proprieties again:
\begin{enumerate}
    \item $\vec{x} \cdot \vec{y} = \vec{y} \cdot \vec{x}$.
    \item $\vec{x} \cdot (\vec{y} + \vec{z}) = \vec{x} \cdot \vec{y} + \vec{x} \cdot \vec{z}$.
    \item $\vec{x} \cdot \vec{x} \geq 0$ and $\vec{x} \cdot \vec{x} = 0 \iff \vec{x} = \vec{0}$.
\end{enumerate}

\subsubsection{Norm}

\textbf{\underline{Definition} (norm)}: Let $\vec{x} \in \R^d$. The norm of $\vec{x}$ is defined as

$$
    \norm{\vec{x}} = \sqrt{\vec{x} \cdot \vec{x}}
$$

For a more general definition see the linear algebra notes.

\subsection{Cauchy-Schwarz inequality}

\textbf{\underline{Theorem} (Cauchy-Schwarz inequality)}: Let $\vec{x}, \vec{y} \in \R^d$. Then

$$
    \abs{\vec{x} \cdot \vec{y}} \leq \norm{\vec{x}} \norm{\vec{y}}
$$

\begin{proof}
    We introduce $f(t) = \norm{\vec{x} + t \vec{y}}^2$.

    We apply some algebraic manipulation to $f(t)$ according to the proprieties of the dot product and the norm:

    \begin{align*}
        f(t) & = \norm{\vec{x} + t \vec{y}}^2                                                                                                              \\
             & = (\vec{x} + t \vec{y}) \cdot (\vec{x} + t \vec{y})                                                                             \\
             & = \vec{x} \cdot (\vec{x} + t \vec{y}) + t \vec{y} \cdot (\vec{x} + t \vec{y})                                       \\
             & = \vec{x} \cdot \vec{x} + t \vec{x} \cdot \vec{y} + t \vec{y} \cdot \vec{x} + t^2 \vec{y} \cdot \vec{y} \\
             & = \norm{\vec{x}}^2 + 2t \vec{x} \cdot \vec{y} + t^2 \norm{\vec{y}}^2
    \end{align*}

    If $\vec{y} \ne 0$ we have that $f(t)$ is a parabola which is always positive, hence $\Delta \leq 0$. We have

    \begin{align*}
        \Delta & = (2 \vec{x} \cdot \vec{y})^2 - 4 \norm{\vec{x}}^2 \norm{\vec{y}}^2        \\
               & = 4 (\vec{x} \cdot \vec{y})^2 - 4 \norm{\vec{x}}^2 \norm{\vec{y}}^2 \leq 0 \\
    \end{align*}

    We pass to the inequality and we get

    \begin{align*}
        \cancel{4}(\vec{x} \cdot \vec{y})^2 & \leq \cancel{4} \norm{\vec{x}}^2 \norm{\vec{y}}^2 \\
        \abs{\vec{x} \cdot \vec{y}} ^2      & \leq \norm{\vec{x}}^2 \norm{\vec{y}}^2            \\
        \abs{\vec{x} \cdot \vec{y}}         & \leq \norm{\vec{x}} \norm{\vec{y}}
    \end{align*}
\end{proof}

Note that if $\Delta = 0$ it's easy to prove that $\vec{x}$  = t $\vec{y}$, hence they are linearly dependent. This is the only case in which the inequality becomes an equality.

\subsection{Defining the angle between two vectors}

\textbf{\underline{Definition} (angle)}: Let $\vec{x}, \vec{y} \in \R^d$. The following relation holds between the dot product and the angle $\theta$ between the two vectors:

$$
    \vec{x} \cdot \vec{y} = \norm{\vec{x}} \norm{\vec{y}} \cos \theta
$$

hence

$$
    \theta = \arccos \left( \frac{\vec{x} \cdot \vec{y}}{\norm{\vec{x}} \norm{\vec{y}}} \right)
$$

From the definition we get that $\vec{x} \cdot \vec{y} = 0 \iff \theta = \frac{\pi}{2}$.

Cauchy-Schwarz inequality guarantees that the argument of $\arccos$ is always between -1 and 1.

Note that this is a definition, not a theorem: we are defining the concept of angle from the dot product.

\section{Class of 14/02/2024}

These theorems have been covered already in linear algebra, see the notes for more details.

\subsection{Triangle inequality}

\textbf{\underline{Theorem} (triangle inequality)}: Let $\vec{x}, \vec{y} \in \R^d$. Then

$$
    \norm{\vec{x} + \vec{y}} \leq \norm{\vec{x}} + \norm{\vec{y}}
$$

\begin{proof}
    We have that

    \begin{align*}
        \norm{\vec{x} + \vec{y}}^2 & = (\vec{x} + \vec{y}) \cdot (\vec{x} + \vec{y})                                                                         \\
                                               & = \vec{x} \cdot (\vec{x} + \vec{y}) + \vec{y} \cdot (\vec{x} + \vec{y})                                     \\
                                               & = \vec{x} \cdot \vec{x} + \vec{x} \cdot \vec{y} + \vec{y} \cdot \vec{x} + \vec{y} \cdot \vec{y} \\
                                               & = \norm{\vec{x}}^2 + 2 \vec{x} \cdot \vec{y} + \norm{\vec{y}}^2
    \end{align*}

    We apply the Cauchy-Schwarz inequality and we get

    \begin{align*}
        \norm{\vec{x} + \vec{y}}^2 & \leq \norm{\vec{x}}^2 + 2 \norm{\vec{x}} \norm{\vec{y}} + \norm{\vec{y}}^2 \\
        \norm{\vec{x} + \vec{y}}^2 & \leq \left(\norm{\vec{x}} + \norm{\vec{y}}\right)^2
    \end{align*}

    We take the square root of both sides and we get the result.
\end{proof}

\subsection{Pythagorean theorem}

\textbf{\underline{Theorem} (Pythagorean theorem)}: Let $\vec{x}, \vec{y} \in \R^d$ be orthogonal. Then

$$
    \norm{\vec{x} + \vec{y}}^2 = \norm{\vec{x}}^2 + \norm{\vec{y}}^2
$$

I will not report the proof here, if you're interested you can find it in my linear algebra notes.

\subsection{Law of cosines}

This is a generalization of the Pythagorean theorem.

\textbf{\underline{Theorem} (law of cosines)}: Let $\vec{x}, \vec{y} \in \R^d$. Then

$$
    \norm{\vec{x} - \vec{y}}^2 = \norm{\vec{x}}^2 + \norm{\vec{y}}^2 - 2 \norm{\vec{x}} \norm{\vec{y}} \cos \theta
$$

where $\theta$ is the angle between $\vec{x}$ and $\vec{y}$.

\subsection{Planes}

Planes in $\R^3$ are identified by a point $A$ and a normal vector $\vec{n}$.

A point $M = \begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix} \in \R^3$ belongs to a plane $\pi$ if and only if the vector $AM$ is orthogonal to $\vec{n}$.

This means that the equation of the plane is

$$
    \left(
    \begin{pmatrix}
            x \\ y \\ z
        \end{pmatrix} - \begin{pmatrix}
            A_x \\ A_y \\ A_z
        \end{pmatrix}
    \right)
    \cdot
    \begin{pmatrix}
        n_x \\ n_y \\ n_z
    \end{pmatrix}
    = 0
$$

\subsection{Orientation of vectors}

Consider $B_1, B_2$ be pairs of vectors in $\R^2$.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \draw[->] (-1, 0) -- (0, 1) node[anchor=south] {$x$};
        \draw[->] (-1, 0) -- (0, 0) node[anchor=north] {$y$};

        \draw[->] (1, 1) -- (2, 1) node[anchor=south] {$y$};
        \draw[->] (1, 1) -- (2, 0) node[anchor=north] {$x$};
    \end{tikzpicture}

    \caption{Two pair of vectors $B_1$ and $B_2$}
    \label{fig:orientation_of_vectors}
\end{figure}

$B_1$ and $B_2$ are both bases of $\R^2$, but if we try to \say{continuously} transform one into the other by \say{rotating} the vectors
while keeping them a basis we will see that at some point we will have $\vec{x} \cdot \vec{y} = 0$,
hence the two vectors no longe form a basis.

We call this propriety the \textbf{orientation} of the basis.
When we choose the orientation of the axis of a space we are choosing the orientation of the basis.

By convention we say that the positive orientation of the basis is the one defined by the right-hand rule.

We can extend this concept to $\R^d$ by considering the determinant of the matrix of the vectors.

\subsection{Determinant}

Determinants have been covered already in linear algebra, for definition and proprieties see those notes.
We will discuss here some of their geometric proprieties that weren't covered before.

In $\R^2$ the determinant of a matrix of two vectors is the area of the parallelogram defined by the two vectors.
In $\R^3$ it is the volume of the parallelepiped defined by the three vectors.

\begin{gather*}
    \det \left( \begin{pmatrix}
            \vec{x} & \vec{y}
        \end{pmatrix} \right) = \text{ area} \\
    \det \left( \begin{pmatrix}
            \vec{x} & \vec{y} & \vec{z}
        \end{pmatrix} \right) = \text{ volume}
\end{gather*}

\subsubsection{Cross product}

\textbf{\underline{Definition} (cross product)}: Let $\vec{x} = \begin{pmatrix}
        x_1 \\ x_2 \\ x_3
    \end{pmatrix}$ and $\vec{y} = \begin{pmatrix}
        y_1 \\ y_2 \\ y_3
    \end{pmatrix}$.
The cross product of $\vec{x}$ and $\vec{y}$ is the vector

$$
    \vec{x} \times \vec{y} = \begin{pmatrix}
        x_2 y_3 - x_3 y_2 \\
        x_3 y_1 - x_1 y_3 \\
        x_1 y_2 - x_2 y_1
    \end{pmatrix}
$$

\subsubsection{Cross product and determinant}

\textbf{\underline{Theorem}}: Let $\vec{x}, \vec{y}, \vec{z} \in \R^3$. Then

$$
    \det \left( \begin{pmatrix}
            \vec{x} & \vec{y} & \vec{z}
        \end{pmatrix} \right) = (\vec{x} \times \vec{y}) \cdot \vec{z}
$$

\begin{proof}
    We have that

    \begin{align*}
        \det \left( \begin{pmatrix}
                            \vec{x} & \vec{y} & \vec{z}
                        \end{pmatrix} \right)
         & = \det \begin{pmatrix}
                      x_1 & y_1 & z_1 \\
                      x_2 & y_2 & z_2 \\
                      x_3 & y_3 & z_3
                  \end{pmatrix}                                                        \\
         & = z_1 \det \begin{pmatrix}
                          x_2 & y_2 \\
                          x_3 & y_3
                      \end{pmatrix}
        - z_2 \det \begin{pmatrix}
                       x_1 & y_1 \\
                       x_3 & y_3
                   \end{pmatrix}
        + z_3 \det \begin{pmatrix}
                       x_1 & y_1 \\
                       x_2 & y_2
                   \end{pmatrix}                                                        \\
         & = z_1 (x_2 y_3 - x_3 y_2) - z_2 (x_1 y_3 - x_3 y_1) + z_3 (x_1 y_2 - x_2 y_1) \\
         & = \begin{pmatrix}
                 z_1 \\ z_2 \\ z_3
             \end{pmatrix} \cdot (\vec{x} \times \vec{y})
    \end{align*}

    The last steps follow from the definition of determinant for $2 \times 2$ matrixes and the definition of the cross product.
\end{proof}

\section{Class of 15/02/2024}

\subsection{Proprieties of the cross product}

\textbf{\underline{Proposition} (characterization of the cross product)}:

\begin{enumerate}
    \item $\vec{x} \times \vec{y}$ is orthogonal to $\vec{x}$ and $\vec{y}$.
    \item $\norm{\vec{x} \times \vec{y}} = \norm{\vec{x}} \norm {\vec{y}} \abs{\sin \theta}$
          where $\theta$ is the angle between $\vec{x}$ and $\vec{y}$.
    \item $\det (\vec{x}, \vec{y}, \vec{x} \times \vec{y}) \geq 0$
          so that the basis $\vec{x}, \vec{y}, \vec{x} \times \vec{y}$ is positively oriented.
\end{enumerate}

\begin{proof}
    We will prove the proprieties one by one.
    These proofs will make heavy use of the definition of the cross product and the determinant from above.
    \begin{enumerate}
        \item From the definition of cross product and knowing that the determinant of a matrix is 0 if two rows are equal we have
              \begin{itemize}
                  \item $(\vec{x} \times \vec{y}) \cdot \vec{x} = \det (\vec{x}, \vec{y}, \vec{x}) = 0$
                  \item $(\vec{x} \times \vec{y}) \cdot \vec{y} = \det (\vec{x}, \vec{y}, \vec{y}) = 0$
              \end{itemize}
        \item Let $\vec{z}$ be a vector normal to $\vec{x}$ and $\vec{y}$, with $\norm{\vec{z}} \ne 0$.
              We have that $\abs{\det (\vec{x}, \vec{y}, \vec{z})} = \abs{(\vec{x} \times \vec{y}) \cdot \vec{z}}$.
              Recall that the formula to compute the area of a parallelogram is $\norm{\vec{x}} \norm{\vec{y}} \abs{\sin \theta}$.
              Since $\det (\vec{x}, \vec{y}, \vec{z})$ is the volume of the parallelepiped defined by the three vectors we have that
              $\abs{\det (\vec{x}, \vec{y}, \vec{z})} = \left(\norm{\vec{x}} \norm{\vec{y}} \abs{\sin \theta}\right) \cdot \norm{\vec{z}}$ just by using the normal formula for the volume (area of base times height).
              Because of (1) we have that $\sin \theta = 1$ and simplifying $\norm{\vec{z}}$ we get the result.
        \item We have that $\det (\vec{x}, \vec{y}, \vec{x} \times \vec{y}) = \norm{\vec{x} \times \vec{y}}^2 \geq 0$.
    \end{enumerate}
\end{proof}

\textbf{\underline{Proposition} (proprieties of the cross product)}:
Let $\vec{x}, \vec{y}, \vec{z} \in \R^3$ and $a, b \in \R$. Then

\begin{enumerate}
    \item \textit{Bilinearity}: $(a \vec{x} + b \vec{y}) \times \vec{z} = a (\vec{x} \times \vec{z}) + b (\vec{y} \times \vec{z})$ and $\vec{x} \times (a \vec{y} + b \vec{z}) = a (\vec{x} \times \vec{y}) + b (\vec{x} \times \vec{z})$.
    \item \textit{Asymmetry}: $\vec{x} \times \vec{y} = - \vec{y} \times \vec{x}$.
    \item \textit{Linear independence}: $\vec{x} \times \vec{y} = \vec{0} \iff \vec{x}$ and $\vec{y}$ are linearly dependent.
\end{enumerate}

\begin{proof}
    To prove (1) and (2) we just apply the definition of the cross product and the determinant.

    For (3) we have
    \begin{align*}
        \norm{\vec{x} \times \vec{y}} = 0
         & \iff
        \norm {\vec{x}} \norm{\vec{y}} \abs{\sin \theta} = 0                                        \\
         & \iff \abs{\sin \theta} = 0 \text{ or } \norm{\vec{x}} = 0 \text{ or } \norm{\vec{y}} = 0 \\
         & \iff \vec{x} \text{ and } \vec{y} \text{ are linearly dependent}
    \end{align*}

\end{proof}

\subsection{Parametrization}

TODO

\section{Class of 16/02/2024}

\subsection{Parametrizing ellipses}

Last class we saw how to parametrize lines and circles, now we will have a look at ellipses.

An ellipse is defined by the parametrization

\[
    \begin{cases}
        x = a \cos t \\
        y = b \sin t
    \end{cases}
\]

where $a$ and $b$ are the semi-axes of the ellipse. Note that if $a = b$ we get a circle of radius $a$.

For the equation we have

$$
    \frac{x^2}{a^2} + \frac{y^2}{b^2} = 1
$$

The ellipse is the image of the unit circle by tge linear transformation $\begin{pmatrix}
        x \\ y
    \end{pmatrix} \to \begin{pmatrix}
        ax \\ by
    \end{pmatrix}$.

\subsubsection{Applying transformations to curves}

Consider curve $\gamma_1$ which we want to rotate by an angle $\theta$ to obtain $\gamma_2$.

Remember the rotation matrix

$$
    R(\theta) = \begin{pmatrix}
        \cos \theta & -\sin \theta \\
        \sin \theta & \cos \theta
    \end{pmatrix}
$$

For the parametrization is quite easy since we just need to multiply the vector by the rotation matrix.

$$
    \begin{pmatrix}
        x \\ y
    \end{pmatrix} = \begin{pmatrix}
        \cos \theta & -\sin \theta \\
        \sin \theta & \cos \theta
    \end{pmatrix} \begin{pmatrix}
        f(t) \\ g(t)
    \end{pmatrix}
$$

Getting the equation is a bit trickier.
We have to invert the transformation matrix:
consider a point $(x, y)$ on $\gamma_2$, we have that $(x, y) = R(\theta) (x', y')$, hence $(x', y') \in \gamma_1 = R(-\theta) (x, y)$.
We have that $(x,y) \in \gamma_2 \iff R(-\theta)(x, y) \in \gamma_1$.
Now we have $(x', y')$ in terms of $(x, y)$ and we can plug the result of the reverse transformation into the equation of $\gamma_1$.

\subsection{Cylinders}

Let $(x, y, z) \in \R^3$. We have that $\sqrt{x^2 + y^2 + z^2}$ is the distance from the origin and $\sqrt{x^2 + y^2}$ is the distance to the $Oz$ axis.

The equation of a cylinder is

$$
    \left\{ (x, y, z) \in \R^3  : x^2 + y^2 = r^2\right\}
$$

we are basically saying that the distance from the $Oz$ axis is constant and is equal to $r$.

\subsection{Cones}

The way we describe cones is by saying that if we slice the cone at altitude $z$ we will have a circle of radius $z$ and center at $(0,0,z)$.

So we want $\sqrt{x^2 + y^2} = |z|$, hence

$$
    \left\{ (x, y, z) \in \R^3 : x^2 + y^2 = z^2 \right\}
$$

\subsubsection{Variations of the cone}

We can consider the following variations

$$
    x^2 + y^2 = z^2 + 1 \\
    x^2 + y^2 = z^2 - 1
$$

In the first one we have that at $z=0$ we have a circle of radius 1, in the second one we have that at $z=0$ we have a circle of radius $-1$ which is not possible, hence the second equation only gets to $z = \pm 1$.

These type of surfaces are called hyperboloids of revolution.

% TODO: Add the drawings

\subsection{Parametric curves}

We can now give a formal definition of a parametric curve.

\underline{Definition}: A parametric curve $\gamma$ is a function of $t \in I \subseteq \R$ and valued in $\R^d$.

$$
    \gamma(t) = \begin{pmatrix}
        \gamma_1(t) \\
        \vdots      \\
        \gamma_d(t)
    \end{pmatrix} \in \R^d
$$

where $\gamma_1, \ldots, \gamma_d$ are the coordinate functions defined as $\gamma_i: I \to \R$.

Parametric curves are basically the description of the motion of a point in space as a function of time.

\underline{Definition}: The \textbf{domain of definition} of $\gamma$ is the intersection of the domains of the coordinate functions.

\subsubsection{How to represent a curve}

There are many ways to represent a curve, for example we could graph the individual coordinate functions but this is not very representative of the curve itself.

We can try to use a graph to represent a curve $\gamma$:

\underline{Definition}: The \textbf{graph} of $\gamma$ is the subset of $\R^{d+1}$ made of points
$$
    (t, \gamma(t)) = (t, \gamma_1(t), \ldots, \gamma_d(t))
$$
for $t \in I$.

The graphs contains all the information about the curve, but it is not very practical to use because of its complexity.

\underline{Definition}: The \textbf{image} of $\gamma$ is the subset of $\R^d$ made of points $\gamma(t)$ for $t \in I$.

The image is basically the graphs seen from the $x$ axis.

This representation is quite practical but we are losing some information about the parameter $t$, this is mostly ok though.

Note that different curves will always have different graphs but might have the same image.

\underline{Remark}: Every graph is the image of some function in $\R^{d+1}$, but not all images are graphs of some function in $\R^{d-1}$.

\section{Class of 19/02/2024}

\subsection{Continuity and differentiability for parametric curves}

\textbf{\underline{Definition} (continuity)}: Let $\gamma: t \to (\gamma_1, \gamma_2, \ldots, \gamma_d) \in R^d$ be a parametric curve.
$\gamma$ is continuous at $t_0 \in I$ if all the functions $\gamma_1, \ldots, \gamma_d$ are continuous at $t_0$.

\textbf{\underline{Definition} (differentiability)}: Let $\gamma: t \to (\gamma_1, \gamma_2, \ldots, \gamma_d) \in R^d$ be a parametric curve.
$\gamma$ is differentiable at $t_0 \in I$ if all the functions $\gamma_1, \ldots, \gamma_d$ are differentiable at $t_0$.
We define $\gamma'(t_0) = (\gamma_1'(t_0), \ldots, \gamma_d'(t_0))$.

\textbf{\underline{Definition} (speed)}: Let $\gamma: t \to (\gamma_1, \gamma_2, \ldots, \gamma_d) \in R^d$ be a parametric curve differentiable at $t_0 \in I$.
Its speed at $t_0$ is $\norm{\gamma'(t_0)}$.

\underline{Example}: consider the unit circle $\gamma(t) = (\cos t, \sin t)$, we have that

$$
    \gamma'(t) = (-\sin t, \cos t)
$$

and
$$
    \norm{\gamma'(t)} = \sqrt{(-\sin t)^2 + \cos^2 t} = 1
$$

\textbf{\underline{Definition} (higher order differentiability)}:
The parametric curve $\gamma$ is $k$ times differentiable at $t_0 \in I$ if all the functions $\gamma_1, \ldots, \gamma_d$ are $k$ times differentiable at $t_0$.

Note that in physics we sometimes use $\dot{o}, \ddot{o}, \ldots$ to denote the derivative of a function with respect to time.

\subsection{Taylor expansion and local behavior}

\textbf{\underline{Definition} (regular point)}:
If $\gamma: I \to \R^d$ is differentiable at $t_0 \in I$, we say that $t_0$ is a regular point if $\gamma'(t_0) \neq \vec{0}$.

\textbf{\underline{Definition} (tangent line)}:
If $\gamma$ is differentiable and $t_0$ is a regular point, the tangent to the image of $\gamma$ at $\gamma(t_0)$ is the line passing through $\gamma(t_0)$ and directed by $\gamma'(t_0)$.
That is

$$
    h \in R \mapsto \gamma(t_0) + h \gamma'(t_0) \in \R^d
$$

\subsubsection{Equation of a line}

We can write the equation of a line from a point and a normal vector:

$$
    \left(
    \begin{pmatrix}
            x \\ y
        \end{pmatrix} - \begin{pmatrix}
            x_0 \\ y_0
        \end{pmatrix}
    \right) \cdot \begin{pmatrix}
        a \\ b
    \end{pmatrix} = 0
$$

where $\begin{pmatrix}
        a \\ b
    \end{pmatrix}$ is the normal vector to the line and $\begin{pmatrix}
        x_0 \\ y_0
    \end{pmatrix}$ is a point on the line.


\underline{Example} (unit circle):
Consider the tangent on the unit circle. We have that its parametrization is
$$
    h \mapsto \begin{pmatrix}
        \cos t_0 \\ \sin t_0
    \end{pmatrix} + h \begin{pmatrix}
        -\sin t_0 \\ \cos t_0
    \end{pmatrix} =
    \begin{pmatrix}
        \cos t_0 - h \sin t_0 \\
        \sin t_0 + h \cos t_0
    \end{pmatrix}
$$

In the case of the unit circle the normal vector is

$$
    \begin{pmatrix}
        a \\ b
    \end{pmatrix}
    =
    \begin{pmatrix}
        -\cos t_0 \\ -\sin t_0
    \end{pmatrix}
    =
    - \gamma(t_0)
$$

(Note that this is the case only for the unit circle.)

We have that the equation of the tangent line of the unit circle at $t_0$ is

$$
    \left(
    \begin{pmatrix}
            x \\ y
        \end{pmatrix} - \begin{pmatrix}
            \cos t_0 \\ \sin t_0
        \end{pmatrix}
    \right)
    \cdot
    \begin{pmatrix}
        -\cos t_0 \\ -\sin t_0
    \end{pmatrix}
    = 0
$$

\underline{Example} ($t_0$ is not a regular point):
Let $\gamma(t) = (t^2, t^3)$, which is continuous and differentiable.
Let $t_0 = 0$, we that $\gamma'(t_0) = \vec{0}$.
We have that
$$
    \begin{cases}
        y = x^{3/2}  & \text{ if } t \geq 0 \\
        y = -x^{3/2} & \text{ if } t \leq 0
    \end{cases}
$$

with $x \geq 0$. We see how $\gamma'(t_0)$ exists but is not regular.

We call this an \textbf{ordinary cusp} (formal definition later).

\begin{figure}[H]
    \centering
    \subfloat[\centering Image]{{
                \includegraphics[width=0.35\textwidth]{assets/analysis_2/ordinary_cusp_image.png}
            }}
    \subfloat[\centering Graph]{{
                \includegraphics[width=0.35\textwidth]{assets/analysis_2/ordinary_cusp_graph.png}
            }}
    \caption{Ordinary cusp}
    \label{fig:ordinary_cusp}
\end{figure}

\subsubsection{Taylor expansion}

\textbf{\underline{Definition} (Taylor expansion)}:
Let $\gamma: I \to \R^d$ be $k$ times differentiable over $I$.
Then

$$
    \gamma(t_0 + h) =
    \gamma(t_0) +
    h \gamma'(t_0) +
    \frac{h^2}{2!} \gamma''(t_0) +
    \ldots +
    \frac{h^k}{k!} \gamma^{(k)}(t_0) +
    o(h^k)
$$

From this definition we see that the image of $\gamma$ is close to $\gamma (t_0)$ since it is the principal term of the expansion.

Moreover note that the Taylor expansion of order 1 with $\gamma'(t_0) \ne \vec{0}$ is

$$
    \gamma(t_0 + h) = \gamma(t_0) + h \gamma'(t_0) + o(h)
$$

which is the equation of the tangent line.

\subsubsection{Local behavior}

We can use the Taylor expansion to study the behavior of a curve in the neighborhood of $t_0$.

% TODO: add graphs

\textbf{\underline{Definition} (biregular point)}:
Let $\gamma: I \to \R^d$ such that $\gamma'(t_0) \ne \vec{0}$ and $\gamma''(t_0)$ is linearly independent from $\gamma'(t_0)$.
Then we say that $t_0$ is a biregular point.

\textbf{\underline{Definition} (inflection point)}:
$t_0$ is an inflection point if $\gamma'(t_0) \ne \vec{0}$ but $\gamma''(t_0)$ is linearly dependent from $\gamma'(t_0)$.

This means that the curve crosses the tangent line at $t_0$.


\textbf{\underline{Definition} (ordinary cusp)}:
$t_0$ is an ordinary cusp if $\gamma'(t_0) = \vec{0}$ but $\gamma''(t_0), \gamma'''(t_0)$ are linearly independent.

\begin{figure}[H]
    \centering
    \subfloat[\centering Inflection point]{{
                \includegraphics[width=0.35\textwidth]{assets/analysis_2/inflection_point.png}
            }}
    \subfloat[\centering Ordinary cusp]{{
                \includegraphics[width=0.35\textwidth]{assets/analysis_2/ordinary_cusp.png}
            }}
    \caption{Inflection point and ordinary cusp}
    \label{fig:inflection_point_and_ordinary_cusp}
\end{figure}

\section{Class of 21/02/2024}

\textbf{\underline{Lemma} ($\gamma'$ and $\gamma''$)}:
If $\gamma: I \to \R^d$ is of class $C^2$ then

$$
    \gamma'(t) \cdot \gamma''(t) = \dv{t}\left(\frac{1}{2}\norm{\gamma'(t)}^2\right)
$$

A direct consequence of this is that $\forall t \in I$, $\gamma'(t) \cdot \gamma''(t) = 0 \iff t \mapsto \norm{\gamma'(t)}^2$ is constant.

\begin{proof}
    We have that

    \begin{align*}
        \dv{t}\norm{\gamma'(t)}^2 & = \dv{t}(\gamma'(t) \cdot \gamma'(t))                         \\
                                  & = \gamma''(t) \cdot \gamma'(t) + \gamma'(t) \cdot \gamma''(t) \\
                                  & = 2 \gamma''(t) \cdot \gamma'(t)
    \end{align*}
\end{proof}

\subsection{Mean value theorem}

Recall the mean value theorem for functions of one variable:

\begin{itemize}
    \item If $f : [a, b] \to \R$ is differentiable then $\exists c \in (a, b)$ such that

          $$
              f'(c) = \frac{f(b) - f(a)}{b - a}
          $$

    \item If $f$ is of class $C^1$ then $\exists c \in (a, b)$ such that

          $$
              f(b) - f(a) = \int_a^b f'(x) \dd{x}
          $$
\end{itemize}

Note that for curves the first point is \textbf{not} generally true.
Consider as counterexample $\gamma(t) = \begin{pmatrix}
        \cos t \\ \sin t
    \end{pmatrix}$ with $t \in [0, 2\pi]$.
We have in fact that $ \frac{\gamma(2\pi) - \gamma(0)}{2\pi - 0} = \vec{0}$ which is not equal to $\gamma'(t)$ for any $t$.

However the second point is still true.

\textbf{\underline{Theorem} (mean value theorem)}:
If $\gamma$ is of class $C^1$ then

$$
    \gamma(b) - \gamma(a) = \int_a^b \gamma'(t) \dd{t}
$$

where $\int_a^b \gamma'(t) \dd{t}$ is the vector in $\R^d$ whose $i$-th component is $\int_a^b \gamma_i'(t) \dd{t}$.

\subsection{Curves in polar coordinates}

\textbf{\underline{Definition} (polar coordinates functions)}:
Let $I \subseteq \R$ and $g: I \to [0, +\infty)$ be a non-negative function.
The curve $r = g(\theta)$ is the real valued function

$$
    \gamma: \theta \in I \to g(\theta) \begin{pmatrix}
        \cos \theta \\ \sin \theta
    \end{pmatrix} \in \R^2
$$

Let's now introduce

$$
    e_r = \begin{pmatrix}
        \cos \theta \\ \sin \theta
    \end{pmatrix},
    \quad
    e_\theta = \begin{pmatrix}
        -\sin \theta \\ \cos \theta
    \end{pmatrix}
$$

These two vectors represent the basis of the polar coordinates system.

\textbf{\underline{Lemma} ($e_r$ and $e_\theta$)}:
\begin{itemize}
    \item $\forall \theta \in I$, $e_r \cdot e_\theta = 0$.
    \item $\theta \mapsto e_r(\theta)$ and $\theta \mapsto e_\theta(\theta)$ are class $C^\infty$ and $e_r' = e_\theta$, $e_\theta' = -e_r$.
\end{itemize}

\begin{proof}
    We use the lemma discussed at the beginning of the class:

    TODO
\end{proof}

\textbf{\underline{Lemma}}: Let $g: I \subseteq \R \to [0, +\infty)$ of class $C^2$.
Then the curve $\gamma: \theta \mapsto g(\theta) e_r(\theta)$ is of class $C^2$ and

\begin{align*}
    \gamma'  & = g' e_r + g e_\theta                         \\
    \gamma'' & = g'' e_r + g' e_\theta + g' e_\theta - g e_r \\
             & = (g'' - g) e_r + 2 g' e_\theta
\end{align*}


\textbf{\underline{Remark}}:
\begin{align*}
    \text{a point $\theta_0$ is regular} & \iff \gamma'(\theta_0) \ne \vec{0}                                           \\
                                         & \iff g'(\theta_0) e_r(\theta_0) + g(\theta_0) e_\theta(\theta_0) \ne \vec{0} \\
                                         & \iff \begin{pmatrix}
                                                    g'(\theta_0) \\ g(\theta_0)
                                                \end{pmatrix} \ne \begin{pmatrix}
                                                                      0 \\ 0
                                                                  \end{pmatrix}                                              \\
                                         & \iff g'(\theta_0) \ne 0
\end{align*}

\begin{proof}
    TODO, just show the calculations, it's just a bunch of derivatives.
\end{proof}

\begin{proof}
    The first steps are just the definition of regular point and deriving $\gamma$.

    The last step is true because we know that $g \geq 0$ by definition, hence if $g = 0$ then it is a minimum and $g' = 0$.
\end{proof}

\subsection{Bezier curves and splines}

Bezier curves are a way for computers to represent curves.
We use a set of control points we can move around to change the shape of the curve.

By convention $t$ is in $[0, 1]$. We will look at the cases for 2 and 3 control points and then generalize to $n$ control points.

\begin{description}
    \item[2 points] We have $P_0$ and $P_1$.
        In this case there isn't much to do, we just have a line from $P_0$ to $P_1$. The parametrization is

        $$
            \gamma(t) = (1 - t) \vec{P_0} + t \vec{P_1}
        $$

    \item[3 points] We have $P_0$, $P_1$ and $P_2$.
        We could just draw straight lines from $P_0$ to $P_1$ and from $P_1$ to $P_2$ but we want to have a smooth curve.

        Let $\gamma_{0,1}$ and $\gamma_{1,2}$ be the parametrizations of the lines from $P_0$ to $P_1$ and from $P_1$ to $P_2$.
        Then $\gamma_{0,1,2}$ will be the interpolation between $\gamma_{0,1}$ and $\gamma_{12}$.

        \begin{align*}
            \gamma_{0,1,2}(t) & = (1 - t) \gamma_{0,1}(t) + t \gamma_{1,2}(t)                                                                                   \\
                              & = (1 - t) \left((1 - t) \vec{P_0} + t \vec{P_1}\right) + t \left((1 - t) \vec{P_1} + t \vec{P_2}\right)
        \end{align*}

    \item[General case] We define this by recursion:
        \begin{itemize}
            \item $\gamma_{0,1}(t) = (1 - t) \vec{P_0} + t \vec{P_1}$
            \item $\gamma_{0,1, \ldots, n}(t) = (1 - t) \gamma_{0,1, \ldots, n}(t) + t \gamma_{1,2, \ldots, n+1}(t)$
        \end{itemize}
\end{description}

\textbf{\underline{Lemma}}:
For every $t \in [0, 1]$

$$
    \gamma_{0,1, \ldots, n}(t) = \sum_{k=0}^n \binom{n}{k} (1 - t)^{n-k} t^k \vec{P_k}
$$

We see how this is a polynomial of class $C^\infty$. It is also very easy to compute the derivative and the bounding box of the curve.

You can find many animations of Bezier curves on the internet, such as this one: \url{https://www.jasondavies.com/animated-bezier/}.

\subsubsection{Spines}

Spines are a generalization of Bezier curves.

\textbf{\underline{Definition}}: Let $\Psi_{k, n}: [0, 1]\to [0, 1]$ such that

$$
    \gamma_{0,1, \ldots, n}(t) = \sum_{k=0}^n \Psi_{k, n}(t) \vec{P_k}
$$

with $\sum_{k=0}^n \Psi_{k, n}(t) = 1$.

See \url{https://www.ibiblio.org/e-notes/Splines/basis.html} for animations of spines.

\section{Class of 22/02/2024 - Notions of topology}


\subsection{Introduction to path integrals}

This is not part of this section of the course but we are doing it now because we will use it in physics very soon.

Let $\gamma: t \in I \to \R^d$ with $d = 1, 2, 3$.
If $\gamma(t)$ is the position at time $t$, then $\gamma'(t) = \dot{\gamma}(t)$ is the velocity, $\gamma''(t) = \ddot{\gamma}(t)$ is the acceleration and the image of $\gamma$ is the trajectory.

Many times in physics we have differential equations of the form

$$
    m \ddot{\gamma} = F(\gamma, \dot{\gamma}, t)
$$

We might use path integrals of a vector fields, for example, to calculate the work done by a force along a trajectory.

Consider the force vector field:

\begin{align*}
    \vec{F}: \R^d & \to \R^d                \\
    F(x,y)              & = \begin{pmatrix}
                                F_1(x, y) \\ F_2(x, y)
                            \end{pmatrix}
\end{align*}

\textbf{\underline{Definition} (path integral)}:

$$
    \int_\gamma \vec{F} \cdot \dd{\vec{s}} = \int_a^b \vec{F}(\gamma(t)) \cdot \gamma'(t) \dd{t}
$$

From this formula we obtain 2 results that we will not prove yet because we don't have the tools but they are important to know:

\begin{itemize}
    \item The result does not depend on the speed of the curve, only on the trajectory.
    \item If $\vec{F} = \nabla g$ (with $g: \R^2 \to \R$) is the gradient of a potential $g$, then $\int_\gamma \vec{F} \cdot \dd{\vec{s}} = \int_\gamma \nabla g \cdot \dd{\vec{s}} = g(\gamma(b)) - g(\gamma(a))$.

          If $g = g(x, y)$ then we have that $\nabla g = \begin{pmatrix}
                  \pdv{g(x, y)}{x} \\ \pdv{g(x,y)}{y}
              \end{pmatrix}$.

          Note that this is basically the extension of the fundamental theorem of calculus in higher dimensions.
\end{itemize}

\subsection{Topology in higher dimensions}

\subsubsection{Useful notions}

We recall some notions we saw earlier in the course:

\begin{itemize}
    \item $\norm{\vec{x}} \in [0,+\infty)$ with $\vec{x} \in \R^d$ (norm).
    \item $\norm{\vec{x} + \vec{y}} \leq \norm{\vec{x}} + \norm{\vec{y}}$ (triangle inequality).
    \item $\norm{a \vec{x}}$ = $\abs{a} \norm{\vec{x}}$.
    \item $\norm{\vec{x}} = 0 \iff \vec{x} = \vec{0}$.
\end{itemize}

\textbf{\underline{Definition} (distance)}: If $\vec{x}, \vec{y} \in \R^d$ then the distance between $\vec{x}$ and $\vec{y}$ is

$$
    d = \norm{\vec{x} - \vec{y}} \in \R
$$

\subsubsection{Limits}

\textbf{\underline{Definition} (limits in $\R^d$)}:
We say that a sequence in $\R^d$, $\left(\vec{x}_n\right)_{n \in \N}$ converges to $\vec{a} \in \R^d$ if the real-valued sequence $\left(\norm{\vec{x}_n - \vec{a}}\right)_{n \in \N}$ converges to 0.

We can also define limits using the usual $\varepsilon - N$ characterization: $\left(\vec{x}_n\right)_{n \in \N}$ converges to $\vec{a} \in \R^d$ iff

$$
    \forall \varepsilon > 0, \exists N \in \N : n \geq N \implies \norm{\vec{x}_n - \vec{a}} < \varepsilon
$$

Geometrically, this means considering a ball of radius $\varepsilon$ centered at $\vec{a}$, then all the points of the sequence will eventually be inside the ball.

\textbf{\underline{Proposition}}:
Let $\vec{x}_n = \begin{pmatrix}
        x_{n,1} \\ \vdots \\ x_{n,d}
    \end{pmatrix}$.

$\left(\vec{x}_n\right)_{n \in \N}$ converges iff for $i \in \{1, \ldots, d\}$, $\left(x_{n,i}\right)_{n \in \N}$ converges to $a_i$ as $n \to +\infty$.

\begin{proof}
    \skiplineafterproof
    \begin{description}
        \item[$\implies$]
            Assume $\norm{\vec{x}_n - \vec{a}}$ converges to 0.
            Fix $i \in \{1, \ldots, d\}$.
            Note that

            $$
                \sum_{j = 1}^d \left(x_{j, n} - a_j\right)^2 \geq \left(x_{i, n} - a_i\right)^2
            $$

            So if we take the square root we get that

            $$
                \norm{\vec{x}_n - \vec{a}} \geq \abs{x_{i, n} - a_i}
            $$

            So since $\norm{\vec{x}_n - \vec{a}}$ converges to 0 we have that $\abs{x_{i, n} - a_i}$ converges to 0 as well, thus $x_{i, n}$ converges to $a_i$.
        \item[$\impliedby$]
            Assume that $\left(x_{n, i}\right)_{n \in \N}$ converges to $a_i$ for $i \in \{1, \ldots, d\}$.
            Let $u_{i, n} = x_{i, n} - a_i$, then $u_{i, n}$ converges to 0.

            We want to prove that

            $$
                \norm{\vec{x}_n - \vec{a}} = \sqrt{\sum_{i=1}^d \left(x_{i, n} - a_i\right)^2} = \sqrt{\sum_{i=1}^d \left(u_{i, n}\right)^2}
            $$
            converges to 0.

            Since $u_{i, n} \to 0$ then also $\left(u_{i, n}\right)^2 \to 0$.
            Then since the square root is continuous we can apply the transference principle to obtain the result.
    \end{description}
\end{proof}

\textbf{\underline{Definition} (operations on limits)}:
Let $\left(\vec{x}_n\right)_{n \in \N}$ and $\left(\vec{y}_n\right)_{n \in \N}$ be sequences in $\R^d$ converging to $\vec{a}$ and $\vec{b}$ respectively.
Then

\begin{enumerate}

    \item $\left(\vec{x}_n + \vec{y}_n\right)_{n \in \N}$ converges to $\vec{a} + \vec{b}$.
    \item $\left(\vec{x}_n \cdot \vec{y}_n\right)_{n \in \N}$ converges to $\vec{a} \cdot \vec{b}$.
    \item If $d = 3$ then $\left(\vec{x}_n \times \vec{y}_n\right)_{n \in \N}$ converges to $\vec{a} \times \vec{b}$.
    \item $\norm{\vec{x}_n}$ converges to $\norm{\vec{a}}$.
\end{enumerate}

\begin{proof}
    The proof of this is very easy and can be obtained by applying the previous proposition and properties of limits in $\R$.
\end{proof}

\section{Class of 23/02/2024 - Interior and closure}

\subsection{Interior}

\textbf{\underline{Definition} (ball)}:
Let $\vec{a} \in \R^d$ and $r \geq 0$.
\begin{itemize}
    \item The open ball of radius $r$ centered at $\vec{a}$ is
          $$
              B_o(\vec{a}, r) = \left\{ \vec{x} \in \R^d : \norm{\vec{x} - \vec{a}} < r \right\}
          $$
    \item The closed ball of radius $r$ centered at $\vec{a}$ is
          $$
              B_c(\vec{a}, r) = \left\{ \vec{x} \in \R^d : \norm{\vec{x} - \vec{a}} \leq r \right\}
          $$
\end{itemize}

\textbf{\underline{Definition} (interior)}:
If $V \subseteq \R^d$, we call interior of $V$, written $\mathring{V}$, the set of $n \in \R^d$ such that

$$
    \exists \varepsilon > 0, \enspace
    \forall \vec{y} \in B_c(\vec{x}, \varepsilon), \enspace
    \vec{y} \in V
$$

then

$$
    \vec{x} \in \mathring{V} \iff \exists \varepsilon > 0, \enspace
    B_o(\vec{x}, \varepsilon) \subseteq V
$$

\subsubsection{Theorem: interior means removing the boundary}

\underline{Statement}: Let $V \subseteq \R^d = \{ (x, y) \in \R^2 : x^2 + y^2 \leq 1 \}$.
Then $\mathring{V} = \{ (x, y) \in \R^2 : x^2 + y^2 < 1 \}$.

\begin{proof}
    We have 3 cases to consider:
    \begin{enumerate}
        \item $x + y < 1$
        \item $x + y = 1$
        \item $x + y > 1$
    \end{enumerate}

    Then
    \begin{enumerate}
        \item $(x, y) \notin V$ so $(x, y) \notin \mathring{V}$. See next proposition for the proof.
        \item Let $(x, y)$ s.t $x + y = 1$.
              For $\varepsilon > 0$ consider $(x, y - \varepsilon)$.
              Then $x + (y - \varepsilon) = 1 - \varepsilon < 1$, thus $(x, y - \varepsilon) \notin V$.
              But $\norm{\begin{pmatrix}
                          x \\ y
                      \end{pmatrix} - \begin{pmatrix}
                          x \\ y - \varepsilon
                      \end{pmatrix}} = \varepsilon$ so $(x, y - \varepsilon) \in B_c((x, y), \varepsilon)$.
              Therefore $(x, y) \notin \mathring{V}$.

        \item Let $(x_0, y_0)$ s.t. $x_0 + y_0 > 1$.
              We know that the direction towards the boundary is $\begin{pmatrix}
                      -1 \\ -1
                  \end{pmatrix}$,
              so the curve $\gamma: t \mapsto \begin{pmatrix}
                      x_0 \\ y_0
                  \end{pmatrix} + t \begin{pmatrix}
                      -1 \\ -1
                  \end{pmatrix}$
              intersects the boundary when $\gamma_1(t) + \gamma_2(t) = 1$.
              That is when $x_0 - t + y_0 - t = 1$ and $t^* = \frac{x_0 + y_0 - 1}{2}$.
              The distance is then $\norm{\gamma(t^*) - \begin{pmatrix}
                          x_0 \\ y_0
                      \end{pmatrix}} = \frac{\sqrt{2}}{2} (x_0 + y_0 - 1)$.
              Therefore for $\varepsilon > 0$ with $\varepsilon < \frac{\sqrt{2}}{2} (x_0 + y_0 - 1)$ we have that $B_o((x_0, y_0), \varepsilon) \subseteq V$, thus $(x_0, y_0) \in \mathring{V}$.
    \end{enumerate}
\end{proof}

\subsubsection{Neighborhoods}

\textbf{\underline{Definition} (neighborhood)}:
If $\vec{n} \in \R^d$, a set $V$ is said the neighborhood of $\vec{n}$ iff $\vec{n} \in \mathring{V}$.

The idea is that when we speak about the interior we fix the set and look at the points inside, while when we speak about neighborhoods we fix the point and look at the sets around it.

\subsubsection{Theorem: relationships between interior and other sets}

\underline{Statement}: Let $V, W \subseteq \R^d$.
Then

\begin{enumerate}
    \item $\mathring{V} \subseteq V$
    \item If $V \subseteq W$ then $\mathring{V} \subseteq \mathring{W}$
\end{enumerate}

\begin{proof}
    \skiplineafterproof
    \begin{enumerate}
        \item Let $\vec{x} \in \mathring{V}$.
              Since $\vec{x} \in \mathring{V}$ then $\exists \varepsilon > 0$ such that $B_c(\vec{x}, \varepsilon) \subseteq V$.
              Thus, since $\vec{x} \in B_c(\vec{x}, \varepsilon)$ we have that $\vec{x} \in V$.
        \item Let $V, W$ such that $V \subseteq W$ let and $\vec{x} \in \mathring{V}$.
              Then, by definition, $\exists \varepsilon > 0$ such that $B_c(\vec{x}, \varepsilon) \subseteq V$.
              As $V \subseteq W$ then $B_c(\vec{x}, \varepsilon) \subseteq V \subseteq W$.
              Therefore $\exists \varepsilon > 0$ such that $B_c(\vec{x}, \varepsilon) \subseteq W$.
              Thus $\vec{x} \in \mathring{W}$.
    \end{enumerate}
\end{proof}

\subsection{Closure}

\textbf{\underline{Definition} (closure)}:
Let $V \subseteq \R^d$. The closure of $V$, written $\overline{V}$, is the set of $\vec{x} \in \R^d$ such that there exists a sequence $\left(\vec{x}_n\right)_{n \in \N}$ of elements of $V$ converging to $\vec{x}$.

For example on $\R$ the closure of $(0, +\infty)$ is $[0, +\infty)$.

                \subsubsection{Theorem: closure adds the boundary}

                \underline{Statement}: Let $\vec{a} \in \R^d$ and $r > 0$.
                Then the closure of $B_o(\vec{a}, r)$ is $B_c(\vec{a}, r)$.

                \begin{proof}
                    We have 3 cases

                    \begin{enumerate}
                        \item $\norm{\vec{x} - \vec{a}} < r$
                        \item $\norm{\vec{x} - \vec{a}} = r$
                        \item $\norm{\vec{x} - \vec{a}} > r$
                    \end{enumerate}

                    Then

                    \begin{enumerate}
                        \item Since $\vec{x} \in V$ then $\vec{x} \in \overline{V}$. See next proposition for the proof.
                        \item Let $\vec{x}$ s.t. $\norm{\vec{x} - \vec{a}} = r$.
                              Let $\vec{x}_n = \vec{x} + t_n(\vec{a} - \vec{x})$, where we want that $\vec{x}_n \to \vec{x}$ and $\vec{x}_n \in V \enspace \forall n$.
                              The first condition is equivalent to saying that $\norm{\vec{x}_n - \vec{x}} = \norm{t_n(\vec{a} - \vec{x})} = t_n \norm{\vec{a} - \vec{x}} \to 0$, therefore we want $t_n \to 0$.
                              For the second condition we want that $\norm{\vec{x}_n - \vec{a}} < r$, which is
                              $\norm{\vec{x}_n - \vec{a}} = \norm{\vec{x} + t_n(\vec{a} - \vec{x}) - \vec{a}} = \norm{(\vec{x} - \vec{a})(1 - t_n)} = r \abs{1 - t_n} < r$. Then this is $< r$ if $t_n \in [0, 1]$.
                              We can take $t_n = \frac{1}{2^n}$.
                        \item We will prove this one by contradiction. Let $\vec{x}$ s.t. $\norm{\vec{x} - \vec{a}} > r$.
                              Take a sequence $\left(\vec{x}_n\right)_{n \in \N} \in V$ converging to $\vec{x}$.
                              This means that $\norm{\vec{x}_n - \vec{a}} < r$ for any $n$.
                              As $\vec{x}_n \to \vec{x}$ we have that $(\vec{x}_n - \vec{a}) \to (\vec{x} - \vec{a})$.
                              Then $\norm{\vec{x}_n - \vec{a}} \to \norm{\vec{x} - \vec{a}}$.
                              So $\lim_{n \to +\infty} \norm{\vec{x}_n - \vec{a}} = \norm{\vec{x} - \vec{a}}$, thus $\norm{\vec{x} - \vec{a}} \leq r$ and we have a contradiction.
                    \end{enumerate}
                \end{proof}

                \subsubsection{Theorem: relationships between closures and other sets}

                \underline{Statement}: Let $V, W \subseteq \R^d$.
                Then

                \begin{enumerate}
                    \item $V \subseteq \overline{V}$
                    \item If $V \subseteq W$ then $\overline{V} \subseteq \overline{W}$
                \end{enumerate}

                \begin{proof}
                    \skiplineafterproof
                    \begin{enumerate}
                        \item Let $\vec{x} \in V$.
                              Then let $\left(\vec{x}_n\right)_{n \in \N} = \vec{x}$ be a constant sequence of elements of $V$.
                              Then $\vec{x}_n \to \vec{x}$ and $\vec{x} \in \overline{V}$.
                        \item Let $V, W$ such that $V \subseteq W$ let and $\vec{x} \in \overline{V}$.
                              Then, by definition, $\exists \left(\vec{x}_n\right)_{n \in \N} \in V$ such that $\vec{x}_n \to \vec{x}$.
                              As $V \subseteq W$ then $\vec{x}_n \in W$ and $\vec{x} \in \overline{W}$.
                    \end{enumerate}
                \end{proof}

                \subsection{Boundary}

                \textbf{\underline{Definition} (boundary)}:
                If $V \subseteq \R^d$, the (topological) boundary of $V$, written $\partial V$, is the set defined as $\partial V = \overline{V} \setminus \mathring{V}$.

                \section{Class of 26/02/2024}

                \subsection{Examples of interior, closure and boundary}

                \begin{center}
                    \begin{tabular}{ |c|c|c|c| }
                        \hline
                                                            & Interior                                   & Closure                                       & Boundary                                   \\
                        \hline
                        $V_1 = \{ (x, y) : x + y \geq 1 \}$ & $x + y > 1$                                & $x + y \geq 1$                                & $x + y = 1$                                \\
                        $V_2 = B_o(\vec{a}, r)$       & $\norm{\vec{x} - \vec{a}} < r$ & $\norm{\vec{x} - \vec{a}} \leq r$ & $\norm{\vec{x} - \vec{a}} = r$ \\
                        $V_3 = \R^d$                        & $\R^d$                                     & $\R^d$                                        & $\varnothing$                              \\
                        $V_4 = \varnothing$                 & $\varnothing$                              & $\varnothing$                                 & $\varnothing$                              \\
                        $V_5 = \Q \subseteq \R$             & $\varnothing$                              & $\R$                                          & $\R$                                       \\
                        $V_6 = (a, b] \subset \R$           & $(a, b)$                                   & $[a, b]$                                      & $\{a, b\}$                                 \\
                        \hline
                    \end{tabular}
                \end{center}

                We note these interesting facts:
                \begin{itemize}
                    \item The closure of $V_4$ does not contain $\pm \infty$, this is because the definition of converging sequence does not include them.
                    \item The interior of $V_5$ is empty because $B_o(q, \varepsilon)$ contains numbers $\notin \Q$.
                    \item The boundary of $V_5$ is $\R$ we can find sequences of rational numbers converging to any real number.
                \end{itemize}

                Also see last class notes for the proofs of interior of $V_1$ and closure of $V_2$.

                \subsection{Theorem: complement of interiors and closures}

                Let $V^c = \R^d \setminus V$, therefore $\R^d = V \cup V^c$.

                \textbf{\underline{Statement}}: Let $V \subseteq \R^d$.
                Then $\mathring{V} \cup \overline{V^c} = \R^d$ and the union is disjoint.
                As a consequence
                \begin{itemize}
                    \item $\left(\mathring{V}\right)^c = \overline{V^c}$.
                    \item $\left(\overline{V}\right)^c = \mathring{V^c}$.
                \end{itemize}

                \begin{proof}
                    We will first prove that $\mathring{V} \cup \overline{V^c} = \R^d$ and then union is disjoint.

                    \begin{itemize}
                        \item
                              First we prove that $\mathring{V} \cup \overline{V^c} = \R^d$. That is to show that if $\vec{x} \notin \mathring{V}$ then $\vec{x} \in \overline{V^c}$.
                              Let $\vec{x} \notin \mathring{V}$, that is
                              $\forall \varepsilon > 0, \enspace \exists \vec{y} \in B_o(\vec{x}, \varepsilon) : \vec{y} \notin V$, which is the same as saying that $\vec{y} \in V^c$.

                              Then for $\varepsilon = \frac{1}{n}$ we can find a $y_n \in B_0 \left(\vec{x}, \frac{1}{n} \right)$ such that $y_n \in V^c$.
                              By the definition of $B_o$ we have that $\norm{\vec{y} - \vec{x}} < \frac{1}{n}$, so $\lim_{n \to +\infty} \vec{y}_n = \vec{x}$.

                              Then the sequence $y_n$ is converging to $\vec{x}$, thus $\vec{x} \in \overline{V^c}$.

                        \item
                              To prove that the union is disjoint (i.e. $\mathring{V} \cap \overline{V^c} = \varnothing$), we take $\vec{x} \in \mathring{V} \cap \overline{V^c}$ and show that this leads to a contradiction.

                              We have that if $\vec{x} \in \mathring{V}$ then $\exists \varepsilon > 0$ such that $B_o(\vec{x}, \varepsilon) \subseteq V$.

                              We also have that since $\vec{x} \in \overline{V^c}$ then $\exists \left(\vec{x}_n\right)_{n \in \N} \in V^c$ such that $\vec{x}_n \to \vec{x}$.
                              As $\lim_{n \to +\infty} \vec{x}_n = \vec{x}$ then $\exists N$ such that $\vec{x_n} \in B_o(\vec{x}, \varepsilon)$ for $n \geq N$.

                              So $\vec{x}_n \in V^c$ (by assumption) and $\vec{x}_n \in B_o(\vec{x}, \varepsilon) \subseteq V$, which is a contradiction.
                    \end{itemize}

                    Now we are left to prove the consequences.

                    Recall that $W_1 \cup W_2 \text{ (disjoint union)} \iff W_1 = W_2^c \iff W_2 = W_1^c$.

                    \begin{itemize}
                        \item Since we just proved that $\mathring{V} \cup \overline{V^c} = \R^d$ and the union is disjoint then we apply the propriety above to get that $\left(\mathring{V}\right)^c = \overline{V^c}$.
                        \item Let $V = W^c$ for some $W \subseteq \R^d$.
                              Then we apply the propriety we just proved to get that $\left(\overline{V}\right)^c = \mathring{V^c}$, that is $\left(\mathring{W^c}\right)^c = \overline{W^{cc}} = \overline{W}$.
                              Then we take the complement on each side and get that $\left(\mathring{W^c}\right)^{cc} = \mathring{W^c} = \left(\overline{W}\right)^c$
                    \end{itemize}
                \end{proof}

                \subsection{Open and closed sets}

                In $\R$ open and closed sets are just the intervals. We can generalize the definition to $\R^d$.

                \textbf{\underline{Definition} (open set)}: A set $V$ is open if $V = \mathring{V}$.

                \textbf{\underline{Definition} (closed set)}: A set $V$ is closed if $V = \overline{V}$.

                Note that not all sets are open or closed, for example $(a, b]$ is neither open nor closed.

In $\R^d$ open balls are open, closed balls are closed.
Moreover $\R^d$ and $\varnothing$ are both both open and closed.

\textbf{\underline{Proposition} (complement of open and closed sets)}:
The complement of a closed set is open and the complement of an open set is closed.

\begin{proof}
    Let $V$ be a closed set (i.e. $V = \overline{V}$).
    We want to show that $V^c$ is open (i.e. $V^c = \mathring{V^c}$).

    We have that $\mathring{V^c} = \left(\overline{V}\right)^c = V^c$, by the previous theorem and by the definition of closed set.

    The proof of the other case is the same.
\end{proof}

\section{Class of 28/02/2024}

\subsubsection{Theorem: properties of open and closed sets}

\textbf{\underline{Proposition}}: Let $V \subseteq \R^d$.
Then $\mathring{V}$ is open and its the largest open set contained in $V$.
Also $\overline{V}$ is closed and its the smallest closed set containing $V$.

\begin{proof}
    \skiplineafterproof
    \begin{itemize}
        \item
              Let $V$ and $\mathring V$ be its interior. We want to prove that $\mathring V$ is open, that is $\forall \vec{x} \in \mathring V, \exists \varepsilon > 0 : B_o(\vec{x}, \varepsilon) \subseteq \mathring V$ (which is exactly the same as saying $\vec{x} \in \mathring{\mathring{V}}$)

              Let $x \in \mathring V$, then $\exists \varepsilon > 0 : B_o(\vec{x}, \varepsilon) \subseteq V$.
              But we know that $\mathring{B_o(\vec{x}, \varepsilon)} \subseteq \mathring{V}$.
              But $B_o(\vec{x}, \varepsilon)$ is open, so $\mathring{B_o(\vec{x}, \varepsilon)} \subseteq \mathring{V}$, thus $\mathring{V}$ is open.

              Moreover, let $W$ be an open set such that $W \subseteq V$.
              Then $\mathring{W} \subseteq \mathring{V}$, but $W$ is open so $\mathring{W} \subseteq \mathring{V}$. Thus any open set contained in $V$ is contained in $\mathring{V}$.

        \item
              Let $V$ and $\overline V$ be its closure. We want to prove that $\overline V$ is closed, that is if we take a sequence $\left(\vec{x}_n\right) \in \overline{V}$ converging, we need to prove that it converges to a $\vec{x} \in \overline V$.

              We could prove it this way but it's quite involved, we will use a different (simpler) approach.

              Recall that $\left(\overline{V}\right)^c = \mathring{V^c}$, so $\left(\overline{V}\right)^c$ is open (since is an interior).
              $\overline{V}$ is the complement of an open set, therefore is closed.

              For the second part we need to prove that if $V \subseteq W$ and $W$ is closed then $\overline{V} \subseteq W$.
              TODO: add this, it wasn't done in class
    \end{itemize}
\end{proof}

\subsection{Functions of several variables}

We will start by considering functions of the type $f: \R^2 \to \R$ since they are very common in many other subjects like probability, physics, economics, etc.

\textbf{\underline{Definition}}: We look at $f: D \subseteq \R^2 \to \R$ which maps every point $(x, y) \in D$ to a real number $f(x, y)$.
It's graph is a subset of $\R^3$ made of points $(x, y, f(x, y))$ for $(x, y) \in D$.

\textbf{\underline{Definition} (level sets)}: the level set of $f: D \subseteq \R^2 \to \R$ at level $c \in \R$ is the subset of $\R^2$ made of points $\{ (x, y) \in D : f(x, y) = c \}$.

\section{Class of 29/02/2024}

Consider the function $f: \R^2 \to \R$ that defines a plane in $\R^3$:

$$
    z = f(x, y) = ax + by + c
$$

Then the graph of such function is a plane normal to
$\begin{pmatrix}
        a \\ b \\ -1
    \end{pmatrix}$
and passing through the point
$\begin{pmatrix}
        0 \\ 0 \\ c
    \end{pmatrix}$
.

Its level set at height $k$ is the line with equation $ax + by = k - c$. All of these lines are parallel to each other and normal to the vector
$\begin{pmatrix}
        a \\ b
    \end{pmatrix}$
.


\subsection{Continuity}

Recall the definition of continuity for function $f: \R \to \R$:

$$
    \forall \varepsilon > 0, \enspace \exists \delta > 0, \enspace \forall x \in \R, \enspace \abs{x - x_0} < \delta \implies \abs{f(x) - f(x_0)} < \varepsilon
$$

The definition of continuity for functions of several variables is the same, but we replace the absolute value with the norm where needed.

\textbf{\underline{Definition} (continuity)}: A function $f: D \subseteq \R^2 \to \R$ is continuous at $\vec{x}_0 \in D$ if

$$
    \forall \varepsilon > 0, \enspace \exists \delta > 0, \enspace \forall \vec{x} \in D, \enspace \norm{\vec{x} - \vec{x}_0} < \delta \implies \abs{f(\vec{x}) - f(\vec{x}_0)} < \varepsilon
$$

That is, for every $\varepsilon > 0$ we can find a ball of radius $\delta$ around $\vec{x}_0$ such that $f(B_o(\vec{x}_0, \delta)) \subseteq B_o(f(\vec{x}_0), \varepsilon)$.

\textbf{\underline{Propostion} (transference principle)}:
Let $f: D \subseteq \R^2 \to \R$, then $f$ is continuous at $\vec{x}_0 \in D$ iff for every sequence $\left(\vec{x}_n\right)_{n \in \N} \in D$ converging to $\vec{x}_0$ there holds $\lim_{n \to +\infty} f(\vec{x}_n) = f(\vec{x}_0)$.

\begin{proof}
    \skiplineafterproof
    \begin{description}
        \item[$\implies$]
            Assume $f$ continous at $x_0$ and let $\left(\vec{x}_n\right)_{n \in \N} \in D$ converging to $\vec{x}_0$.
            Let $\varepsilon > 0$.
            As $f$ is continuous there is a $\delta > 0$ with $\abs{f(\vec{x}) - f(\vec{x}_0)} < \varepsilon$ if $\norm{\vec{x} - \vec{x}_0} < \delta$.

            As $\vec{x}_n \to \vec{x}_0$ then $\exists N$ such that $\norm{\vec{x}_n - \vec{x}_0} < \delta$ for $n \geq N$.
            Then $\abs{f(\vec{x}_n) - f(\vec{x}_0)} < \varepsilon$ for $n \geq N$, thus $\lim_{n \to +\infty} f(\vec{x}_n) = f(\vec{x}_0)$.

        \item[$\impliedby$]
            We will argue by contraposition and say that if $f$ is not continuous, then there exists a sequence $\left(\vec{x}_n\right)_{n \in \N} \in D$ converging to $\vec{x}_0$ such that $\lim_{n \to +\infty} f(\vec{x}_n) \neq f(\vec{x}_0)$.

            Since $f$ is not continuous $\exists \varepsilon > 0$ such that $\forall \delta > 0, \exists \vec{x} \in D$ such that $\norm{\vec{x} - \vec{x}_0} < \delta$ but $\abs{f(\vec{x}) - f(\vec{x}_0)} > \varepsilon$.

            Fix such $\varepsilon > 0$. For $\delta = 1/n$ we can find $\vec{x}_n \in D$ such that $\norm{\vec{x}_n - \vec{x}_0} < 1/n$ but $\abs{f(\vec{x}_n) - f(\vec{x}_0)} > \varepsilon$.

            Then $\vec{x}_n \to \vec{x}_0$ but $\lim_{n \to +\infty} f(\vec{x}_n) \neq f(\vec{x}_0)$.
    \end{description}

    This proof is very similar to the one seen in analysis 1 for functions of one variable.
\end{proof}

\textbf{\underline{Proposition} (operations on continuous functions)}:
If $f, g: D \subseteq \R^2 \to \R$ are continuous at $\vec{x}_0 \in D$ then
\begin{enumerate}[label=\roman*.]
    \item $f + g$ is continuous at $\vec{x}_0$.
    \item $f \cdot g$ is continuous at $\vec{x}_0$.
    \item If $g(\vec{x}_0) \neq 0$ then $1/g$ is continuous at $\vec{x}_0$.
\end{enumerate}

\begin{proof}
    oops
\end{proof}

\textbf{\underline{Proposition} (composition of continuous functions)}:
Let $\gamma: I \in \R \to R^2$ be a continuous parametric curve and $f: D \subseteq \R^2 \to \R$ be continuous at $\gamma(t_0)$.
Then $f \circ \gamma: I \to \R$ is continuous at $t_0$.

\begin{proof}
    Let $\left(t_n\right)_{n \in \N} \in I$ be a sequence converging to $t_0$.
    Then, as $\gamma$ is continuous at $t_0$, $\lim_{n \to +\infty} \gamma(t_n) = \gamma(t_0)$ (limit coordinate by coordinate).
    Then, as $f$ is continuous at $\gamma(t_0)$, $\lim_{n \to +\infty} f(\gamma(t_n)) = f(\gamma(t_0))$.
\end{proof}

\section{Class of 01/03/2024}

\subsubsection{Example: limits and continuity}

Consider the function

$$
    f(x, y) = \frac{x y^2}{x^2 + y^4}
$$

We want to find $\lim_{(x, y) \to (0, 0)} f(x, y)$.

First we consider the denominator and notice that $x^2 \gg y^4$ as $(x, y) \to (0, 0)$.
Then we can write

\begin{align*}
    f(x, y) = \frac{x y^2}{x^2 + y^4} & \approx \frac{x y^2}{x^2} \\
                                      & = \frac{y^2}{x}           \\
\end{align*}

and since $x \gg y^2$ we have that the limit is 0 and we could think that the function is continuous at $(0, 0)$.

But this depends on the path we take to $(0, 0)$: if we approach $(0, 0)$ in such a way that $x \sim y^2$ then the limit is $\frac{1}{2}$.

This means that we can define a sequence that converges to $(0, 0)$ such that the limit of the function is different from 0, in the case of this particular function we can choose the following sequence:

$$
    \begin{pmatrix}
        x_n \\ y_n
    \end{pmatrix} = \begin{pmatrix}
        \frac{1}{n^2} \\ \frac{1}{n}
    \end{pmatrix}
$$

You can see the graph of this function here:
\url{https://www.geogebra.org/3d/wujrd97y}.

\subsection{Partial derivatives}

\textbf{\underline{Definition} (partial derivative)}: Let $f: D \subseteq \R^2 \to \R$ and $(x_0, y_0) \in D$.
We call partial derivative with respect to $x$ at $(x_0, y_0)$, if exists

$$
    \pdv{f}{x}(x_0, y_0) = \lim_{h \to 0} \frac{f(x_0 + h, y_0) - f(x_0, y_0)}{h}
$$

and partial derivative with respect to $y$ at $(x_0, y_0)$, if exists

$$
    \pdv{f}{y}(x_0, y_0) = \lim_{h \to 0} \frac{f(x_0, y_0 + h) - f(x_0, y_0)}{h}
$$

These are basically the normal derivatives but we fix one variable and let the other vary. Therefore when, for example, we take the partial derivative with respect to $x$ we consider the function $f(x, y_0)$ and we consider $y$ as a constant.

\textbf{\underline{Definition}(directional derivative)}: Let $f: D \subseteq \R^2 \to \R$, $(x_0, y_0) \in D$ and $\vec{u} = (u_1, u_2) \in \R^2$ be a vector with norm $\norm{\vec{u}} = 1$.
The directional derivative in the direction $\vec{u}$ is, if exists,

$$
    \pdv{f}{\vec{u}}(x_0, y_0) = \lim_{h \to 0} \frac{f(x_0 + h u_1, y_0 + h u_2) - f(x_0, y_0)}{h}
$$

This is just a generalization of the partial derivative, in fact, if $\vec{u} = (1, 0)$ then we get the partial derivative with respect to $x$ and if $\vec{u} = (0, 1)$ then we get the partial derivative with respect to $y$.

Note that to compute partial derivatives we almost never have to compute limits. If we consider $\gamma: t \in \R \mapsto (x_0, y_0) + t\vec{u} = (x_0 + t u_1, y_0 + t u_2)$ then $\pdv{f}{\vec{u}}() (x_0, y_0)$ is the derivative at $t = 0$ of $t \mapsto f(\gamma(t))$.

\subsubsection{Resolving confusing cases}

Consider $f(x, y) = xy^2$. We what to understand what the following expressions mean:

\begin{itemize}
    \item $\pdv{f}{x}()(y,x)$: here we have that we take the derivative with respect to the first variable and then we evaluate it at $(y, x)$.
    \item $\pdv{f}{x}()(x,x)$: here we have that we take the derivative with respect to the first variable and then we evaluate it at $(x, x)$. It could be clearer to use $\pdv{f}{\vec{u}}()(x,x)$ with $\vec{u} = (1, 0)$.
    \item $\dv{f}{x}()(x,x)$: here we substitute $y$ with $x$ first and then we take the (normal) derivative with respect to $x$.
\end{itemize}

\section{Class of 05/03/2024}

\subsubsection{Example: computing partial derivatives}

Consider the function $f(x, y) = \sqrt{\abs{xy}}$. We want to compute the partial derivatives at $(0, 1)$.

We need to consider the mappings

\begin{align*}
    h & \mapsto f(x_0 +h, y_0) \\
    h & \mapsto f(x_0, y_0 +h)
\end{align*}

and check weather the limits exist as $h \to 0$.

We have

\begin{align*}
    f(h, 1)     & = \sqrt{h} & \text{ not differentiable} \\
    f(0, 1 + h) & = 0        & \text{ differentiable}     \\
\end{align*}

Therefore the solution to the problem is

\begin{align*}
    \pdv{f}{x}()(0, 1) & \text{ does not exist} \\
    \pdv{f}{y}()(0, 1) & = 0
\end{align*}

\subsubsection{Example: non-differentiability}

Let

$$
    f(x, y) = \begin{cases}
        \frac{x y^2}{x^2 + y^4} & \text{ if } (x, y) \neq (0, 0) \\
        0                       & \text{ if } (x, y) = (0, 0)
    \end{cases}
$$

as we saw in the previous class, the directional derivatives of this function at $(0, 0)$ exist from any direction, but the the function is still not continuous nor differentiable at $(0, 0)$.

This is the worst case scenario and it can be hard to spot it.

We proceed by computing the directional derivative with respect to $\vec{u} = (u_1, u_2)$.

\begin{align*}
    f(x_0 + hu_1, y_0 + hu_2) & = f(hu_1, hu_2)                         \\
                              & = \frac{hu_1(u_2)^2}{u_1^2 + h^2 u_2^4} \\
\end{align*}

and we can also calculate the limit as $h \to 0$.

\begin{align*}
    \pdv{f}{\vec{u}}()(0, 0) & = \dv{h} \left[ f(hu_1, hu_2) \right]_{h = 0} \\
                                   & = \frac{u_2^2}{u_1}                           \\
\end{align*}

\subsection{Differentiability}

As we just saw in the previous example, the existence of directional derivatives does not imply the differentiability of the function.
We need to add another condition.

We will now introduce the concept of \textbf{differentials}.

Recall the definition of differentiability for functions $f: \R \to \R$:

$$
    \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h} = f'(x_0)
$$

We have trouble to generalize this definition for functions of several variables, because if $h$ becomes a vector then the division by $h$ is not defined a priori.

Recall also the the taylor expansion

$$
    f(x_0 + h) = f(x_0) + hf'(x_0) + o(h)
$$

The best way to think about this in $\R^d$ is to consider $hf'(x_0)$ as a linear function of $h$ and let $g(h) = o(h)$ with $\lim_{h \to 0} \frac{g(h)}{h} = 0$.

In $\R^2$ we can write the taylor expansion as

$$
    \begin{cases}
        f(x_0 + h, y_0) = f(x_0, y_0) + h \pdv{f}{x}()(x_0, y_0) + o(h) \\
        f(x_0, y_0 + k) = f(x_0, y_0) + k \pdv{f}{y}()(x_0, y_0) + o(k)
    \end{cases}
$$

We would like something similar to

$$
    f(x_0 + h, y_0 + k) \approx f(x_0, y_0) + h \pdv{f}{x}()(x_0, y_0) + k \pdv{f}{y}()(x_0, y_0)
$$

this is in fact the definition of differentiability: we not only need the existence of the partial derivatives but also that the first order taylor expansion is a good approximation of the function.

The only missing piece is now to define what exactly means $\approx$ in this context, that is what is the definition of $o(h, k)$.

\textbf{\underline{Proposition} (little o)}:
Let $g : D \in R^2 \to \R$ with $D$ be a neighborhood of $(0,0)$.
Then the following are equivalent:

\begin{enumerate}[label=(\roman*)]
    \item $g(0,0) = 0$ and the function $\frac{g(h,k)}{\norm{(h,k)}}$ converges to 0 as $(h,k) \to (0,0)$. That is
          $$
              \forall \varepsilon > 0, \enspace \exists \delta > 0, \enspace \forall (h,k) \in D, \enspace \norm{(h,k)} \leq \delta \implies \frac{g(h,k)}{\norm{(h,k)}} \leq \varepsilon
          $$
    \item
          $$
              \forall \varepsilon > 0, \enspace \exists \delta > 0, \enspace \norm{(h,k)} \leq \delta \implies g(h, k) \leq \varepsilon \norm{(h,k)}
          $$

    \item There exists $\omega: D \to \R$, continuos, with $\omega(0,0) = 0$ and
          $$
              g(h,k) = \norm{(h,k)} \omega(h,k)
          $$
\end{enumerate}

If $g$ satisfies these proprieties then we say that $g = o(h,k)$ as $(h,k) \to (0,0)$.

\begin{proof}
    \skiplineafterproof
    \begin{description}
        \item[(i) $\iff$ (ii)]
            We multiply/divide by $\norm{(h,k)}$.

        \item[(i) $\iff$ (iii)]
            Define $\omega$ as

            $$
                \omega(h,k) = \begin{cases}
                    \frac{g(h,k)}{\norm{(h,k)}} & \text{ if } (h,k) \neq (0,0) \\
                    0                           & \text{ if } (h,k) = (0,0)
                \end{cases}
            $$
    \end{description}
\end{proof}

\textbf{\underline{Definition} (differentiability)}:
Let $f: D \subseteq \R^2 \to \R$ and $(x_0, y_0) \in D$ with $D$ open.
We say that $f$ is differentiable at $(x_0, y_0)$ if all of the following comditions are true
\begin{itemize}
    \item Both partial derivative exist at $(x_0, y_0)$
    \item There exists $r > 0$ such that $(x_0 + h, y_0 + k) \in D$ for any $(h, k) \in B_o \left((0,0), r\right)$
    \item The first order taylor expansion is a good approximation of the function, that is
          $$
              f(x_0 + h, y_0 + k) = f(x_0, y_0) + h \pdv{f}{x}()(x_0, y_0) + k \pdv{f}{y}()(x_0, y_0) + o(h, k)
          $$
\end{itemize}

\textbf{\underline{Definition} (differential)}:
Let $Df_{(x_0, y_0)}$ be the linear map defined as

$$
    Df_{(x_0, y_0)}: \begin{pmatrix}
        h \\ k
    \end{pmatrix}
    \mapsto h \pdv{f}{x}()(x_0, y_0) + k \pdv{f}{y}()(x_0, y_0)
$$

\textbf{\underline{Definition} (gradient)}:
If $f$ is differentiable at $(x_0, y_0)$ then the gradient of $f$ at $(x_0, y_0)$ is

$$
    \nabla f(x_0, y_0) = \begin{pmatrix}
        \pdv{f}{x}()(x_0, y_0) \\ \pdv{f}{y}()(x_0, y_0)
    \end{pmatrix} \in \R^2
$$

Note that

\begin{align*}
    h \pdv{f}{x}()(x_0, y_0) + k \pdv{f}{y}()(x_0, y_0) & = \begin{pmatrix}
                                                                h \\ k
                                                            \end{pmatrix} \cdot \begin{pmatrix}
                                                                                    \pdv{f}{x}()(x_0, y_0) \\ \pdv{f}{y}()(x_0, y_0)
                                                                                \end{pmatrix} \\
                                                        & = \begin{pmatrix}
                                                                h \\ k
                                                            \end{pmatrix} \cdot \nabla f(x_0, y_0)
\end{align*}

And we can write the taylor expansion as

$$
    f(\vec{x_0} + \vec{h}) = f(\vec{x_0}) + \nabla f(\vec{x_0}) \cdot \vec{h} + o(\vec{h})
$$

\textbf{\underline{Remark} (the gradient is the direction of maximum increase)}:
If we choose $\vec{u} = \nabla f(\vec{x_0})$ then we have that the directional derivative is the maximum possible.
Moreover if we choose $\vec{u} = -\nabla f(\vec{x_0})$ then we have that the directional derivative is the minimum possible.

Instead, if we choose $\vec{u}$ orthogonal to $\nabla f(\vec{x_0})$ then we have that the directional derivative is 0 and we find a level set of the function.

\textbf{\underline{Proposition} (differentiability implies continuity)}:
Let $f: D \subseteq \R^2 \to \R$ be differentiable at $(x_0, y_0)$.
Then $f$ is continuous at $(x_0, y_0)$.

\begin{proof}
    We want to prove that $\vec{x_0} = (x_0, y_0)$, $\vec{h} \mapsto f(\vec{x_0} + \vec{h})$ is continuous at $\vec{h} = \vec{0}$.

    Since $f$ is differentiable we have that

    $$
        f(\vec{x_0} + \vec{h}) =
        \underbrace{f(\vec{x_0})}_{\text{constant}}
        + \underbrace{\nabla f(\vec{x_0}) \cdot \vec{h}}_{\text{continuous and } \to 0}
        + \underbrace{o(\vec{h})}_{\to 0 \text{ by definition}}
    $$

    For $g(\vec{h}) o(\vec{h})$ we say $\forall \varepsilon > 0, \enspace \exists \delta > 0, \enspace \norm{\vec{h}} \leq \delta \implies \abs{g(\vec{h})} \leq \varepsilon \norm{\vec{h}}$.

    If we set $\varepsilon = 1$ we have that there exists some $\delta > 0$ such that
    $\norm{\vec{h}} \leq \delta \implies \abs{o(\vec{h})} \leq \norm{\vec{h}}$.
    Then $\lim_{\vec{h} \to \vec{0}} g(\vec{h}) = 0$.

\end{proof}

\end{document}