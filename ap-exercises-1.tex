\documentclass[12pt]{extarticle}

\title{Advanced Programming and Optimization Algorithms \\ Homework 01}
\author{Giacomo Ellero}
\date{24/02/2025}

\usepackage{preamble_base}
\usepackage{preamble_math}

\numberwithin{equation}{section}

\begin{document}

\maketitle

\section*{Exercise 1}
\stepcounter{section}

Ideally we'd like to minimize the distance function for every point inside the polyhedra,
however this function is not linear, therefore we have to do a little more work.
We introduce two new variables $z, w \in \R^n$ defined as
\begin{align}
	x_i - y_i & = z_i - w_i \\
	z         & \geq 0      \\
	w         & \geq 0
\end{align}
so that $z$ is the positive part of the distance between two points and $w$ is the negative part
and $\abs{x_i - y_i} = z_i + w_i$.

Then we can write the linear program as follows:
\begin{align}
	\min                                          & \sum_{i = 1}^n z_i + \sum_{i = 1}^n w_i \\
	\text{s.t. } \forall i = 1, \dots, n \enspace & A_i x_i \leq a_i                        \\
	                                              & B_i y_i \leq b_i                        \\
	                                              & z_i \geq 0                              \\
	                                              & y_i \geq 0                              \\
	                                              & x_i - y_i = z_i - w_i
\end{align}

Note that for each $i$ the optimal solution must have that either $z_i^* = 0$ or $w_i^* = 0$
since otherwise we could decrease both and get a better solution.

\section*{Exercise 2}
\stepcounter{section}

We want to show that
\begin{equation}
	a^T_j \left(\sum_i^N \alpha_i x_i\right) \leq b_j \quad \forall j = 1, \dots, m
\end{equation}
for some $\alpha_1, \dots, \alpha_n \geq 0$ such that $\sum_i^n \alpha_i = 1$.

Recall that by hypothesis we have that $a^T_j x_i \leq b_j$, then for each $j$ we can write
\begin{align}
	a^T_j \left(\sum_i^n \alpha_i x_i\right) & = \sum_i^n a_j^T \alpha_i x_i \\
	                                         & \leq \sum_i^n \alpha_i b_j    \\
	                                         & = b_j \sum_i^n \alpha_i = b_j
\end{align}
as desired.

\section*{Exercise 3}
\stepcounter{section}

We know that the payoff matrix is
where the rows indicate what SC plays (from top to bottom R, P and S)
while the columns represent what EB plays (from left to right R and P).

We have shown in class that SC wants to solve
\begin{align}
	\max             x_0 & \quad \text{subject to} \\
	x_2 - x_3 - x_0      & \geq 0                  \\
	-x_1 + x_3 - x_0     & \geq 0                  \\
	x_1 + x_2 + x_3      & = 1                     \\
	x_1, x_2, x_3        & \geq 0
\end{align}

From here we follow the dualization recipe:
\begin{itemize}
	\item We will have $3$ variables, say $y_1, y_2, y_0$, since in the primal we have 3 constraints
	\item Our matrix is
	      \begin{equation}
		      A = \begin{pmatrix}
			      0  & 1 & -1 & -1 \\
			      -1 & 0 & 1  & -1 \\
			      1  & 1 & 1  & 0
		      \end{pmatrix}
	      \end{equation}
	      therefore for the dual we take its transpose:
	      \begin{equation}
		      A^T = \begin{pmatrix}
			      0  & -1 & 1 \\
			      1  & 0  & 1 \\
			      -1 & 1  & 1 \\
			      -1 & -1 & 0
		      \end{pmatrix}
	      \end{equation}
	\item The cost vector $c$ is $[0,0,0,1]$, therefore in the dual we will have it on
	      the right-hand side of the constraints;
	\item The right-hand side of the constraints $b$ of the primal is $[0, 0, 1]$, which means
	      that the objective function of the dual is $b^T y$;
	\item The first two constraints of the primal have a $\geq$ sign,
	      therefore $y_1, y_2$ are $\leq 0$;
	\item The third constraint of the primal have an $=$ sign, therefore $y_0 \in \R$;
	\item Since $x_1, x_2, x_3 \geq 0$ in the primal we will have a $\geq$ sign
	      in the first three constraints of the dual;
	\item In the primal we had to maximize the objective, so in the dual we minimize it.
\end{itemize}

Now we can write the dual:
\begin{align}
	\min y_0          & \quad \text{subject to} \\
	-y_2 + y_0        & \geq 0                  \\
	y_1 + y_0         & \geq 0                  \\
	- y_1 + y_2 + y_0 & \geq 0                  \\
	-y_1 - y_2        & = 1                     \\
	y_1, y_2          & \leq 0
\end{align}

To obtain the same result as we did in class while reasoning on the actual 0-sum game we rewrite
the program with some new variables: $z_1 = -y_1$, $z_2 = -y_2$ and $z_0 = y_0$.
\begin{align}
	\min z_0          & \quad \text{subject to} \\
	-z_2 - z_0        & \leq 0                  \\
	z_1 - z_0         & \leq 0                  \\
	- z_1 + z_2 - z_0 & \leq 0                  \\
	z_1 + z_2         & = 1                     \\
	z_1, z_2          & \geq 0
\end{align}
where we have multiplied some of the constraints by $-1$ to get to the result.

\end{document}
