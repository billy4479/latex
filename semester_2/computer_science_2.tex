\documentclass[10pt]{extarticle}
\title{Computer Science 2 Notes}
\author{Giacomo Ellero}
\date{Semester 2, 2023/2024}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{dirtytalk}
\usepackage{parskip}
\usepackage{mathrsfs}
\usepackage[many]{tcolorbox}
\usepackage{xparse}
\usepackage[a4paper,margin=1.5cm]{geometry}
\usepackage{bookmark}
\usepackage{bytefield}
\usepackage{minted}
\usepackage{capt-of}

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\newenvironment{absolutelynopagebreak}
  {\par\nobreak\vfil\penalty0\vfilneg
   \vtop\bgroup}
  {\par\xdef\tpd{\the\prevdepth}\egroup
   \prevdepth=\tpd}

\newtcolorbox{examplebox}[1]{colback=green!5!white,colframe=green!40!black,title={#1},fonttitle=\bfseries,parbox=false}
\newtcolorbox{notebox}[1]{colback=blue!5!white,colframe=blue!40!black,title={Note: #1},fonttitle=\bfseries,parbox=false}
\newtcolorbox{bluebox}[1]{colback=blue!5!white,colframe=blue!40!black,title={#1},fonttitle=\bfseries,parbox=false}
\newtcolorbox{warningbox}[1]{colback=orange!5!white,colframe=orange!90!black,title={Warning: #1},fonttitle=\bfseries,parbox=false}
   
\begin{document}

\maketitle
\tableofcontents
\clearpage

\section{Class of 06/02/2024}

\subsection{Asymptotic notation}

\begin{bluebox}{Definition}
    If $\exists C \in \R^+$ and $N \in \N$ such that for two sequences $a_n, b_n > 0$
    we have that $a_n \leq Cb_n$ for all $n \geq N$, then we write $a_n = O(b_n)$ and we read \say{$a_n$ is big O of $b_n$}.
\end{bluebox}

These sequences describe the time it takes for an algorithm to solve a certain problem of size $n$.

\begin{examplebox}{Example}
    Let $a_n = n^2 + 2n + 1$. We will prove that $a_n = O(n^2)$.

    \begin{proof}
        We have that
        \begin{align*}
            a_n & = n^2 + 2n + 1        \\
                & \leq n^2 + 2n^2 + n^2 \\
                & = 4n^2
        \end{align*}
    \end{proof}

    As we can see just need to show that $C$ exists, we don't need to find the best one.
\end{examplebox}

Usually we don't use limits to prove that a sequence is big O of another, it is usually more convenient to proceed by inequalities.

\begin{notebox}{Logarithms}
    When we have sequences with logarithms we don't need to specify the basis,
    as the logarithm is a constant factor of another logarithm by the change of basis formula.
\end{notebox}

\subsubsection{Operations and other notation}

Let $a_n = O(c_n)$ and $b_n = O(d_n)$, then we have that

\begin{itemize}
    \item $a_n + b_n = O(\max\{c_n, d_n\})$
    \item $a_n \cdot b_n = O(c_n \cdot d_n)$
\end{itemize}

We also define the \say {opposite} of big O notation, the $\Omega$ notation:
$a_n$ is $\Omega(b_n)$ if $a_n \geq Cb_n$ for all $n \geq N$.

Moreover, if $a_n = O(b_n)$ and $a_n = \Omega(b_n)$ (for some different $C$ and $N$) then we write $a_n = \Theta(b_n)$.

\subsection{Randomness}

When we run an experiment we can define
\begin{itemize}
    \item an outcome $\omega_i$
    \item the value of the outcome $x_i$ (similar to a bet)
    \item the probability of the outcome $p_i$
\end{itemize}

We can also define the expected result $E(X)$ of the experiment as
$$
    E(X) = \sum ^n _{i = 1} x_i p_i
$$

\subsection{IEEE-754}

This is not in the syllabus but it's useful to know.

IEEE-754 is a standard for representing floating point numbers in computers.
We will discuss in particular the 32-bits representation but larger or smaller formats also exist.

We subdivide the 32 bits in 3 parts: 1 bit for the \textbf{sign}, 8 bits for the \textbf{exponent}, and 23 bits for the \textbf{fraction} or mantissa.

\begin{center}
    \begin{bytefield}[bitwidth=1.1em, bitheight=\widthof{~Sign~}]{32}
        \bitheader{0,8,31} \\
        \bitbox{1}{1}
        \bitboxes{1}{00100101}
        \bitboxes{1}{00110110101101001111011} \\
        \bitbox{1}{\rotatebox{90}{Sign}}
        \bitbox{8}{Exponent}
        \bitbox{23}{Fraction}
    \end{bytefield}
\end{center}

The number is interpreted according to the following formula:

$$
    n = (-1)^s \cdot 2^{e - 127} \cdot (1 + f)
$$

Basically the number is represented in scientific notation, in base 2.

Note that when converting the fraction part to decimal the powers of 2 are negative and decreasing
(i.e.: bit at 9 is $2^{-1}$, bit at 10 is $2^{-2}$, etc).

\section{Class of 08/02/2024}

\subsection{Introductory statements}

We will start by stating the following facts without providing a proof (they can be proven but in class we skipped them).

\begin{itemize}
    \item If $c \in \R^+$, then $g(n) = 1 + c + c^2 + \dots + c^n$ is $\Theta(1)$ if $c < 1$, $\Theta(n)$ if $c = 1$, and $\Theta(c^n)$ if $c > 1$.
    \item In any base $b \ge 2$ the sum of any 3 single-digit numbers is at most 2 digits long.
    \item $\forall n \in \N$ and any base $b$ there exists a power of $b$ in $[n, bn].$
\end{itemize}

\begin{notebox}{$\Theta(1)$}
    When we say that a function is $\Theta(1)$ we mean that it is bounded from above by a constant $C$ (since it is big O of 1)
    and bounded from below by a constant $c$ (since it is $\Omega(1)$).

    This usually means that $a_n \ne 0$ for all $n$.
\end{notebox}

\subsection{How to approach a algorithm problem}

We usually proceed by following these steps:
\begin{enumerate}
    \item Find an algorithm
    \item Prove that the algorithm is correct
    \item Calculate the time complexity of the algorithm
          \begin{enumerate}
              \item Can we do better?
          \end{enumerate}
\end{enumerate}

We will use the following steps to approach some classic problems such as addition and multiplication of binary numbers.

\begin{notebox}{Shifts}
    We refer to shifts as the operation of moving all the bits of a number to the left or to the right by a certain amount of positions.
    This operation mathematically corresponds to multiplying or dividing the number by a power of the base.

    $$
        x \ll n = x \cdot b^n
    $$

    The $\ll$ symbol denotes a left shift by $n$ positions, while the $\gg$ symbol denotes a right shift by $n$ positions.

    In our mathematical world each shift is $O(n)$, in the real world it is usually $O(1)$.
\end{notebox}

\subsection{Addition}

\underline{Problem}: Let $x, y$ be binary numbers of $n$ bits. We want to compute $x + y$.

We proceed by using the normal addition algorithm: this is a well know algorithm that we know is correct.

At each step we are adding at most 3 numbers (2 bits and a carry).
We know that the result of each step is at most 2 bits long: one bit goes for the result and the other goes for the carry.
This ensures that at the next step we will also be adding at most 3 numbers, hence each step is performed in a finite amount of time.

Since we are performing $n$ steps, the time complexity of this algorithm is $O(n)$.

\textit{Can we do better?} No, we can't, since we need to read all the bits of the input and this operation is already $O(n)$.

\subsection{Multiplication}

\underline{Problem}: Let $x, y$ be binary numbers of $n$ bits. We want to compute $x \cdot y$.

Again, we proceed using the normal multiplication algorithm that we know is correct.
The algorithm has the following parts:
\begin{enumerate}
    \item Write the multiplications of $x$ by each bit of $y$
    \item Sum all the results
\end{enumerate}

\textbf{Part 1}: Let $(s_n)$ be the sequence of the shifted values of $x$ and $p_n$ be the sequence of the partial products.

\begin{align*}
    s_0 & = x                         \\
    s_n & = s_{n-1} \ll 1             \\
    p_n & = \begin{cases*}
                0   & \text{if } y[n] = 0 \\
                s_n & \text{if } y[n] = 1
            \end{cases*}
\end{align*}

Choosing the right $p_n$ is $O(1)$, but computing $s_n$ is $O(n)$ hence this part is $O(n)$.

Note that we are \say{storing} the result of the previous shifts, so if we want to compute $s_5$, for example, we don't need to compute $s_4$ again.
Without this optimization the time complexity of this part would be $O(n^2)$.

\textbf{Part 2}:
In this step we are computing $\sum_{i = 0}^n p_n$.
These are sums of numbers of at most $2n$ bits, hence this part is $O(n)$.

\textbf{Conclusion}: The time complexity of the multiplication algorithm is $O(n) \cdot O(n) = O(n^2)$.

\subsubsection{Egyptian Multiplication}

This is an ancient algorithm that is used to multiply two numbers.
It works as follows:

\begin{enumerate}
    \item Write the two numbers in two columns
    \item Divide the first number by 2, floor the result and write it underneath it in the same column
    \item Multiply the second number by 2 and write the result underneath it in the same column
    \item When the first number is 1, sum all the numbers in the second column if the corresponding number in the first column is odd
\end{enumerate}

We can easily implement this algorithm in Python as follows:
\begin{minted}{python}
    def egyptian_multiplication(x, y):
        result = 0
        while x >= 1:       # This loop runs n times since x has n bits
            if x % 2 == 1:  # We always consider the worst case, hence this is always true
                result += y # Sums are O(n)
            x = x >> 2      # Shifts are O(n)
            y = y << 2      # Shifts are O(n)
        return result
\end{minted}

The complexity of the algorithm is $(O(n) + O(n) + O(n)) \cdot O(n) = O(n^2)$.

\subsubsection{Other multiplication algorithms}

Another algorithm we can use to implement multiplication works by dividing each number in half and recursively computing the result.

First we write the two numbers as
\begin{align*}
    x & = 2^{\frac{n}{2}}x_{\text{up}} + y_{\text{low}} \\
    y & = 2^{\frac{n}{2}}y_{\text{up}} + y_{\text{low}}
\end{align*}

Where $x_{\text{up}}$ and $y_{\text{up}}$ are the upper halves of $x$ and $y$ and $x_{\text{low}}$ and $y_{\text{low}}$ are the lower halves of $x$ and $y$.

Then we compute the result as
$$
    x \cdot y = 2^n x_{\text{up}}y_{\text{up}} + 2^{\frac{n}{2}}(x_{\text{up}}y_{\text{low}} + x_{\text{low}}y_{\text{up}}) + x_{\text{low}}y_{\text{low}}
$$

Each time we are performing 4 multiplication of half the size and adding them together.
The time this algorithm takes is
$$
    T(n) = 4T\left(\frac{n}{2}\right) + O(n)
$$

However if we keep expanding the recursion we will see that eventually we get to $T(1)$ which is $O(1)$
and we are left with $n$-many $O(n)$ terms, hence the time complexity of this algorithm is also $O(n^2)$.

\section{Class of 09/02/2024 - Divide and Conquer}

In this class we will see a class of algorithm that
split the problem in smaller sub-problems which are easier to compute and then combine the results.

\subsection{Karatsuba's algorithm}

This is a multiplication algorithm that is based on the last algorithm we discussed in the last class with some variations.

The algorithm takes inspiration from the multiplication of two complex numbers:
\begin{align*}
    (a + bi)(c + di) & = (ac - bd) + (ad + bc)i   \\
    \implies bc + ad & = (a + b)(c + d) - ac - bd
\end{align*}

Applying this to the multiplication of two binary numbers we get
\begin{align*}
    x \cdot y & = x_{\text{up}} y_{\text{up}} 2^n
    + 2^\frac{n}{2}((x_{\text{up}} + x_{\text{low}})(y_{\text{up}} + y_{\text{low}}) - x_{\text{up}}y_{\text{up}} - x_{\text{low}}y_{\text{low}}) + x_{\text{low}}y_{\text{low}}
\end{align*}

By doing this we are performing 3 multiplications of half the size and 4 additions.
Since we saw how additions are faster than multiplications we can expect this algorithm to be faster than the previous one.

$$
    T(n) = 3T\left(\frac{n}{2}\right) + O(n)
$$

We can arrange the work in nodes of a tree:

\begin{center}
    \begin{tabular}{ |c|c|c|c| }
        \hline
        level  & nodes  & work per node    & total work                      \\
        \hline
        0      & 1      & $Cn$             & $Cn$                            \\
        1      & 3      & $\frac{n}{2}C$   & $\frac{3}{2}Cn$                 \\
        2      & 9      & $\frac{n}{4}C$   & $\frac{9}{4}Cn$                 \\
        \vdots & \vdots & \vdots           & \vdots                          \\
        $k$    & $3^k$  & $\frac{n}{2^k}C$ & $\left(\frac{3}{2}\right)^k Cn$ \\
        \hline
    \end{tabular}
\end{center}

We have that the total work is the sum of the total work at each level of the tree:
$$
    Cn\left(1+\frac{3}{2}+\left(\frac{3}{2}\right)^2+\dots+\left(\frac{3}{2}\right)^k\right)
$$

Compared to the \say{old} algorithm that has a total work that looks like

$$
    Cn\left(1+2+2^2+\dots+2^k\right)
$$

Through some algebra we can show that this sum is $3Cn^{\log_{2}3}$,
hence proving that the algorithm is $O(n^{\log_{2}3}) \sim O(n^{1.58})$.

\subsection{The master theorem}

This theorem helps us to compute the time complexity of divide and conquer algorithms.
The proof of the theorem is quite complex and, despite being written here, it is not as important as understanding the statement.

\underline{Statement}: If $a > 0, b > 1, d \ge 0$, $n$ is a power of $b$, and
$T(n)$ is defined by induction as
\begin{align*}
    T(1) & = 1                                   \\
    T(n) & = aT\left(\frac{n}{b}\right) + O(n^d)
\end{align*}

Where
\begin{itemize}
    \item $a$ is the number of subproblems
    \item $b$ is the factor by which the problem size is reduced at each step
    \item $d$ is the exponent in the work done at each level
\end{itemize}

Then

\begin{align*}
    T(n) = \begin{cases}
               O(n^d)          & \text{if } d > \log_b a \\
               O(n^d \log n)   & \text{if } d = \log_b a \\
               O(n^{\log_b a}) & \text{if } d < \log_b a
           \end{cases}
\end{align*}

\begin{proof}
    We can write a table with the work to be done at each size of the problem

    \begin{center}
        \begin{tabular}{ |c|c|c|c| }
            \hline
            level  & \# of problems & size of each problem & total work                         \\
            \hline
            0      & 1              & $n$                  & $Cn^d$                             \\
            1      & $a$            & $\frac{n}{b}$        & $Ca\left(\frac{n}{b}\right)^d$     \\
            2      & $a^2$          & $\frac{n}{b^2}$      & $Ca^2\left(\frac{n}{b^2}\right)^d$ \\
            \vdots & \vdots         & \vdots               & \vdots                             \\
            $k$    & $a^k$          & $\frac{n}{b^k}$      & $Ca^k\left(\frac{n}{b^k}\right)^d$ \\
            \hline
        \end{tabular}
    \end{center}

    with $k = \log_b n$.

    To find the total work we need to sum all the total work at each step.
    With some algebra we can show that the total work is
    $$
        Cn^d\left(1+\left(\frac{a}{b^d}\right)+\left(\frac{a}{b^d}\right)^2+\dots+\left(\frac{a}{b^d}\right)^k\right)
    $$

    We can consider the following 3 cases:
    \begin{itemize}
        \item If $\frac{a}{b^d} = 1$, then $d = \log_b a$ and the sum becomes $ ??? $ and its complexity is $O(n^d \log n)$
        \item If $\frac{a}{b^d} < 1$, then $d > \log_b a$ and the sum becomes $ ??? $ and its complexity is $O(n^d)$
        \item If $\frac{a}{b^d} > 1$, then $d < \log_b a$ and the sum becomes $ ??? $ and its complexity is $O(n^{\log_b a})$
    \end{itemize}

    % TODO: complete the proof with picture and book
\end{proof}

\subsection{Introduction to sorting algorithms}

\subsubsection{Comparison based sorting}

Every comparison-based sorting algorithm
must do $\Omega(n \log n)$ comparison to sort $n$ distinct elements in the worst case.

\underline{Statement}: For any comparison-based algorithm $A$, for all $n \ge 2$ there exists an input of size $n$
such that $A$ makes at least $\log_2(n!) = \Omega(n \log n)$ comparisons.

The $n!$ comes from the fact that the factorial represents all the possible permutations of $n$ numbers,
hence, when we are sorting, we are just finding the \say{correct} permutation that sorts the input.

The $\log_2$ comes from the fact that each comparison we are making a choice, which the end gives two outcomes,
each one with half the options available for the next permutation.
Since we are looking for the worst outcome we have that we stop after $\log_2$ of the number we started with.

\subsubsection{Sorting without comparisons}

This is kinda surprising but it is actually possible if we have some other information about the input we are sorting.

For example, if we know that the input is a sequence of integers in a certain range we can use the \textbf{counting sort} algorithm.
This algorithm works by counting the occurrences of each number in the input and then writing them in the correct order.
This is a very fast algorithm compared to other generic sorting and takes $O(n)$ time.

Other examples of sorting algorithms that don't use comparisons are \textbf{radix sort} and \textbf{bucket sort}.

\subsection{Finding the median}

In general we can sort the list and find the median like that, which would be $O(n \log n)$.
It turns out that we can do better than that.

We will solve the more general problem that looks like this:
\begin{itemize}
    \item \textit{Input}: A list of numbers $S$; an integer $k$
    \item \textit{Output}: The $k$-th smallest element of $S$
\end{itemize}

In particular if $k = \frac{|S|}{2}$ we have that $k$ is the median.

\subsubsection{Description of the algorithm}

To do so we will use a recursion based algorithm:

\begin{enumerate}
    \item Select an element $V$ from $S$ at random
    \item We create 3 sub-lists:
          \begin{enumerate}
              \item $S_L = {x \in S : x < V}$
              \item $S_V = {x \in S : x = V}$
              \item $S_R = {x \in S : x > V}$
          \end{enumerate}
    \item We now call the algorithm recursively with the following parameters:
          \begin{enumerate}
              \item If $k \le |S_L|$ then we call the algorithm with $S_L$ and $k$
              \item If $|S_L| < k \leq |S_L| + |S_V|$ then we return $V$
              \item If $k > |S_L| + |S_V|$ then we call the algorithm with $S_R$ and $k - |S_L| - |S_V|$
          \end{enumerate}
\end{enumerate}

\subsubsection{The choice of V}

To calculate the time complexity of this algorithm we first have to choose a good $V$.
This is a hard task, because the \say{best} $V$ would be the median, but that's exactly what we are trying to find;
on the other hand the \say{worst} $V$ would be the minimum or the maximum of the input, which would reduce the problem size by only 1.

Since we cannot choose a good $V$ we will choose a random $V$:
the odds of choosing the best or the worst cases are both extremely unlikely,
hence we need to define a \say{good enough} $V$.

We consider a good enough $V$ if it lies between $\frac{1}{4}$ and $\frac{3}{4}$ of the input,
hence we can expect that the size of the sub-problems will be at most $\frac{3}{4}$ of the original problem.

This choice is arbitrarily, we could have chosen any other fraction and the time complexity,
as we will see later, would have been the same.
We chose these fractions because in this way the input
is divided in 2 \say{bad} cases of size $\frac{1}{4}$ and a \say{good} case of size $\frac{1}{2}$,
hence the size of the \say{bad} part is the same as the size of the \say{good} part.

\subsection{Time complexity}

We can now compute the time complexity

$$
    T(n) = \underbrace{T\left(\frac{3}{4}n\right)}
    _{\text{expected reduction}} +
    \underbrace{O(n)}_{\text{comparisons with } V}
    + \underbrace{\gamma O(n)}_{\text{finding a good } V}
$$

Note that $\gamma$ is the byproduct of the random choice of $V$,
we don't know how many times we will have to choose $V$ to find a good one.

Although we cannot be sure about the exact time this algorithm takes,
we can compute the \textbf{expected} time.

We saw before how the probability of choosing a good $V$ is $\frac{1}{2}$.
We can calculate the expected time as follows:
we pick a random $V$ and

\begin{enumerate}
    \item If $V$ is good we are done
    \item If $V$ is bad we need to repeat the algorithm
\end{enumerate}

Hence we have that $E = 1 + \frac{1}{2} E$, thus $E = 2$.
We have


\begin{align*}
    E(T(n)) & = E\left(T\left(\frac{3}{4}n\right)\right) + O(n) + 2O(n) \\
            & = O(n) + O(n) + 2O(n)                                     \\
            & = O(n)
\end{align*}

\begin{notebox}{Other solutions}
    This algorithm is presented to illustrate that algorithms that make use of random variables exist and can be useful;
    in the case of this specific problem there exist better algorithms that can solve the problem in a deterministic way,
    such as the \textbf{median of medians} algorithm.
\end{notebox}

\section{Class of 13/02/2024 - Graphs}

This week we will be working on graphs, this is referenced in chapter 3 of the book.

\begin{center}

    \begin{tikzpicture}
        \node[draw, circle] (A) at (0, 0) {A};
        \node[draw, circle] (B) at (2, 0) {B};
        \node[draw, circle] (C) at (2, 2) {C};
        \node[draw, circle] (D) at (6, 2) {D};
        \node[draw, circle] (E) at (0, 5) {E};
        \node[draw, circle] (F) at (2, 4) {F};
        \node[draw, circle] (G) at (-2, 4) {G};
        \node[draw, circle] (H) at (3, 3) {H};

        \draw (A) -- (B);
        \draw (B) -- (C);
        \draw (C) -- (D);
        \draw (D) -- (A);
        \draw (A) -- (E);
        \draw (E) -- (F);
        \draw (F) -- (C);
        \draw (C) -- (G);
        \draw (H) -- (C);
    \end{tikzpicture}
    \captionof{figure}{An example of a graph} \label{fig:graph}
\end{center}

Graphs can be useful to represent a lot of different things,
for example relationships, networks, maps, tournaments, etc.

\subsection{Definitions}

\begin{bluebox}{Definition}
    A \textbf{graphs} $G = (V, E)$ is a pair of finite sets where
    $V$ is the set of \textbf{vertices} (or nodes) and $E$ is the set of \textbf{edges}.

    $E$ is a subset of $V \times V$.
\end{bluebox}

The elements of $E$ are pairs of vertices that represent the connections between the vertices.

These are some other useful definitions we will use:
\begin{description}
    \item[Adjacent vertices] If $e = (u, v) \in E$ then we say that $u$ and $v$ are adjacent.
    \item[Incident edges] If $e = (u, v) \in E$ then we say that $e$ is incident to $u$ and $v$.
    \item[Neighbour] A vertex joined by an edge to another vertex is called a neighbor.
    \item[Directed and undirected graphs] a graph is \textit{directed} if the edges have a direction, otherwise it is \textit{undirected}. In an undirected graph the edges are unordered pairs $\{u, v\}$, in a directed graph they are ordered pairs $(u,v)$.
    \item[Loop] if a vertex is connected to itself we say that it is a loop.
    \item[Simple graph] a graph is simple if it has no loops and any pair of vertices is connected by at most one edge.
    \item[Degree of a vertex] the degree of a vertex $v$, $d(v)$ is the number of edges incident to it.
    \item[Walk] a walk on a graph is an alternating series of vertices and edges in which every edge is incident to the two vertices immediately preceding and following it.
    \item[Path] a path is a walk in which no vertex is repeated.
    \item[Cycle] a cycle is a path in which the first and last vertices are the same.
    \item[Acyclic] a graph is acyclic if it has no cycles.
    \item[Connectedness] we have two definition depending if the graph is directed or undirected:
        \begin{itemize}
            \item An \underline{undirected graph} is connected if every vertex is reachable from all other vertices.
            \item A \underline{directed graph} is \textit{strongly connected} if every vertex is reachable from all other vertices. We use the word \textit{strongly} because some nodes may have edges connecting to it but not going out of it.
        \end{itemize}

        We call each connected part of the graph a \textbf{connected component} of $G$. In \textit{undirected graphs} this is an equivalence relation on the set of vertices.
    \item[Bipartite graph] a bipartite graph is an undirected graph $G = (V, E)$ where $V$ can be partitioned into two sets $V_1, V_2$ such that every edge $(u, v) \in E$ is such that $u \in V_1$ and $v \in V_2$.
        This means that there are no edges between vertices in the same set.

    \item[Tree] a tree is a connected acyclic undirected graph, or a connected forest.
        In every tree it is possible to identify a node a the \underline{root}, every node can be seen as the root of the tree.
        We call \underline{descendant} of a node $v$ every node that is reachable from $v$ going downwards and \underline{ancestor} every node that is reachable from $v$ going upwards.
    \item[Forest] a forest is an acyclic undirected graph, or a union of trees.
\end{description}

\subsection{Memory representation}

\subsubsection{Adjacency matrix}

\begin{bluebox}{Definition}
    Let $G = (V, E)$ be a graph with $|V| = n$.

    An \textbf{adjacency matrix} is a $|V| \times |V|$ matrix $A$ where
    $$
        a_{ij} = \begin{cases}
            1 & \text{if } (i, j) \in E    \\
            0 & \text{if } (i, j) \notin E
        \end{cases}
    $$
\end{bluebox}

We want to represent the following graph using an adjacency matrix:

\begin{absolutelynopagebreak}
    \begin{center}
        \begin{tikzpicture}
            \node[draw, circle] (A) at (0, 2) {1};
            \node[draw, circle] (B) at (2, 2) {2};
            \node[draw, circle] (C) at (0, 0) {3};
            \node[draw, circle] (D) at (2, 0) {4};

            \draw (A) -- (B);
            \draw (A) -- (C);
            \draw (A) -- (D);
        \end{tikzpicture}
        \captionof{figure}{An example graph} \label{fig:repr:graph1}
    \end{center}
\end{absolutelynopagebreak}

We can represent this graph using the following adjacency matrix:

\begin{absolutelynopagebreak}
    \begin{center}
        $$
            \begin{bytefield}[bitwidth=1.25em]{4}
                \bitheader{0,1,2,3} \\
                \bitboxes{1}{0111} \\
                \bitboxes{1}{1000} \\
                \bitboxes{1}{1000} \\
                \bitboxes{1}{1000} \\
            \end{bytefield}
        $$
        \captionof{figure}{The adjacency matrix of the graph in (\ref{fig:repr:graph1})}
        \label{fig:repr:adjmatrix1}
    \end{center}
\end{absolutelynopagebreak}

Using this representation we can easily check if two vertices are connected ($O(1)$) but we need a lot of space to store it ($O(|V|^2)$).

Adjacency matrices can be useful can be useful since we can perform linear algebra magic on them
(e.g.: compute the $k$ power, or find the eigenvalues) to find some properties of the graph.

\subsubsection{Adjacency list}

This is a more space efficient way to represent a graph, especially if the graph is sparse (meaning it has few edges and a lot of nodes).

\begin{bluebox}{Definition}
    Let $G = (V, E)$ be a graph with $|V| = n$.

    An \textbf{adjacency list} is a list of $n$ lists where the $i$-th list contains all the vertices adjacent to the $i$-th vertex.
\end{bluebox}

\textbf{Example}: The graph in (\ref{fig:repr:graph1}) can be represented using the following adjacency list:

\begin{absolutelynopagebreak}
    \begin{center}
        \begin{tabular}{c|c}
            Vertex & Adjacent vertices \\
            \hline
            1      & 2, 3, 4           \\
            2      & 1                 \\
            3      & 1                 \\
            4      & 1
        \end{tabular}
        \captionof{figure}{The adjacency list of the graph in (\ref{fig:repr:graph1})}
        \label{fig:repr:adjlist1}
    \end{center}
\end{absolutelynopagebreak}

This in practice can be implemented using linked lists: in this case we have a space complexity of $O(|V| + |E|)$ while the time complexity to find if a vertex is connected to another one is $O(|E|)$.

(Is really linked lists the best way to implement this? What about other data structures?)

\subsubsection{Summerizing}

We can use the following table to summarize the two representations:

\begin{center}
    \begin{tabular}{c|c|c|c}
               & space          & $(u, v) \in E$ & all neighbors \\
        \hline
        matrix & $O(|V|^2)$     & $O(1)$         & $O(|V|)$      \\
        list   & $O(|V| + |E|)$ & $O(d(v))$      & $O(d(v))$
    \end{tabular}
    \captionof{table}{Comparison of graph representations}
    \label{tab:graphrepr}
\end{center}

\subsection{Exploring a graph - Depth First Search (DFS)}

In this section we will look at algorithms to explore a graph.

We setup the algorithm like this:

\begin{minted}{python}
    # These are the variables we will be using:
    clock = 1
    visited = {}
    pre = {}
    post = {}

    def setup(G):
        clock = 1
        for v in G.vertices():
            visited[v] = False
            pre[v] = None  # time of first visit of `v`
            post[v] = None # time of last visit of `v`
\end{minted}

The actual algorithm is as follows:

\begin{minted}{python}
    def explore(G, v):
        visited[v] = True
        pre[v] = clock
        clock += 1
        for (u, v) in v.edges():
            if not visited[u]:
                explore(G, u)
        post[v] = clock
        clock += 1
\end{minted}

Then we call \texttt{explore} on any vertex of $G$ and let the function traverse the graph.

\section{Class of 14/02/2024 - DFS continued}

\subsection{Formal statement}

We can now state the algorithm formally:

\underline{Statement}: Let $G = (V, E)$ be a graph and $v \in V$.
The algorithm \texttt{explore(G, v)} visits all the vertices of $G$ that are reachable from $v$.

This is equivalent to

\underline{Statement}: $v$ can reach $u$ $\iff$ \texttt{visited[u]} is \texttt{True} after completing \texttt{explore(G, v)}.

\begin{proof}
    We prove each implication separately:

    \begin{description}
        \item[$\impliedby$] Since we started from \texttt{visited} containing only \texttt{False} values,
            the only way \texttt{visited[u]} can be \texttt{True} is if $u$ was part of the exploration of some other node $w$.
            If $W = v$ we are done, otherwise we can repeat the argument for $w$.
            We know this procedure will terminate since the graph is finite.
        \item[$\implies$] We prove this by contradiction.
            Suppose that $u$ is reachable from $v$ and that there is a run of \texttt{explore} that misses $u$.
            Since $u$ is reachable from $v$ there is a path from $v$ to $u$.
            Let $w$ be the first vertex on this path that is not visited by the run of \texttt{explore} and let $z$ be the vertex that precedes $w$.
            This is a contradiction, by the way the algorithm is defined, since $w$ is a neighbor of $z$ and it is not visited we visit it.
    \end{description}
\end{proof}

By the definition of the algorithm we have that it doesn't work if we have a graph with more than 1 connected component.

We can complete the algorithm to take care of this case too:

\begin{minted}{python}
    def DFS(G):
        # Setup
        clock = 1
        visited = {}
        pre = {}
        post = {}

        for v in G.vertices():
            visited[v] = False
            pre[v] = None
            post[v] = None

        # Run the algorithm for all nodes.
        # This will actually call `explore` just once
        # per connected component.
        for v in G.vertices():
            if not visited[v]:
                explore(G, v)

\end{minted}

\subsection{Time comlexity}

We have that the setup part is $O(|V|)$, since we are looping through all nodes.

For the actual \texttt{explore} part we have at least $O(|V|)$ since for each node we have to set \texttt{visited} to \texttt{True},
but we have to take in account the time to visit all the edges of the graph:
each edge is visited exactly twice.

Combining the two parts we have that the time complexity of the algorithm is $O(|V| + |E|)$.

\subsection{About the pre and post variables}

\underline{Remark}: For every node $v$ we have that \texttt{pre[v]} $<$ \texttt{post[v]}.

\begin{proof}
    This is obvious from the definition of the algorithm.
\end{proof}

\underline{Theorem}: For every node $v$ and $u$, the intervals $[\text{pre}(v), \text{post}(v)]$ and $[\text{pre}(u), \text{post}(u)]$ are either disjoint or one is contained in the other.

\begin{proof}
    We have two cases:
    \begin{enumerate}
        \item $\text{pre}(v) < \text{post}(u)$, hence $\text{pre}(u) < \text{pre}(v) < \text{post}(u)$. This means that $v$ is a descendant of $u$, thus the exploration of $v$ must have ended before the exploration of $u$.
              We have that $\text{pre}(u) < \text{pre}(v) < \text{pre}(u) < \text{post}(v)$, hence the intervals are contained in one another.
        \item $\text{pre}(u) > \text{post}(v)$, hence $\text{pre}(u) < \text{post}(u) < \text{pre}(v) < \text{post}(v)$ and the intervals are disjoint.
    \end{enumerate}
\end{proof}

\subsection{Counting connected components}

We can modify the algorithm to count the number of connected components in the graph.

\begin{minted}{python}
    def DFS(G):
        clock = 1
        visited = {}
        pre = {}
        post = {}
        cc = {}

        for v in G.vertices():
            visited[v] = False
            pre[v] = None
            post[v] = None
            cc[v] = None

        connected_component = 0

        for v in G.vertices():
            if not visited[v]:
                connected_component += 1
                explore(G, v)


    def explore(G, v):
        cc[v] = connected_component
        # The rest of the algorithm is the same


\end{minted}

In this way, at the end of the procedure the variable \texttt{connected\_component} will contain the number of connected components in the graph
and the variable \texttt{cc} will contain the connected component of each node.

\section{Class of 15/02/2024 - DFS continued}

\subsection{DFS-Tree}

We can introduce a new variable, initialized as

\begin{minted}{python}
    parent = {}
    for v in G.vertices():
        parent[v] = None
\end{minted}

When exploring the graph we set this variable to the node that called the current one.
In this way we can build a tree that represents the exploration of the graph.

\subsection{Directed graphs}

\begin{center}
    \begin{tikzpicture}
        \node[draw, circle] (A) at (0, 0) {A};
        \node[draw, circle] (B) at (-2, 0) {B};
        \node[draw, circle] (C) at (2, 0) {C};
        \node[draw, circle] (D) at (2, -2) {D};
        \node[draw, circle] (E) at (-2, -2) {E};
        \node[draw, circle] (F) at (0, -2) {F};
        \node[draw, circle] (G) at (0, -4) {G};
        \node[draw, circle] (H) at (2, -4) {H};

        \draw[->, very thick] (A) -- (B);
        \draw[->, very thick] (A) -- (C);
        \draw[->, very thick] (C) -- (D);
        \draw[->, very thick] (D) -- (A);
        \draw[->, very thick] (A) -- (F);
        \draw[->, very thick] (B) -- (E);
        \draw[->, very thick] (E) -- (F);
        \draw[->, very thick] (E) -- (G);
        \draw[->, very thick] (E) -- (H);
        \draw[->, very thick] (D) -- (H);
        \draw[->, very thick] (F) -- (G);
        \draw[->, very thick] (H) -- (G);
    \end{tikzpicture}
    \captionof{figure}{An example of a directed graph} \label{fig:directedgraph}
\end{center}

By looking at the graph in (\ref{fig:directedgraph}) and performing a DFS in our head we can see that, starting from $A$, we can reach all the other nodes.
We also note that this is not true for every node, for example $G$ cannot reach any other node.

If we construct the DFS tree of this graph starting from $A$ and breaking ties in alphabetical order we get the following tree:

\begin{center}
    \begin{tikzpicture}
        \node[draw, circle] (A) at (0, 0) {A};
        \node[draw, circle] (B) at (-2, -2) {B};
        \node[draw, circle] (C) at (2, -2) {C};
        \node[draw, circle] (D) at (2, -4) {D};
        \node[draw, circle] (E) at (-2, -4) {E};
        \node[draw, circle] (F) at (-1, -6) {F};
        \node[draw, circle] (G) at (-1, -8) {G};
        \node[draw, circle] (H) at (-3, -6) {H};

        \draw[->, very thick] (A) -- (B);
        \draw[->, very thick] (A) -- (C);
        \draw[->, very thick] (C) -- (D);
        \draw[->, very thick] (B) -- (E);
        \draw[->, very thick] (E) -- (F);
        \draw[->, very thick] (F) -- (G);
        \draw[->, very thick] (E) -- (H);
    \end{tikzpicture}
    \captionof{figure}{The DFS tree of the graph in (\ref{fig:directedgraph})} \label{fig:dfs-tree}
\end{center}

We can define the following for DFS trees:

\begin{description}
    \item[Forward edge] an edge of the original graph that goes from a node to one of its descendants in the DFS tree.
    \item[Back edge] an edge of the original graph that goes from a node to one of its ancestors in the DFS tree.
    \item[Cross edge] an edge of the original graph that goes from a node to a node that is neither an ancestor nor a descendant.
\end{description}

\begin{center}
    \begin{tikzpicture}
        \node[draw, circle] (A) at (0, 0) {A};
        \node[draw, circle] (B) at (-2, -2) {B};
        \node[draw, circle] (C) at (2, -2) {C};
        \node[draw, circle] (D) at (2, -4) {D};
        \node[draw, circle] (E) at (-2, -4) {E};
        \node[draw, circle] (F) at (-1, -6) {F};
        \node[draw, circle] (G) at (-1, -8) {G};
        \node[draw, circle] (H) at (-3, -6) {H};

        \draw[->, very thick] (A) -- (B);
        \draw[->, very thick] (A) -- (C);
        \draw[->, very thick] (C) -- (D);
        \draw[->, very thick] (B) -- (E);
        \draw[->, very thick] (E) -- (F);
        \draw[->, very thick] (F) -- (G);
        \draw[->, very thick] (E) -- (H);

        % Back edges
        \draw[->, very thick, blue] (D) to [bend left] (A);
        \draw[->, very thick, blue] (F) to [bend right] (B);

        % Forward edges
        \draw[->, very thick, red] (A) to [bend left] (F);
        \draw[->, very thick, red] (E) to [bend right] (G);

        % Cross edges
        \draw[->, very thick, green] (D) to (H);
        \draw[->, very thick, green] (H) to [bend right] (G);

    \end{tikzpicture}
    \captionof{figure}{The DFS tree of the graph in (\ref{fig:directedgraph}) with forward (red), back (blue), and cross edges (green)} \label{fig:dfs-tree-fbg}
\end{center}

We note that if we consider the pre and post variable for each node as an \say{interval},
we can see that nodes connected with a forward edge have the interval of the children contained into the interval of the parent,
nodes connected with a back edge have the interval of the parent contained in the interval of the children,
and nodes connected with a cross edge have disjoint intervals.

\subsection{Theorems about edges}

\subsubsection{Edges in undirected graphs}

\underline{Statement}: In a DFS of an undirected graph, every edge is either a tree edge or a back edge.

\begin{proof}
    Let $\{u, v\}$ be an edge of $G$ and suppose that $\text{pre}(u) < \text{pre}(v)$, hence $u$ is discovered before $v$.

    We have two cases:

    \begin{itemize}
        \item If the first time the search explores $\{u, v\}$ is in the direction from $u$ to $v$ then $v$ is undiscovered until that time.
              Then the edge $\{u, v\}$ is a tree edge.
        \item If the first time the search explores $\{u, v\}$ is in the direction from $v$ to $u$ then $v$ is already discovered.
              Then the edge $\{u, v\}$ is a back edge.
    \end{itemize}
\end{proof}

\subsubsection{Cycles and back edges}

\underline{Statement}: A directed graph has a cycle $\iff$ a DFS of the graph has at least one back edge.

\begin{proof}
    We prove each implication separately:

    \begin{description}
        \item[$\impliedby$] If $(u, v)$ is a back edge then there is a cycle consisting of the edge $(u, v)$ and the path from $v$ to $u$ in the DFS tree. We know such path exists since to have a back edge we have to get to $v$ with $u$ already being discovered.
        \item[$\implies$] If $G$ has a cycle, then we can write it as $v_0 \to v_1 \to \dots \to v_k \to v_0$.
            Let $v_i$ be the first vertex in the cycle that is discovered.
            When we enter the cycle all the other nodes of the cycle are undiscovered since $v_i$ is the first one by definition. Since DFS visits all the \say{legal} edges the algorithm will visit all edges of the cycle, hence eventually it will reach the edge $(v_{i-1}, v_i)$, which is a back edge.
    \end{description}
\end{proof}

\subsection{Topological Sort}

\underline{Definition}: If $G = (V, E)$ is a directed graph we say that an ordered list $L$ of the vertices in $V$ is a \textbf{topological sort} of $G$ if for every edge $(u, v) \in E$, the vertex $u$ comes before $v$ in $L$.

\underline{Example}: Consider the graph in Figure (\ref{fig:directedgraph}). A topological sort of this graph is $[A, C, D, B, E, F, H, G]$.
Note that topological sorts are not unique.

\end{document}