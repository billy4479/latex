\documentclass[12pt]{extarticle}

\title{Analysis 3 Theorems}
\author{Giacomo Ellero}
\date{a.y. 2024/2025}

\usepackage{preamble_base}
\usepackage{preamble_math}

% \renewcommand{\vec}[1]{\uvec{#1}}
\numberwithin{equation}{section}

\begin{document}

\section{Maximal solutions either coincide or do not intersect}

\begin{theorem}{}{}
    In the same setting of the Peano-Picard theorem,
    consider the maximal solution $u: I \to \R$
    for the i.v.p. $u(t_0) = \lambda_0$.
    Given $t_1 \in I$ and $\lambda_1 = u(t_1)$,
    consider $\tilde{u}: \tilde I \to \R$,
    the solution to the same ODE with initial condition $\tilde{u}(t_1) = \lambda_1$.
    Then
    \begin{equation}
        I = \tilde I \quad \text{and} \quad u = \tilde u
    \end{equation}
\end{theorem}

\begin{proof}
    Since, by assumption, $u$ also solves the i.v.p. with initial condition $(t_1, \lambda_1)$,
    by Peano-Picard, $I \subseteq \tilde I$ and $u = \left. \tilde u \right|_I$.

    Since $t_0 \in I$ it means $t_0 \in \tilde I$, meaning that $\tilde u(t_0) = \lambda_0$
    (since $u(t_0) = \lambda_0$), but now we can apply Peano-Picard again for the initial conditions $(t_0, \lambda_0)$
    and prove the claim.
\end{proof}


\begin{theorem}{}{}
    Under the assumptions of Peano-Picard, let $(t_0, \lambda_0)$ and $(t_0, \mu_0)$ with $\lambda_0 \ne \mu_0$.
    Consider $u: I' \to \R$ and $v: I'' \to \R$ to be solutions for the ODE with initial conditions as above.
    Then
    \begin{equation}
        u(t) \neq v(t) \quad \forall t \in I' \cap I''
    \end{equation}
\end{theorem}

\begin{proof}
    Assume that there exists $t_1 \in I' \cap I''$ such that $u(t_1) = v(t_1) = \lambda_1$.
    Let $w: I \to \R$ be the maximal solution for initial condition $(t_1, \lambda_1)$,
    then we must have that
    \begin{equation}
        I'\subseteq I, \quad I'' \subseteq I, \quad u = \left. w\right|_{I'}, \quad v = \left. w\right|_{I''}
    \end{equation}

    In particular, we must have $t_0 \in I \implies u(t_0) = w(t_0) = v(t_0)$, which is a contradiction.
\end{proof}

\section{\texorpdfstring{$t^j e^{\alpha_\ell t}$ form a basis of solutions}{Basis of solutions}}

\begin{theorem}{}{}
    Assume that the roots of the characteristic polynomial $p(z)$ are all real.
    Let $\alpha_1, \dots, \alpha_n$ be the distinct roots of $p(z)$ and
    $M_1, \dots, M_n$ be the algebraic multiplicity of each $\alpha_\ell$.

    For each root, consider the following set of functions:
    \begin{equation}
        S = \left\{ t^j e^{\alpha_l t} : l \in 1, \dots, n; j \in 0, \dots, m_k -1 \right\}
    \end{equation}
    each one of them solves the ODE and $S$ is a basis of the space of solutions.
\end{theorem}

\begin{proof}
    We will show that all the functions in $S$ solve the ODE.
    Then, to show that they form a basis, it is sufficient to note that
    $\abs{S} = \sum M_i = k$ where $k$ is the degree of $p(z)$,
    hence we just need to show that they are linearly independent.
    We will not prove linear independence.

    Introduce the operator $a(D) : X \to X$, where $a$ is a polynomial and
    $X$ is the set of functions of class $C^\infty$.
    Given $a(z) = \sum_{j= 0}^\ell \mu_j z^j$, the operator is defined as
    \begin{equation}
        a(D)f = \sum^{\ell}_{j = 0} \mu_j \dv[j]{f(t)}{t}
    \end{equation}
    Note that this operator is linear and behaves like a polynomial,
    notably if $c(z) = a(z)b(z)$ then $c(D) = a(D) \circ b(D)$ is commutative.

    Assume the characteristic polynomial has leading coefficient of $1$,
    then $u$ is a solution to the ODE (up to multiplying it by the inverse of the leading coefficient) if
    \begin{equation}
        p(D)u = 0
    \end{equation}

    Let us factorize $p(z)$ as
    \begin{equation}
        p(z) = (z-\alpha_1)^{M_1} \dots (z-\alpha_n)^{M_n}
    \end{equation}
    now isolate $r(z) = (z-\alpha_\ell)^{M_\ell}$ for some $\ell \in 1, \dots, n$
    and let $q(z) = \prod_{j \neq \ell} (z-\alpha_j)^{M_j}$
    such that $p(z) = q(z)r(z)$.
    Then, to show that $u$ solves the ODE it is enough to show that
    \begin{equation}
        r(D)u = (D - \alpha_\ell)^{M_\ell}u = 0
    \end{equation}

    We want to show this holds for each candidate solution.
    We can write a general candidate as $h(t) e^{\alpha_\ell t}$
    where $h(t)$ is a polynomial of degree less than $M_\ell$.
    Hence our proof reduces to showing that
    \begin{equation}
        (D - \alpha_\ell)^{M} [h(t) e^{\alpha_\ell t}] = 0
    \end{equation}
    with $M = M_\ell$.

    We can now proceed by induction on $M$:
    \begin{description}[font=\normalfont\itshape\space]
        \item[Base case]
              Since $M = 1$ the degree of $h(t)$ has to be $0$, hence it is constant.
              We have
              \begin{equation}
                  (D - \alpha_\ell)[h e^{\alpha_\ell t}] = h \dv{t} e^{\alpha_\ell t} - h\alpha e^{\alpha_\ell t} = 0
              \end{equation}
        \item[Inductive step]
              \begin{align}
                  (D - \alpha_\ell)^M[h(t) e^{\alpha_\ell t}] & = (D - \alpha_\ell)^{M-1}\left[ (D - \alpha_\ell) \left[h(t) e^{\alpha_\ell t}\right]\right]                                                               \\
                                                              & = (D - \alpha_\ell)^{M-1}\left[ h'(t) e^{\alpha_\ell t} + \cancel{h(t)\alpha_\ell e^{\alpha_\ell t}} - \cancel{h(t) \alpha_\ell e^{\alpha_\ell t}} \right] \\
                                                              & = 0
              \end{align}
              where in the last step we notice that since $h'(t)$ has degree less than $M -1$
              hence the inductive hypothesis applies.
    \end{description}
\end{proof}

\section{\texorpdfstring{$m_\gamma \leq M_\gamma$}{Geometric multiplicity less than or equal to algebraic}}

\begin{theorem}{}{}
    For all eigenvalues $\gamma \in \R$ there holds $1 \leq m \leq M$,
    where $m$ is geometric multiplicity of $\gamma$ and $M$ is the algebraic multiplicity of $\gamma$.
\end{theorem}

\begin{proof}
    Since $\gamma$ is an eigenvalue there exists an eigenvector $v \neq 0$,
    hence $\ker (A - \gamma \mathds{1}) \ne 0$ and $m \geq 1$.

    Let $f: V \to V$ be the linear map associated to $A$: $f(v) = A v$, where $V = \R^n$.
    Let $\{v_1, \dots, v_m\}$ be the basis of the eigenspace of $\gamma$
    and complete it to $\{v_1, \dots, v_n\}$ a basis of $V$.
    Then let $P = \begin{pmatrix}v_1 & \dots & v_n\end{pmatrix}$ be the change of basis matrix
    such that
    \begin{equation}
        A = P B P^{-1}
    \end{equation}

    Since $f(v_j) = \gamma v_j$ for all $j \in 1, \dots, m$, $B$ has the form of
    \begin{equation}
        B = \left(\begin{array}{c|c}
            \begin{matrix}
                \gamma &        & 0      \\
                       & \ddots &        \\
                0      &        & \gamma
            \end{matrix} & *_1          \\
            \hline
            0                           & *_2
        \end{array}
        \right)
    \end{equation}
    Notice that $z \mathds{1} - B$ is a triangular block matrix, hence its characteristic polynomial is
    \begin{equation}
        \det(z \mathds 1 - B) = (z-\gamma)^m \det(z \mathds 1_{n-m} - *_2)
    \end{equation}
    but since $A$ and $B$ are similar they have the same characteristic polynomial,
    hence $(z-\gamma)^m$ divides the characteristic polynomial of $A$,
    which means $M \geq m$.
\end{proof}

\section{Diagonalizable \texorpdfstring{$\iff m_\gamma = M_\gamma$}{iff geometric equals to algebraic}}

\begin{theorem}{}{}
    A square matrix is diagonalizable over $\C$ if and only if
    for each root $\gamma$ of the characteristic polynomial
    $m_\gamma = M_\gamma$.

    Moreover, it is diagonalizable over $\R$ if and only if all the roots are also real.
\end{theorem}

\begin{proof}
    We will just show the statement over $\R$ for simplicity.

    We will also use lemma which reads that non-zero eigenvector corresponding
    to distinct eigenvalues are linearly independent without proof.

    \begin{description}
        \item[$\implies$] If a matrix is diagonalizable over $\R$ we can find a basis of eigenvector,
              $\mathcal B = \{v_1, \dots, v_n \} \in \R^n$: call $\gamma_j$ their eigenvalues.
              Fix a specific $\gamma_j = \gamma$, then let $n_\gamma$ be the number of elements of $\mathcal B$
              which are eigenvectors of $\gamma$.
              These vectors belong to the eigenspace $\ker (A - \gamma \mathds 1)$ and,
              since they are part of a basis, they are linearly independent.
              So far we have that
              \begin{equation}
                  n_\gamma \leq m_\gamma \leq M_\gamma
              \end{equation}
              Moreover, since the vectors $v_j$ form a basis, necessarily $\sum n_\gamma = n$.
              On the other hand, the sum of the multiplicities of the real roots of a polynomial is at most its degree,
              which means $\sum M_\gamma \leq n$, and we obtain
              \begin{equation}
                  n = \sum n_\gamma \leq \sum m_\gamma \leq \sum M_\gamma \leq n
              \end{equation}
              therefore we must have $n_\gamma = m_\gamma = M_\gamma$ for all real roots $\gamma$.
              It also follows that, since $\sum M_\gamma = n$ (where $n$ was just the sum over the real roots),
              $p(z)$ has only real roots (there is no \say{space} for complex ones).
        \item[$\impliedby$] If all roots are real and $m_\gamma = M_\gamma$, then $\sum m_\gamma = \sum M_\gamma = n$.
              This means that we can find a basis of $\ker(A - \gamma \mathds{1})$ consisting of $m_\gamma$ vectors
              for each $\gamma$, which together form a collection of $n$ vectors.
              To conclude they are a basis we are just left to check that they are linearly independent.
              We now just use the fact that basis of eigenspaces are indeed linearly independent among the same eigenspace
              and combine it with the lemma.
    \end{description}
\end{proof}

\end{document}